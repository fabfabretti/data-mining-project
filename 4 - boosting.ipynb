{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 3 - Sequence tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import os\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "# Takes a timedelta and prints it in a human-readable format.\n",
    "def format_timedelta(td: timedelta) -> str:\n",
    "    days = td.days\n",
    "    years, days = divmod(days, 365)\n",
    "    months, days = divmod(days, 30)\n",
    "    hours, remainder = divmod(td.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    \n",
    "    formatted_str = \"\"\n",
    "    if years:\n",
    "        formatted_str += f\"{years}y \"\n",
    "    if months:\n",
    "        formatted_str += f\"{months}mo \"\n",
    "    if days:\n",
    "        formatted_str += f\"{days}d \"\n",
    "    if hours:\n",
    "        formatted_str += f\"{hours}h \"\n",
    "    if minutes:\n",
    "        formatted_str += f\"{minutes}m \"\n",
    "    if seconds:\n",
    "        formatted_str += f\"{seconds}s \"\n",
    "    \n",
    "\n",
    "    \n",
    "    return formatted_str[:-1] if formatted_str else \"0s\"\n",
    "  \n",
    "# Given a set with binary classes, computes entropy\n",
    "def compute_entropy(dataset_Y):\n",
    "    ones = len(list(filter(lambda classification : classification == 1,dataset_Y)))\n",
    "    zeros = len(list(filter(lambda classification : classification == -1,dataset_Y)))\n",
    "\n",
    "    if(ones == 0 or zeros==0):\n",
    "        return 0\n",
    "    \n",
    "    entropy = ones/len(dataset_Y)*math.log2(1/(ones/len(dataset_Y))) + zeros/len(dataset_Y)*math.log2(1/(zeros/len(dataset_Y)))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "# returns a list of tuples (i,x) where i is the index of the patient in the dataset and x is a timedelta of per quanto tempo abbiamo rilevazioni\n",
    "def find_durations(dataset):\n",
    "    lengths = []\n",
    "    for entry in dataset:\n",
    "        min_t = datetime.max\n",
    "        max_t = datetime.min\n",
    "        for item in entry:\n",
    "            if item[0] < min_t:\n",
    "                min_t = item[0]\n",
    "            elif item[0] > max_t:\n",
    "                max_t = item[0]\n",
    "        lengths.append(max_t-min_t)\n",
    "    return [(i, x) for i, x in enumerate(lengths)]\n",
    "\n",
    "# Find al possible d,l couples that I could split the tree on. Note: Ds are randomly/evenly selected bc otherwise I'd end up with ~36000 pairs...\n",
    "def create_pairs(dataset_X:list,dataset_ST:list,howmany_d =30,random_sampling=False):\n",
    "\n",
    "    #1. Create all labels available from current\n",
    "    labels = set()\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        for item in dataset_X[i]:\n",
    "            if (item[0] > dataset_ST[i]):# Only consider label if it's not been superato\n",
    "                labels.add(item[1])\n",
    "\n",
    "\n",
    "    #2. Find all d\n",
    "    durations = set()\n",
    "    if random_sampling:\n",
    "        durations = set()\n",
    "        for i in range(0,len(dataset_X)):\n",
    "            for item in dataset_X[i]:\n",
    "                if (item[0] > dataset_ST[i]): # Only consider timestamp if it's not been superato\n",
    "                    durations.add(item[0]-dataset_ST[i])\n",
    "        if len(durations)>howmany_d:\n",
    "            durations = random.sample(sorted(durations),howmany_d) # Is this ok?\n",
    "        else:\n",
    "            durations = sorted(durations)\n",
    "    else:\n",
    "        durations = set()\n",
    "        for i in range(0,len(dataset_X)):\n",
    "            for item in dataset_X[i]:\n",
    "                durations.add(item[0]-dataset_ST[i])\n",
    "        delta = max(durations)/(howmany_d+1)\n",
    "        durations = set()\n",
    "        for i in range(1,howmany_d+1):\n",
    "            durations.add(delta*i)\n",
    "\n",
    "    return sorted(list(itertools.product(durations,labels)))\n",
    "\n",
    "# Given a d,l couple, split the dataset and return indexes of true and false entries.\n",
    "def test_event(dataset_X,dl_pair,dataset_ST):\n",
    "    i_T = [] # indexes of entries that have label==l within d time\n",
    "    i_F = [] # indexes of entries that have DON'T HAVE label==l within d time\n",
    "    d,l = dl_pair\n",
    "\n",
    "\n",
    "    #1. Separate entries that satisfy event test from those who don't\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        entry = dataset_X[i]\n",
    "        found=False\n",
    "\n",
    "        for item in entry:\n",
    "            #print(item)\n",
    "            if(found is False and item[0]>=dataset_ST[i] and item[0]<=(dataset_ST[i]+d) and item[1]==l ):\n",
    "                found=True\n",
    "\n",
    "        if(found):\n",
    "            i_T.append(i)\n",
    "        else:\n",
    "            i_F.append(i)\n",
    "    return i_T,i_F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and parsing DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DS loader\n",
      "\tSkipped 46 items for formatting issues in data file. 70 loaded.\n",
      "-- DS builder\n",
      "\t6 entries unsuitable for selected windows.\n",
      "\tFinal dataset size: 64. Classes: 27|37, entropy  0.0\n",
      "--DS state\n",
      "index\tX\tY\tST\t\t\tV \n",
      "0\tl.35\t1\t1991-04-21 09:09:00\tNone\tfile=\n",
      "1\tl.38\t0\t1989-10-10 08:00:00\tNone\tfile=\n",
      "2\tl.49\t1\t1990-07-21 06:43:00\tNone\tfile=\n",
      "3\tl.41\t0\t1990-08-19 17:00:00\tNone\tfile=\n",
      "4\tl.46\t0\t1990-09-01 16:48:00\tNone\tfile=\n",
      "5\tl.38\t0\t1989-03-27 22:00:00\tNone\tfile=\n",
      "6\tl.24\t1\t1990-07-31 12:09:00\tNone\tfile=\n",
      "7\tl.43\t0\t1990-04-22 18:08:00\tNone\tfile=\n",
      "8\tl.34\t0\t1989-02-18 08:00:00\tNone\tfile=\n",
      "9\tl.34\t1\t1990-07-13 09:44:00\tNone\tfile=\n",
      "10\tl.46\t1\t1990-07-22 09:53:00\tNone\tfile=\n",
      "11\tl.58\t1\t1990-09-04 05:53:00\tNone\tfile=\n",
      "12\tl.38\t1\t1991-03-11 18:15:00\tNone\tfile=\n",
      "13\tl.27\t1\t1991-04-13 08:47:00\tNone\tfile=\n",
      "14\tl.22\t1\t1991-05-22 07:24:00\tNone\tfile=\n",
      "15\tl.40\t1\t1990-07-13 09:48:00\tNone\tfile=\n",
      "16\tl.44\t0\t1990-08-18 07:16:00\tNone\tfile=\n",
      "17\tl.45\t1\t1990-09-09 17:23:00\tNone\tfile=\n",
      "18\tl.39\t0\t1991-05-12 06:55:00\tNone\tfile=\n",
      "19\tl.44\t0\t1989-09-03 08:00:00\tNone\tfile=\n",
      "20\tl.36\t0\t1991-03-14 22:05:00\tNone\tfile=\n",
      "21\tl.37\t0\t1991-04-27 23:02:00\tNone\tfile=\n",
      "22\tl.28\t0\t1991-05-28 21:35:00\tNone\tfile=\n",
      "23\tl.13\t0\t1990-07-24 16:00:00\tNone\tfile=\n",
      "24\tl.25\t0\t1988-07-13 08:00:00\tNone\tfile=\n",
      "25\tl.16\t0\t1989-01-29 08:00:00\tNone\tfile=\n",
      "26\tl.16\t0\t1989-11-05 07:00:00\tNone\tfile=\n",
      "27\tl.40\t0\t1990-04-29 07:00:00\tNone\tfile=\n",
      "28\tl.40\t0\t1990-12-18 07:00:00\tNone\tfile=\n",
      "29\tl.39\t0\t1991-05-20 08:00:00\tNone\tfile=\n",
      "30\tl.65\t1\t1990-07-31 18:28:00\tNone\tfile=\n",
      "31\tl.45\t1\t1990-08-23 07:19:00\tNone\tfile=\n",
      "32\tl.52\t1\t1990-09-11 18:00:00\tNone\tfile=\n",
      "33\tl.10\t1\t1991-03-28 15:35:00\tNone\tfile=\n",
      "34\tl.47\t1\t1991-04-26 06:17:00\tNone\tfile=\n",
      "35\tl.55\t0\t1991-06-11 18:05:00\tNone\tfile=\n",
      "36\tl.49\t0\t1991-07-03 12:21:00\tNone\tfile=\n",
      "37\tl.39\t0\t1990-12-16 08:00:00\tNone\tfile=\n",
      "38\tl.41\t0\t1991-06-30 08:00:00\tNone\tfile=\n",
      "39\tl.52\t1\t1990-07-13 11:36:00\tNone\tfile=\n",
      "40\tl.53\t1\t1990-08-26 17:26:00\tNone\tfile=\n",
      "41\tl.51\t1\t1990-09-13 20:56:00\tNone\tfile=\n",
      "42\tl.48\t0\t1991-03-29 18:59:00\tNone\tfile=\n",
      "43\tl.52\t1\t1991-05-04 01:05:00\tNone\tfile=\n",
      "44\tl.39\t1\t1991-07-05 20:47:00\tNone\tfile=\n",
      "45\tl.36\t1\t1990-07-16 11:40:00\tNone\tfile=\n",
      "46\tl.28\t0\t1990-08-03 06:31:00\tNone\tfile=\n",
      "47\tl.45\t0\t1991-03-30 05:56:00\tNone\tfile=\n",
      "48\tl.39\t0\t1991-04-20 11:20:00\tNone\tfile=\n",
      "49\tl.35\t0\t1989-03-28 08:00:00\tNone\tfile=\n",
      "50\tl.39\t0\t1990-07-29 07:00:00\tNone\tfile=\n",
      "51\tl.41\t0\t1991-03-01 08:00:00\tNone\tfile=\n",
      "52\tl.47\t0\t1989-02-03 08:00:00\tNone\tfile=\n",
      "53\tl.22\t0\t1990-06-25 19:16:00\tNone\tfile=\n",
      "54\tl.54\t1\t1990-08-22 05:29:00\tNone\tfile=\n",
      "55\tl.59\t1\t1990-09-04 05:35:00\tNone\tfile=\n",
      "56\tl.56\t0\t1991-04-15 13:08:00\tNone\tfile=\n",
      "57\tl.43\t0\t1991-05-16 20:40:00\tNone\tfile=\n",
      "58\tl.48\t0\t1991-06-12 05:48:00\tNone\tfile=\n",
      "59\tl.45\t1\t1991-07-26 22:04:00\tNone\tfile=\n",
      "60\tl.40\t1\t1989-04-17 06:35:00\tNone\tfile=\n",
      "61\tl.40\t1\t1991-01-01 09:10:00\tNone\tfile=\n",
      "62\tl.12\t0\t1988-03-27 08:00:00\tNone\tfile=\n",
      "63\tl.30\t0\t1989-03-13 08:00:00\tNone\tfile=\n",
      "# entries: 64, entropy= 0.0\n"
     ]
    }
   ],
   "source": [
    "def load_diabetes_dataset(verbose=False)-> list: \n",
    "    folder_path=\"datasets\\\\diabetes\"\n",
    "    dataset = []\n",
    "    errcount=0\n",
    "    print(f\"-- DS loader\")\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if os.path.isfile(file_path)and filename.startswith('data'):\n",
    "            entry=[]\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.readlines()\n",
    "                for line in content:\n",
    "                    item = tuple((line[0:-1] if line.endswith('\\n') else tuple(line)).split(\"\\t\"))\n",
    "\n",
    "                    # If the item is valid, append it to the entry\n",
    "                    try:\n",
    "                        item_f = datetime.strptime(item[0]+\" \"+item[1], \"%m-%d-%Y %H:%M\")\n",
    "                        entry.append((item_f,item[2],item[3]))\n",
    "                    except:\n",
    "                        if(verbose):\n",
    "                            print(f\"\\t[!] Entry {item} in file {filename} is NOT vallid. Skipped!\")\n",
    "                        errcount+=1\n",
    "                # add the entry to the dataset\n",
    "                dataset.append(entry)\n",
    "    print(f\"\\tSkipped {errcount} items for formatting issues in data file. {len(dataset)} loaded.\")\n",
    "    return dataset\n",
    "\n",
    "def compute_datasets(dataset:list,observation_window,waiting_window,prediction_window):\n",
    "\n",
    "    dataset_X = []\n",
    "    dataset_Y = []\n",
    "    \n",
    "    dataset_ST = [entry[0][0] for entry in dataset ]\n",
    "\n",
    "    count_excluded=0\n",
    "\n",
    "    for i in range(0,len(dataset)):\n",
    "        entry = dataset[i]\n",
    "\n",
    "        end_obs = dataset_ST[i]+observation_window\n",
    "        \n",
    "        start_pred = end_obs + waiting_window\n",
    "        end_pred = start_pred + prediction_window\n",
    "\n",
    "        if end_pred < entry[-1][0]:\n",
    "            entry_X = []\n",
    "            found = 0\n",
    "\n",
    "            for item in entry:\n",
    "                if item[0]>= dataset_ST[i] and item[0]<end_obs:\n",
    "                    entry_X.append(item)\n",
    "                if item[0]>=start_pred and item[0]<end_pred:\n",
    "                    # put Y=1 if it has at least one \"65\" entry\n",
    "                    if (item[1]==\"65\"):\n",
    "                        found = 1\n",
    "            dataset_X.append(entry_X)\n",
    "            dataset_Y.append(found)\n",
    "\n",
    "        else:\n",
    "            count_excluded+=1\n",
    "    \n",
    "    dataset_ST = [entry[0][0] for entry in dataset_X ]\n",
    "    dataset_V = [None]*len(dataset_X)\n",
    "    print(f\"-- DS builder\")\n",
    "    print(f\"\\t{count_excluded} entries unsuitable for selected windows.\")\n",
    "    print(f\"\\tFinal dataset size: {len(dataset_X)}. Classes: {sum(1 for c in dataset_Y if c == 1)}|{sum(1 for c in dataset_Y if c == 0)}, entropy {float(compute_entropy(dataset_Y)):4.3}\")    \n",
    "    return dataset_X,dataset_Y,dataset_ST,dataset_V\n",
    "\n",
    "# Quick reload if needed for testing/showcasing purposes\n",
    "def reload_ds():\n",
    "    # prepare\n",
    "    dataset = load_diabetes_dataset(False)\n",
    "    observation_window = timedelta(days=+3)\n",
    "    waiting_window = timedelta(days=+0)\n",
    "    prediction_window = timedelta(days=+10)\n",
    "\n",
    "    dataset_X,dataset_Y, dataset_ST,dataset_V = compute_datasets(dataset,observation_window,waiting_window,prediction_window,)\n",
    "    return dataset_X,dataset_Y, dataset_ST,dataset_V\n",
    "\n",
    "def print_dataset_state(dataset_X,dataset_Y,dataset_ST,dataset_V,dataset_W=None,indexes=None):\n",
    "    print(\"--DS state\")\n",
    "    if (indexes and len(indexes)>len(dataset_X)):\n",
    "        indexes=None\n",
    "    \n",
    "    \n",
    "    print(\"index\\tX\\tY\\tST\\t\\t\\tV\",\"\\tW\" if dataset_W else \"\")\n",
    "\n",
    "    if indexes is None:\n",
    "        indexes = range(0,len(dataset_X))\n",
    "\n",
    "    for i in indexes:\n",
    "        print(f\"{i}\\tl.{len(dataset_X[i])}\\t{dataset_Y[i]}\\t{dataset_ST[i]}\\t{dataset_V[i]}\\t{dataset_W[i] if dataset_W else \"file=\"}\",)\n",
    "    print(f\"# entries: {len(dataset_X)}, entropy={float(compute_entropy(dataset_Y)):4.3}\")\n",
    "    \n",
    "\n",
    "dataset = load_diabetes_dataset(False)\n",
    "\n",
    "observation_window = timedelta(days=+5)\n",
    "waiting_window = timedelta(days=+5)\n",
    "prediction_window = timedelta(days=+15)\n",
    "\n",
    "# Convert raw data into dataset X,Y, etc\n",
    "dataset_X,dataset_Y, dataset_ST,dataset_V = compute_datasets(dataset[:],observation_window,waiting_window,prediction_window)\n",
    "print_dataset_state(dataset_X,dataset_Y, dataset_ST,dataset_V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SequenceTree definition and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treelib import Tree,Node\n",
    "\n",
    "class SequenceTrunk(Tree):\n",
    "\n",
    "    #  -- FUNDAMENTALS\n",
    "\n",
    "    def __init__(self, tree=None, deep=False, node_class=None, identifier=None):\n",
    "        super(SequenceTrunk, self).__init__(tree=tree, deep=deep, node_class=node_class, identifier=identifier)\n",
    "\n",
    "    # Library has a bug that won't show trees correctly unless stdout=False is added.\n",
    "    def display(self):\n",
    "        print(self.show(stdout=False))\n",
    "\n",
    "    #  -- NEW NODE GENERATION\n",
    "\n",
    "    # Let's override original create_node method in order to add new constraints such as child node number and true/false branchs.\n",
    "    def create_node(self, tag=None, identifier=None, parent=None, data=None,branch=None):\n",
    "        \"\"\"\n",
    "        Create a child node for the given @parent node. If ``identifier`` is absent,\n",
    "        a UUID will be generated automatically.\n",
    "        \"\"\"\n",
    "        \n",
    "        new_node = super(SequenceTrunk, self).create_node(tag=tag, parent=parent, data=data)\n",
    "        siblings = super(SequenceTrunk,self).siblings(new_node.identifier)\n",
    "        \n",
    "        if len(super(SequenceTrunk,self).siblings(new_node.identifier))>=2:\n",
    "           raise ValueError(\"Parent node already has maximum number of children\")\n",
    "\n",
    "        if branch in [x.data[\"branch\"] for x in siblings]:\n",
    "           raise ValueError(f\"Parent node already has a {branch} branch\")\n",
    "        \n",
    "        return new_node\n",
    "    \n",
    "    def create_node_event(self,data,parent=None,branch=None,entropy=\"\",size=0,ig=\"\",index=\"\"):\n",
    "        branch_f = \"\" if (branch is None) else str(branch)+\" \"\n",
    "        tag =  f\"\\x1b[32m⬤ {branch_f} ({str(data[1])},{format_timedelta(data[0])})\\x1b[0m - [e={float(entropy):4.2} ig={float(ig):4.2}] [n={size}] {index}\"\n",
    "        data = {\"branch\":branch, \"dl\":(data[0],data[1]),\"entropy\":entropy,\"ig\":ig,\"index\":index}\n",
    "\n",
    "        return     self.create_node(tag,data=data,parent=parent,branch=branch)\n",
    "\n",
    "    def create_node_value(self,label_value,parent=None,branch=None,entropy=\"\",size=0,ig=\"\",index=\"\"):\n",
    "        branch_f = \"\" if (branch is None) else branch+\" \"\n",
    "        tag =  f\"\\x1b[31m■ {branch_f} ({label_value[0]}, {label_value[1]}) \\x1b[0m- [e={float(entropy):2.2} ig={ig:4.2}] [n={size}] {index}\"\n",
    "        data = {\"branch\":branch,\"value\":label_value,\"entropy\":entropy,\"index\":index}\n",
    "        return     self.create_node(tag,data=data,parent=parent,branch=branch)\n",
    "\n",
    "    def create_node_class(self,classification,parent=None,branch=None,entropy=\"\",size=0,index=\"\"):\n",
    "        branch_f = \"\" if (branch is None) else str(branch)+\" \"\n",
    "\n",
    "        tag =  f\"\\x1b[33m◆ {branch_f} {classification} \\x1b[0m- [e={float(entropy):2.2}] \\x1b[33m[n={size}]\\x1b[0m - {index}\"\n",
    "\n",
    "        # If the classsification had \"max length reached\", remove the tag from data\n",
    "        if isinstance(classification, str):\n",
    "            classification = int(classification[0])\n",
    "\n",
    "        data = {\"branch\":branch, \"class\":classification,\"entropy\":entropy,\"index\":index}\n",
    "\n",
    "\n",
    "        return     self.create_node(tag,data=data,parent=parent,branch=branch)\n",
    "\n",
    "    #  -- EVENT TESTING\n",
    "\n",
    "    # Given a pair of duration d and label l, computes its information gain on the dataset if we were to split it according to the sequence tree rules.\n",
    "    def __compute_IG_event(self,dl_pair,dataset_X,dataset_Y,dataset_ST,verbose=False):\n",
    "        #if verbose:\n",
    "        #   print(f\"Computing IG for {dl_pair}\")\n",
    "\n",
    "        entropy_0 = compute_entropy(dataset_Y)\n",
    "\n",
    "        i_T, i_F = test_event(dataset_X,dl_pair,dataset_ST)\n",
    "        \n",
    "        #2. Compute final entropy. first let's generate our new datasets...\n",
    "        dataset_Yt=[ dataset_Y[i] for i in i_T]\n",
    "        dataset_Yf=[ dataset_Y[i] for i in i_F]\n",
    "                \n",
    "        entropy_f = (len(i_T)/len(dataset_X))*compute_entropy(dataset_Yt) + (len(i_F)/len(dataset_X))*compute_entropy(dataset_Yf)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"information gain is {entropy_0-entropy_f} {entropy_0}-> {[ dataset_Y[i] for i in i_T],[ dataset_Y[i] for i in i_F]} {entropy_f}\")\n",
    "            \n",
    "        return(entropy_0-entropy_f,i_T,i_F)\n",
    "\n",
    "    # Given all possible pairs of d,l finds the one with the highest information gain (aka the one I should actually split on)\n",
    "    def __maximize_IG_event(self,dataset_X,dataset_Y, dataset_ST, indexes=None,verbose=False,howmany=30,random_sampling=False):\n",
    "        if indexes is None:\n",
    "            indexes = range(0,len(dataset_X))\n",
    "\n",
    "        \n",
    "        dl_pairs = create_pairs([dataset_X[i] for i in indexes],[dataset_ST[i] for i in indexes],howmany,random_sampling)\n",
    "\n",
    "\n",
    "        igs_list = [(x,self.__compute_IG_event(x,dataset_X,dataset_Y,dataset_ST,verbose)) for x in dl_pairs]\n",
    "        # ogni entry di igs_list è ( (d,l) , (ig,i_T,i_F)   )\n",
    "\n",
    "        max_ig=-1\n",
    "        max_dl=None\n",
    "\n",
    "        for ((d,l),(ig,i_T,i_F)) in igs_list:\n",
    "            if ig > max_ig:\n",
    "                max_ig=ig\n",
    "                max_dl = ((d,l),(ig,i_T,i_F))\n",
    "\n",
    "        \n",
    "        if len(max_dl[1][1])==0 or len(max_dl[1][2])==0:\n",
    "            if verbose:\n",
    "                print(\"Split failed, couldn't find a d,l that separates values :(\")\n",
    "            return None, 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Max IG is in couple d=\",format_timedelta(max_dl[0][0]),\", l=\",max_dl[0][1],\", IG=\",max_dl[1])\n",
    "\n",
    "        max_dl = (max_dl[0],max_dl[1][0])\n",
    "        return max_dl\n",
    "\n",
    "    # Given a d,l couple, split the dataset in two and update starting times and dataset values.\n",
    "    def __perform_event_test(self,max_dl,indexes,dataset_X,dataset_ST,dataset_V,verbose=False):\n",
    "        # divide dataset in t and f...\n",
    "        i_T = []\n",
    "        i_F = []\n",
    "\n",
    "        d,l = max_dl\n",
    "        old_dataset_ST=dataset_ST\n",
    "        \n",
    "        for i in range(0,len(dataset_ST)):\n",
    "            entry = dataset_X[i]\n",
    "\n",
    "            found=False\n",
    "\n",
    "            for item in entry:\n",
    "                #starting from the starting time, see if it exists an item with timestamp < d and label == l\n",
    "                \n",
    "                if(found is False and item[0]>= old_dataset_ST[i] and item[0]<=(old_dataset_ST[i]+d) and item[1]==l ): # If i'm over starting time\n",
    "                    found=True\n",
    "                    dataset_ST[i] = item[0]\n",
    "                    dataset_V[i] = item[2]\n",
    "\n",
    "            if(found):\n",
    "                i_T.append(i)\n",
    "            else:\n",
    "                i_F.append(i)\n",
    "                \n",
    "        return i_T,i_F,dataset_ST,dataset_V\n",
    "\n",
    "    #  -- VALUE TESTING\n",
    "\n",
    "    # Given a dataset, test out all possible values and see which one would lead to the best split. If we have called this function, it means we are in a true branch and all values in dataset_V are already of a single label.\n",
    "    def __maximize_IG_value(self,dataset_V,dataset_Y):\n",
    "        values = set([v for v in dataset_V])\n",
    "        values_ig = []\n",
    "\n",
    "        for v in values:   \n",
    "            entropy_0 = compute_entropy(dataset_Y)\n",
    "            i_T=[]\n",
    "            i_F=[]\n",
    "            for i in range(0,len(dataset_V)):\n",
    "                if dataset_V[i] <= v:\n",
    "                    i_T.append(i)\n",
    "                else:\n",
    "                    i_F.append(i)\n",
    "            dataset_Yt=[ dataset_Y[i] for i in i_T]\n",
    "            dataset_Yf=[ dataset_Y[i] for i in i_F]\n",
    "            entropy_f = (len(i_T)/len(dataset_Y))*compute_entropy(dataset_Yt) + (len(i_F)/len(dataset_Y))*compute_entropy(dataset_Yf)\n",
    "            ig = entropy_0-entropy_f\n",
    "            values_ig.append((v,ig))\n",
    "\n",
    "        return max(values_ig, key=lambda x: x[1])\n",
    "\n",
    "    # Given a d,l couple, split the dataset in two and update dataset values.\n",
    "    def __perform_value_test(self,value,dataset_V):\n",
    "        i_T=[]\n",
    "        i_F=[]\n",
    "        for i in range(0,len(dataset_V)):\n",
    "            if dataset_V[i] <= value:\n",
    "                i_T.append(i)\n",
    "            else:\n",
    "                i_F.append(i)\n",
    "        return i_T,i_F\n",
    "\n",
    "    #  -- FIT ALGORITHM\n",
    "        \n",
    "    def fit(self,dataset_X,dataset_Y,dataset_V,dataset_ST,max_depth:int,parent=None,branch=None,depth=0,indexes=[],verbose=False):\n",
    "        \"\"\"\n",
    "        Implements the fit algorithm as described in the exercise text.\n",
    "\n",
    "        :param list dataset_X: Dataset entries, where each entry is a list of items (time,label,value)\n",
    "        :param list dataset_Y: classes of the dataset, each entry is the class of the dataset entry with the same index\n",
    "        :param list dataset_V: it's a int value if there has been a previous (d,l) test, otherwise it's None\n",
    "        :param list dataset_ST: current starting time (aka, how much of the list I've already read)\n",
    "        :param SequenceTree tree: the SequenceTree tree we're building.\n",
    "        :param int max_depth: how deep can the SequenceTree be before we stop creating new nodes and approximate the result.\n",
    "        :param parent: identifier of the parent node.\n",
    "        :param branch: whether the true node is on the true or false branch.\n",
    "        :param depth: depth of the current node.\n",
    "        :param indexes: original indexes of the entries currently being processed.\n",
    "        :param verbose: prints more info.\n",
    "        \"\"\"\n",
    "        #if ((parent is None) and (event is None) ):\n",
    "        #    raise ValueError('The root of the Sequence Trunk must specify an event.')\n",
    "        \n",
    "        if(parent is None and verbose):\n",
    "            print(\"-- Fit\")\n",
    "        \n",
    "        # BASE CASES:\n",
    "        \n",
    "        # 1. If I have 1 node only or all the classes are the same, or this is the false child of the parent, make a leaf\n",
    "        if  (parent and \"dl\" in parent.data.keys() and branch==\"f\") or (len(dataset_X)==1) or (all(element == dataset_Y[0] for element in dataset_Y)) :\n",
    "            if verbose:\n",
    "                if (parent and \"dl\" in parent.data.keys() and branch==\"f\"):\n",
    "                    print(\"False branch after dl, creating class node..\")\n",
    "                else:\n",
    "                    print(f\"{depth,branch} All items are the same class!\")\n",
    "            self.create_node_class(classification=dataset_Y[0],parent=parent,branch=branch,entropy=compute_entropy(dataset_Y),size=len(dataset_Y),index=indexes)\n",
    "            return         \n",
    "        \n",
    "        # 2. If I've reached the maximum number of tests allowed, create a leaf with the extimation of the class.\n",
    "        elif depth > max_depth:\n",
    "            if (verbose):\n",
    "                print(\"Reached maximum depth!\")\n",
    "            y_1 = sum(1 for c in dataset_Y if c == 1)\n",
    "            y_0 = sum(1 for c in dataset_Y if c == 0)\n",
    "            estimated_class = 1 if y_1>y_0 else 0  \n",
    "            estimated_class = str(estimated_class)+\" MAX DEPTH REACHED\"\n",
    "\n",
    "            self.create_node_class(estimated_class,parent,branch,compute_entropy(dataset_Y),len(dataset_Y),indexes)\n",
    "            return\n",
    "\n",
    "\n",
    "        # INDUCTIVE CASE\n",
    "\n",
    "        # If root, compute test event. Otherwise, do value test.\n",
    "        if (parent is None):\n",
    "            \n",
    "            max_dl, max_dl_ig = self.__maximize_IG_event(dataset_X,dataset_Y,dataset_ST,range(len(dataset_X)),False,howmany=30,random_sampling=False)\n",
    "            node = self.create_node_event(max_dl,parent,branch,compute_entropy(dataset_Y),len(dataset_Y),max_dl_ig)\n",
    "            \n",
    "            i_T, i_F, dataset_ST, dataset_V=self.__perform_event_test(max_dl,indexes,dataset_X,dataset_ST,dataset_V)\n",
    "\n",
    "            self.fit([dataset_X[i] for i in i_T],[dataset_Y[i] for i in i_T],[dataset_V[i] for i in i_T],[dataset_ST[i] for i in i_T],max_depth,node,\"t\",depth+1,[indexes[i] for i in i_T])\n",
    "            self.fit([dataset_X[i] for i in i_F],[dataset_Y[i] for i in i_F],[dataset_V[i] for i in i_F],[dataset_ST[i] for i in i_F],max_depth,node,\"f\",depth+1,[indexes[i] for i in i_F])\n",
    "        else:\n",
    "            max_value, max_value_ig=self.__maximize_IG_value(dataset_V,dataset_Y)\n",
    "            label = parent.data[\"dl\"][1] if \"dl\" in parent.data.keys() else parent.data[\"value\"][0]\n",
    "\n",
    "            # If the separation didn't actually separate a result, just make a class node..\n",
    "            if max_value_ig == 0.0:\n",
    "                classification = -1 if sum(dataset_Y) <= 0.5 else 1\n",
    "                self.create_node_class(classification=classification,parent=parent,branch=branch,entropy=compute_entropy(dataset_Y),size=len(dataset_Y),index=indexes)\n",
    "                return\n",
    "\n",
    "\n",
    "            node = self.create_node_value((label,max_value),parent,branch,compute_entropy(dataset_Y),len(dataset_Y),max_value_ig)\n",
    "            i_T, i_F=self.__perform_value_test(max_value,dataset_V)\n",
    "\n",
    "            self.fit([dataset_X[i] for i in i_T],[dataset_Y[i] for i in i_T],[dataset_V[i] for i in i_T],[dataset_ST[i] for i in i_T],max_depth,node,\"t\",depth+1,[indexes[i] for i in i_T])\n",
    "            self.fit([dataset_X[i] for i in i_F],[dataset_Y[i] for i in i_F],[dataset_V[i] for i in i_F],[dataset_ST[i] for i in i_F],max_depth,node,\"f\",depth+1,[indexes[i] for i in i_F])\n",
    "\n",
    " \n",
    "    #  -- PREDICT ALGORITHM\n",
    "\n",
    "    def __predict_r(self,entry_X,entry_ST,entry_V,node:Node,verbose=False):\n",
    "\n",
    "        if verbose:\n",
    "            print(node)\n",
    "            \n",
    "        # BASE CASE\n",
    "        \n",
    "        if node.is_leaf():\n",
    "            if verbose:\n",
    "                print(\"end\")\n",
    "            return node.data[\"class\"]\n",
    "\n",
    "\n",
    "        # INDUCTIVE CASE    \n",
    "\n",
    "        children = [self.get_node(x) for x in node.successors(self.identifier)]\n",
    "\n",
    "        if \"dl\" in node.data.keys():\n",
    "            if verbose:\n",
    "                print(\"dl test\")\n",
    "            i_T,i_F,entry_ST,entry_V = self.__perform_event_test(node.data[\"dl\"],None,entry_X,entry_ST,entry_V)\n",
    "\n",
    "            branch = \"t\" if i_T else \"f\"\n",
    "            next_node = list(filter(lambda x : x.data[\"branch\"]==branch,children))[0]\n",
    "\n",
    "            return self.__predict_r(entry_X,entry_ST,entry_V,next_node)\n",
    "\n",
    "        elif \"value\" in node.data.keys():\n",
    "            if verbose:\n",
    "                print(\"value test\")\n",
    "            i_T,i_F = self.__perform_value_test(node.data[\"value\"][1],entry_V)\n",
    "            branch = \"t\" if i_T else \"f\"\n",
    "            next_node = list(filter(lambda x : x.data[\"branch\"]==branch,children))[0]\n",
    "\n",
    "            return self.__predict_r(entry_X,entry_ST,entry_V,next_node)\n",
    "\n",
    "    # Given a list of new entries, computes the prediction and returns it. It also prints the confusion matrix!\n",
    "    def predict(self,entry_X,entry_ST,entry_V,entry_Y,verbose=False):\n",
    "        if verbose:\n",
    "            print(\"-- Predict\")\n",
    "        results=[]\n",
    "        root = self.get_node(self.root)\n",
    "\n",
    "\n",
    "        if isinstance(dataset_V,list) and len(dataset_V)==1:\n",
    "            entry_X = [entry_X]\n",
    "            entry_Y = [entry_Y]\n",
    "            entry_ST = [entry_ST]\n",
    "            entry_V = [entry_V]\n",
    "            results.append(self.__predict_r(self.entry_X,entry_ST,entry_V,root),verbose)\n",
    "        else:\n",
    "            for i in range(0,len(entry_ST)):\n",
    "                results.append(self.__predict_r([entry_X[i]],[entry_ST[i]],[entry_V[i]],root,verbose))\n",
    "        \n",
    "        if verbose:\n",
    "            tp = sum(1 for x, y in zip(results, entry_Y) if (x == 1 and y==1))\n",
    "            tn = sum(1 for x, y in zip(results, entry_Y) if (x == -1 and y==-1))\n",
    "            fp = sum(1 for x, y in zip(results, entry_Y) if (x == 1 and y==-1))\n",
    "            fn = sum(1 for x, y in zip(results, entry_Y) if (x == -1 and y==1))\n",
    "\n",
    "            print(f\"\\tP\\tN\\nP\\t{tp}\\t{fn}\\nN\\t{fp}\\t{tn}\")\n",
    "            print(f\"Items: {tp+tn+fp+fn}\")\n",
    "            print(f\"Accuracy: {float((tp+tn)/(tp+tn+fp+fn)):4.3}\")\n",
    "\n",
    "        return results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DS loader\n",
      "\tSkipped 46 items for formatting issues in data file. 70 loaded.\n",
      "-- DS builder\n",
      "\t2 entries unsuitable for selected windows.\n",
      "\tFinal dataset size: 68. Classes: 31|37, entropy  0.0\n",
      "(datetime.timedelta(days=558, seconds=4140), '58')\n",
      "\u001b[32m⬤  (48,2d 17h 1m 1s)\u001b[0m - [e=0.99 ig=0.21] [n=68] \n",
      "├── \u001b[31m■ t  (48, 109) \u001b[0m- [e=0.44 ig=0.053] [n=22] \n",
      "│   ├── \u001b[33m◆ f  1 MAX DEPTH REACHED \u001b[0m- [e=0.57] \u001b[33m[n=15]\u001b[0m - [0, 5, 6, 9, 19, 20, 25, 29, 39, 51, 53, 54, 63, 65, 67]\n",
      "│   └── \u001b[33m◆ t  -1 \u001b[0m- [e=0.0] \u001b[33m[n=7]\u001b[0m - [1, 26, 27, 28, 30, 40, 52]\n",
      "└── \u001b[33m◆ f  1 \u001b[0m- [e=0.95] \u001b[33m[n=46]\u001b[0m - [2, 3, 4, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 31, 32, 33, 34, 35, 36, 37, 38, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 55, 56, 57, 58, 59, 60, 61, 62, 64, 66]\n",
      "\n",
      "-- Predict\n",
      "\tP\tN\n",
      "P\t31\t0\n",
      "N\t30\t7\n",
      "Items: 68\n",
      "Accuracy: 0.559\n"
     ]
    }
   ],
   "source": [
    "# Reload ds\n",
    "dataset_X,dataset_Y, dataset_ST,dataset_V = reload_ds()\n",
    "dataset_Y = [(1 if x==1 else -1) for x in dataset_Y]   \n",
    "\n",
    "#--Fit\n",
    "event=(dataset[0][0][0] - dataset[1][0][0],dataset[0][0][1])\n",
    "print(event)\n",
    "\n",
    "tree = SequenceTrunk()\n",
    "tree.fit(dataset_X,dataset_Y,dataset_V,dataset_ST,max_depth=1,indexes=range(0,len(dataset_X)))\n",
    "if len(tree.all_nodes()) != 0:\n",
    "    print(tree)\n",
    "\n",
    "#--Predict\n",
    "    # Choose which entries to test\n",
    "rangee=len(dataset_V)\n",
    "entry_X,entry_Y, entry_ST,entry_V = dataset_X[0:rangee],dataset_Y[0:rangee], dataset_ST[0:rangee],dataset_V[0:rangee]\n",
    "    # Test'em!\n",
    "predictions = tree.predict(entry_X,entry_ST,entry_V,entry_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up datasets for Boosting task\n",
    "def reload_boosting():\n",
    "    dataset_X,dataset_Y, dataset_ST,dataset_V = reload_ds()\n",
    "    dataset_Y = [(1 if x==1 else -1) for x in dataset_Y] \n",
    "\n",
    "    return  dataset_X,dataset_Y, dataset_ST,dataset_V\n",
    "#print_dataset_state(dataset_X,dataset_Y,dataset_ST,dataset_V,dataset_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_treeerr(tree,dataset_Y):\n",
    "    leaves = tree.leaves()\n",
    "    tree_err = 0\n",
    "    for leaf in leaves:\n",
    "        err = 0\n",
    "        classification_predict = leaf.data[\"class\"]\n",
    "        for index in leaf.data[\"index\"]:\n",
    "            if dataset_Y[index] != classification_predict:\n",
    "                err+=1\n",
    "        tree_err+=err\n",
    "    tree_err = (1/(len(dataset_Y)))*tree_err\n",
    "    return tree_err\n",
    "\n",
    "def flip_tree(tree):\n",
    "    for node in tree.leaves():\n",
    "        node.data[\"class\"] = 1 if node.data[\"class\"]==-1 else -1\n",
    "        node.tag  = node.tag.replace(\" -1 \",\" AAA \")\n",
    "        node.tag  = node.tag.replace(\" 1 \",\" BBB \")\n",
    "        node.tag  = node.tag.replace(\" AAA \",\" 1 \")\n",
    "        node.tag  = node.tag.replace(\" BBB \",\" -1 \")\n",
    "    return\n",
    "\n",
    "def print_weights_status(training_W,training_alpha,training_phi,dataset_Y,alpha,err):\n",
    "    #Stats\n",
    "    print(f\"--- iteration:{len(training_W)-1}\\n - α:{alpha:4.3}\\n - err:{err:4.3}\")\n",
    "\n",
    "    # Header\n",
    "    print(\"i\\t\",end=\"\")\n",
    "    for i in range(0,len(training_W)):\n",
    "        print(f\"w{i}\\t\\t\",end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "    #range_to_print = list(range(0,3))+list(range(len(dataset_Y)-3,len(dataset_Y)))\n",
    "    range_to_print = range(0,len(dataset_Y))\n",
    "    # Weights\n",
    "    for item in range_to_print: #each row in the DS\n",
    "        print(f\"{item}\\t\",end=\"\")\n",
    "        for step in range(0,len(training_W)): # each weight\n",
    "            print(f\"{training_W[step][item]:4.2}\",end=\"\")\n",
    "            if step!=0:\n",
    "                andamento = f\"{\"\\x1b[31m↑\\033[0;37m\" if training_W[step-1][item]<training_W[step][item] else \"\\033[0;32m↓\\033[0;37m\"}\"\n",
    "                if training_W[step-1][item]==training_W[step][item]:\n",
    "                    andamento=\"!\"\n",
    "                print(andamento,end=\"\")\n",
    "            print(\"\\t\\t\",end=\"\")\n",
    "        print(\"\")\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_trunk(dataset_X,dataset_ST,dataset_Y,dataset_V,weights):\n",
    "\n",
    "    # Generate sampled dataset\n",
    "    sampled_X = []\n",
    "    sampled_ST = []\n",
    "    sampled_V = []\n",
    "    sampled_Y = []\n",
    "    sampled_indexes = []\n",
    "    for _ in range(0,100):\n",
    "        # 1. Extract one item. THE LIKELIHOOD MUST BE ACCORDING TO WEIGHTS. \n",
    "        sample = random.choices(range(0,len(dataset_X)), weights=weights, k=1)[0]\n",
    "        sampled_X.append(dataset_X[sample])\n",
    "        sampled_ST.append(dataset_ST[sample])\n",
    "        sampled_V.append(dataset_V[sample])\n",
    "        sampled_Y.append(dataset_Y[sample])\n",
    "        sampled_indexes.append(sample)\n",
    "\n",
    "    # Find tree\n",
    "    tree = SequenceTrunk()\n",
    "    tree.fit(sampled_X,sampled_Y,sampled_V,sampled_ST,1,indexes=sampled_indexes)\n",
    "\n",
    "\n",
    "    tree_err = compute_treeerr(tree,sampled_Y)\n",
    "    flipped = False\n",
    "    if tree_err > 0.5:\n",
    "        flipped = True\n",
    "        flip_tree(tree)\n",
    "        tree_err = compute_treeerr(tree,dataset_Y)\n",
    "    return tree, tree_err, flipped\n",
    "\n",
    "tree, tree_err, flipped = find_best_trunk(dataset_X,dataset_ST,dataset_Y,dataset_V,[1]*len(dataset_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2189665318.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[50], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    tree = SequenceTrunk()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "######################        #d =  sample_item[0]-dataset_ST[sample_entry_i]\n",
    "        #l = sample_item[1]\n",
    "        #find best trunk using this dl combo\n",
    "\n",
    "        tree = SequenceTrunk()\n",
    "        tree.fit(dataset_X,dataset_Y,dataset_V,dataset_ST,max_depth=1,indexes=range(0,len(dataset_X)),event=(d,l))\n",
    "        tree_err = compute_treeerr(tree,dataset_Y)\n",
    "\n",
    "        flipped = False\n",
    "        if tree_err > 0.5:\n",
    "            flipped = True\n",
    "            flip_tree(tree)\n",
    "            tree_err = compute_treeerr(tree,dataset_Y)\n",
    "\n",
    "        trees.append((tree,tree_err,flipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DS loader\n",
      "\tSkipped 46 items for formatting issues in data file. 70 loaded.\n",
      "-- DS builder\n",
      "\t2 entries unsuitable for selected windows.\n",
      "\tFinal dataset size: 68. Classes: 31|37, entropy  0.0\n",
      "--- iteration:1\n",
      " - α:0.171\n",
      " - err:0.441\n",
      "i\tw0\t\tw1\t\t\n",
      "0\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "1\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "2\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "3\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "4\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "5\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "6\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "7\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "8\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "9\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "10\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "11\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "12\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "13\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "14\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "15\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "16\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "17\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "18\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "19\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "20\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "21\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "22\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "23\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "24\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "25\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "26\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "27\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "28\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "29\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "30\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "31\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "32\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "33\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "34\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "35\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "36\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "37\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "38\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "39\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "40\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "41\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "42\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "43\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "44\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "45\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "46\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "47\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "48\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "49\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "50\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "51\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "52\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "53\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "54\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "55\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "56\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "57\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "58\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "59\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "60\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "61\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "62\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "63\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "64\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "65\t0.015\t\t0.013\u001b[0;32m↓\u001b[0;37m\t\t\n",
      "66\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n",
      "67\t0.015\t\t0.017\u001b[31m↑\u001b[0;37m\t\t\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "dataset_X,dataset_Y, dataset_ST,dataset_V= reload_boosting()\n",
    "\n",
    "training_W = [[1/len(dataset_Y)]*len(dataset_Y)]\n",
    "training_phi = [None]\n",
    "training_X = [dataset_X]\n",
    "training_ST = [dataset_ST]\n",
    "training_V = [dataset_V]\n",
    "training_alpha = [None]\n",
    "\n",
    "\n",
    "for iteration in range(0,1):\n",
    "\n",
    "    #Find best trunk \n",
    "    tree, _, _ = find_best_trunk(training_X[iteration],training_ST[iteration],dataset_Y,training_V[iteration],training_W[iteration])\n",
    "    training_phi.append(tree)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    predictions = tree.predict(training_X[iteration],training_ST[iteration],training_V[iteration],dataset_Y)\n",
    "\n",
    "    i_correct=[]\n",
    "    i_mistaken=[]\n",
    "    for i in range(0,len(training_X[iteration])):\n",
    "        if predictions[i]==dataset_Y[i]:\n",
    "            i_correct.append(i)\n",
    "        else:\n",
    "            i_mistaken.append(i)\n",
    "\n",
    "    err = len(i_mistaken)/len(training_X[iteration])\n",
    "\n",
    "    alpha = 0.5*math.log2((1-err)/(err))\n",
    "    training_alpha.append(alpha)\n",
    "\n",
    "\n",
    "    # Compute new weights\n",
    "    training_W1=[None]*len(dataset_Y)\n",
    "    for index in i_correct:\n",
    "        training_W1[index]=training_W[iteration][index]*(math.sqrt(err/(1-err)))\n",
    "    for index in i_mistaken:\n",
    "        training_W1[index]=training_W[iteration][index]*(math.sqrt((1-err)/(err)))\n",
    "    z = sum(training_W1)\n",
    "    training_W1 = [w/z for w in training_W1]\n",
    "\n",
    "\n",
    "    # Update sets TODO\n",
    "    training_W.append(training_W1)\n",
    "    training_X.append(training_X)\n",
    "    training_ST.append(training_ST)\n",
    "    training_V.append(training_V)\n",
    "\n",
    "\n",
    "\n",
    "print_weights_status(training_W,training_alpha,training_phi,dataset_Y,alpha,err)\n",
    "#print_alphas(training_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_alphas(training_alpha):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(2, 2))  # Width, Height in inches\n",
    "    # Adding labels\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('alpha')\n",
    "    plt.plot(training_alpha)\n",
    "\n",
    "    # Displaying the plot\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
