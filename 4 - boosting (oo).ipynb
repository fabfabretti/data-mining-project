{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 4 - Boosting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some helper functions from previous exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import os\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "# Takes a timedelta and prints it in a human-readable format.\n",
    "def format_timedelta(td: timedelta) -> str:\n",
    "    days = td.days\n",
    "    years, days = divmod(days, 365)\n",
    "    months, days = divmod(days, 30)\n",
    "    hours, remainder = divmod(td.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    \n",
    "    formatted_str = \"\"\n",
    "    if years:\n",
    "        formatted_str += f\"{years}y \"\n",
    "    if months:\n",
    "        formatted_str += f\"{months}mo \"\n",
    "    if days:\n",
    "        formatted_str += f\"{days}d \"\n",
    "    if hours:\n",
    "        formatted_str += f\"{hours}h \"\n",
    "    if minutes:\n",
    "        formatted_str += f\"{minutes}m \"\n",
    "    if seconds:\n",
    "        formatted_str += f\"{seconds}s \"\n",
    "    \n",
    "\n",
    "    \n",
    "    return formatted_str[:-1] if formatted_str else \"0s\"\n",
    "  \n",
    "# Given a set with binary classes, computes entropy\n",
    "def compute_entropy(dataset_Y,second_class=-1):\n",
    "    ones = len(list(filter(lambda classification : classification == 1,dataset_Y)))\n",
    "    zeros = len(list(filter(lambda classification : classification == second_class,dataset_Y)))\n",
    "\n",
    "    if(ones == 0 or zeros==0):\n",
    "        return 0\n",
    "    \n",
    "    entropy = ones/len(dataset_Y)*math.log2(1/(ones/len(dataset_Y))) + zeros/len(dataset_Y)*math.log2(1/(zeros/len(dataset_Y)))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "# returns a list of tuples (i,x) where i is the index of the patient in the dataset and x is a timedelta of per quanto tempo abbiamo rilevazioni\n",
    "def find_durations(dataset):\n",
    "    lengths = []\n",
    "    for entry in dataset:\n",
    "        min_t = datetime.max\n",
    "        max_t = datetime.min\n",
    "        for item in entry:\n",
    "            if item[0] < min_t:\n",
    "                min_t = item[0]\n",
    "            elif item[0] > max_t:\n",
    "                max_t = item[0]\n",
    "        lengths.append(max_t-min_t)\n",
    "    return [(i, x) for i, x in enumerate(lengths)]\n",
    "\n",
    "# Find al possible d,l couples that I could split the tree on. Note: Ds are randomly/evenly selected bc otherwise I'd end up with ~36000 pairs...\n",
    "def create_pairs(dataset_X:list,dataset_ST:list,howmany_d =30,random_sampling=False):\n",
    "\n",
    "    #1. Create all labels available from current\n",
    "    labels = set()\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        for item in dataset_X[i]:\n",
    "            if (item[0] > dataset_ST[i]):# Only consider label if it's not been superato\n",
    "                labels.add(item[1])\n",
    "\n",
    "\n",
    "    #2. Find all d\n",
    "    durations = set()\n",
    "    if random_sampling:\n",
    "        durations = set()\n",
    "        for i in range(0,len(dataset_X)):\n",
    "            for item in dataset_X[i]:\n",
    "                if (item[0] > dataset_ST[i]): # Only consider timestamp if it's not been superato\n",
    "                    durations.add(item[0]-dataset_ST[i])\n",
    "        if len(durations)>howmany_d:\n",
    "            durations = random.sample(sorted(durations),howmany_d) # Is this ok?\n",
    "        else:\n",
    "            durations = sorted(durations)\n",
    "    else:\n",
    "        durations = set()\n",
    "        for i in range(0,len(dataset_X)):\n",
    "            for item in dataset_X[i]:\n",
    "                durations.add(item[0]-dataset_ST[i])\n",
    "        delta = max(durations)/(howmany_d+1)\n",
    "        durations = set()\n",
    "        for i in range(1,howmany_d+1):\n",
    "            durations.add(delta*i)\n",
    "\n",
    "    return sorted(list(itertools.product(durations,labels)))\n",
    "\n",
    "# Given a d,l couple, split the dataset and return indexes of true and false entries.\n",
    "def test_event(dataset_X,dl_pair,dataset_ST):\n",
    "    i_T = [] # indexes of entries that have label==l within d time\n",
    "    i_F = [] # indexes of entries that have DON'T HAVE label==l within d time\n",
    "    d,l = dl_pair\n",
    "\n",
    "\n",
    "    #1. Separate entries that satisfy event test from those who don't\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        entry = dataset_X[i]\n",
    "        found=False\n",
    "\n",
    "        for item in entry:\n",
    "            #print(item)\n",
    "            if(found is False and item[0]>=dataset_ST[i] and item[0]<=(dataset_ST[i]+d) and item[1]==l ):\n",
    "                found=True\n",
    "\n",
    "        if(found):\n",
    "            i_T.append(i)\n",
    "        else:\n",
    "            i_F.append(i)\n",
    "    return i_T,i_F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and parsing DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_diabetes_dataset(verbose=False)-> list: \n",
    "    folder_path=\"datasets\\\\diabetes\"\n",
    "    dataset = []\n",
    "    errcount=0\n",
    "    print(f\"-- DS loader\")\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if os.path.isfile(file_path)and filename.startswith('data'):\n",
    "            entry=[]\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.readlines()\n",
    "                for line in content:\n",
    "                    item = tuple((line[0:-1] if line.endswith('\\n') else tuple(line)).split(\"\\t\"))\n",
    "\n",
    "                    # If the item is valid, append it to the entry\n",
    "                    try:\n",
    "                        item_f = datetime.strptime(item[0]+\" \"+item[1], \"%m-%d-%Y %H:%M\")\n",
    "                        entry.append((item_f,item[2],item[3]))\n",
    "                    except:\n",
    "                        if(verbose):\n",
    "                            print(f\"\\t[!] Entry {item} in file {filename} is NOT vallid. Skipped!\")\n",
    "                        errcount+=1\n",
    "                # add the entry to the dataset\n",
    "                dataset.append(entry)\n",
    "    print(f\"\\tSkipped {errcount} items for formatting issues in data file. {len(dataset)} loaded.\")\n",
    "    return dataset\n",
    "\n",
    "def compute_datasets(dataset:list,observation_window,waiting_window,prediction_window):\n",
    "\n",
    "    dataset_X = []\n",
    "    dataset_Y = []\n",
    "    \n",
    "    dataset_ST = [entry[0][0] for entry in dataset ]\n",
    "\n",
    "    count_excluded=0\n",
    "\n",
    "    for i in range(0,len(dataset)):\n",
    "        entry = dataset[i]\n",
    "\n",
    "        end_obs = dataset_ST[i]+observation_window\n",
    "        \n",
    "        start_pred = end_obs + waiting_window\n",
    "        end_pred = start_pred + prediction_window\n",
    "\n",
    "        if end_pred < entry[-1][0]:\n",
    "            entry_X = []\n",
    "            found = 0\n",
    "\n",
    "            for item in entry:\n",
    "                if item[0]>= dataset_ST[i] and item[0]<end_obs:\n",
    "                    entry_X.append(item)\n",
    "                if item[0]>=start_pred and item[0]<end_pred:\n",
    "                    # put Y=1 if it has at least one \"65\" entry\n",
    "                    if (item[1]==\"65\"):\n",
    "                        found = 1\n",
    "            dataset_X.append(entry_X)\n",
    "            dataset_Y.append(found)\n",
    "\n",
    "        else:\n",
    "            count_excluded+=1\n",
    "    \n",
    "    dataset_ST = [entry[0][0] for entry in dataset_X ]\n",
    "    dataset_V = [None]*len(dataset_X)\n",
    "    print(f\"-- DS builder\")\n",
    "    print(f\"\\t{count_excluded} entries unsuitable for selected windows.\")\n",
    "    print(f\"\\tFinal dataset size: {len(dataset_X)}. Classes: {sum(1 for c in dataset_Y if c == 1)}|{sum(1 for c in dataset_Y if c == 0)}, entropy {float(compute_entropy(dataset_Y,0)):4.3}\")    \n",
    "    return dataset_X,dataset_Y,dataset_ST,dataset_V\n",
    "\n",
    "# Quick reload if needed for testing/showcasing purposes\n",
    "def reload_ds():\n",
    "    # prepare\n",
    "    dataset = load_diabetes_dataset(False)\n",
    "    observation_window = timedelta(days=+3)\n",
    "    waiting_window = timedelta(days=+0)\n",
    "    prediction_window = timedelta(days=+10)\n",
    "\n",
    "    dataset_X,dataset_Y, dataset_ST,dataset_V = compute_datasets(dataset,observation_window,waiting_window,prediction_window,)\n",
    "    return dataset_X[:-1],dataset_Y[:-1], dataset_ST[:-1],dataset_V[:-1]\n",
    "# Setting up datasets for Boosting task\n",
    "\n",
    "def reload_boosting():\n",
    "    dataset_X,dataset_Y, dataset_ST,dataset_V = reload_ds()\n",
    "    dataset_Y = [(1 if x==1 else -1) for x in dataset_Y] \n",
    "\n",
    "    return  dataset_X,dataset_Y, dataset_ST,dataset_V\n",
    "#print_dataset_state(dataset_X,dataset_Y,dataset_ST,dataset_V,dataset_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints a table with the number of remaining items, starting time, value if any\n",
    "def print_dataset_state(dataset_X,dataset_Y,dataset_ST,dataset_V,indexes=None):\n",
    "    print(\"--DS state\")\n",
    "    if (indexes and len(indexes)>len(dataset_X)):\n",
    "        indexes=None\n",
    "\n",
    "    # Which items do I print?\n",
    "    if indexes is None:\n",
    "        indexes = range(0,len(dataset_X))\n",
    "    \n",
    "    # Header\n",
    "    print(\"index\\tX\\tY\\tST\\t\\t\\tV\")\n",
    "    # Content\n",
    "    for i in indexes:\n",
    "        uneaten = [entry for entry in dataset_X[i] if entry[0] >= dataset_ST[i]]\n",
    "        print(f\"{i}\\tl.{len(uneaten)}\\t{dataset_Y[i]}\\t{dataset_ST[i]}\\t{dataset_V[i]}\\t\")\n",
    "\n",
    "    # Smol final analysis\n",
    "    print(f\"# entries: {len(dataset_X)}, entropy={float(compute_entropy(dataset_Y)):4.3}\")\n",
    "\n",
    "# Prints a table to show weights (and if weights increase or decrease) for each item and for each iteration, + err/alpha\n",
    "def print_weights_status(training_W,dataset_Y,training_alpha, training_err,range_to_print=None):\n",
    "    \n",
    "    # Which items do I print?\n",
    "    if range_to_print is None:\n",
    "        range_to_print=range(0,len(dataset_Y))\n",
    "\n",
    "\n",
    "    # Header\n",
    "    print(\"i\\t\",end=\"\")\n",
    "    for i in range(0,len(training_W)):\n",
    "        print(f\"w{i}\\t\\t\",end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "    # Content\n",
    "    for item in range_to_print: #each row in the DS\n",
    "        print(f\"{item}\\t\",end=\"\")\n",
    "        for step in range(0,len(training_W)): # each weight\n",
    "            print(f\"{training_W[step][item]:4.2}\",end=\"\")\n",
    "            if step!=0:\n",
    "                andamento = f\"{\"\\x1b[31m↑\\033[0;37m\" if training_W[step-1][item]<training_W[step][item] else \"\\033[0;32m↓\\033[0;37m\"}\"\n",
    "                if training_W[step-1][item]==training_W[step][item]:\n",
    "                    andamento=\"!\"\n",
    "                print(andamento,end=\"\")\n",
    "            print(\"\\t\\t\",end=\"\")\n",
    "        print(\"\")\n",
    "\n",
    "    # Final row to display alpha and error\n",
    "    print(\"α\\t\",end=\"\")\n",
    "    for i in range(0,len(training_alpha)):\n",
    "        print(f\"{training_alpha[i]:4.2}\\t\\t\",end=\"\")\n",
    "    print(\"(done)\")    \n",
    "    print(\"err\\t\",end=\"\")\n",
    "    for i in range(0,len(training_err)):\n",
    "        print(f\"{training_err[i]:4.2}\\t\\t\",end=\"\")\n",
    "    print(\"(done)\")\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted entropy and IG (from professor!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information gain for weighted samples\n",
    "# (weight, label)\n",
    "l = [[(2.3, 1),(4.3, 1), (8.3, 1), (3, -1),( 0.9, 1 ), (99, -1) ],[(99, 1),(8.3, 1), (1.3, -1), (2.3, -1), (9,1)]]\n",
    "import pandas as pd\n",
    "from math import log2\n",
    "\n",
    "\n",
    "def weighted_entropy(lp):\n",
    "    df = pd.DataFrame([{\"w\":x[0], \"l\":x[1]} for x in lp])\n",
    "    try:\n",
    "        return (df.groupby(\"l\").sum() / df[\"w\"].sum())[\"w\"].apply(lambda x: -x * log2(x)).sum(), df[\"w\"].sum()\n",
    "    except:\n",
    "        print(lp)\n",
    "\n",
    "def weighted_ig(split,verbose=False):\n",
    "    e, w = weighted_entropy(split[0] + split[1])\n",
    "    e0, w0  = weighted_entropy(split[0])\n",
    "    e1, w1  = weighted_entropy(split[1])\n",
    "    if verbose:\n",
    "        print(f\"before split: e{e:4.2}, w{w:4.2}\")\n",
    "        print(f\"after split: e0 {e0 :4.2} w0 {w0 :4.2} / e1 {e1 :4.2} w1 {w1 :4.2}\")\n",
    "    return e - (w0 / w) * e0 - (w1 / w) * e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that returns classification \n",
    "def compute_weighted_majority(dataset_Y,weights,indexes,verbose=False):\n",
    "    # Sum weight*prediction\n",
    "    classification=0\n",
    "    for i in indexes:\n",
    "        classification += dataset_Y[i]*weights[i]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"votes for final classification:\",classification/weights[i])\n",
    "\n",
    "    else:\n",
    "        classification = -1 if classification < 0 else 1\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence trunk definition\n",
    "Most of functionality is kept from previous exercise; notable changes are in the \"fit\" function, as tree shape is now predefined and doesn't need recursive construction anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treelib import Tree,Node\n",
    "\n",
    "class SequenceTrunk(Tree):\n",
    "\n",
    "    #  -- FUNDAMENTALS\n",
    "\n",
    "    def __init__(self, tree=None, deep=False, node_class=None, identifier=None):\n",
    "        super(SequenceTrunk, self).__init__(tree=tree, deep=deep, node_class=node_class, identifier=identifier)\n",
    "\n",
    "    # Library has a bug that won't show trees correctly unless stdout=False is added.\n",
    "    def display(self):\n",
    "        print(self.show(stdout=False))\n",
    "\n",
    "    #  -- NEW NODE GENERATION\n",
    "\n",
    "    # Let's override original create_node method in order to add new constraints such as child node number and true/false branchs.\n",
    "    # In order to create nodes, however, we'll always use create_node_* functions\n",
    "    def create_node(self, tag=None, identifier=None, parent=None, data=None,branch=None):\n",
    "        \"\"\"\n",
    "        Create a child node for the given @parent node. If ``identifier`` is absent,\n",
    "        a UUID will be generated automatically.\n",
    "        \"\"\"\n",
    "        \n",
    "        new_node = super(SequenceTrunk, self).create_node(tag=tag, parent=parent, data=data)\n",
    "        siblings = super(SequenceTrunk,self).siblings(new_node.identifier)\n",
    "        \n",
    "        if len(super(SequenceTrunk,self).siblings(new_node.identifier))>=2:\n",
    "           raise ValueError(\"Parent node already has maximum number of children\")\n",
    "\n",
    "        if branch in [x.data[\"branch\"] for x in siblings]:\n",
    "           raise ValueError(f\"Parent node already has a {branch} branch\")\n",
    "        \n",
    "        return new_node\n",
    "    \n",
    "    def create_node_event(self,data,parent=None,branch=None,entropy=\"\",size=0,ig=\"\",index=\"\",weight=0.0):\n",
    "        branch_f = \"\" if (branch is None) else str(branch)+\" \"\n",
    "        tag =  f\"\\x1b[32m⬤ {branch_f} ({str(data[1])},{format_timedelta(data[0])})\\x1b[0m - [e={float(entropy):4.2} ig={float(ig):4.2} w={float(weight):4.2}] [n={size}] {index}\"\n",
    "        data = {\"branch\":branch, \"dl\":(data[0],data[1]),\"entropy\":entropy,\"ig\":ig,\"index\":index,\"weights\":weight}\n",
    "\n",
    "        return     self.create_node(tag,data=data,parent=parent,branch=branch)\n",
    "\n",
    "    def create_node_value(self,label_value,parent=None,branch=None,entropy=\"\",size=0,ig=\"\",index=\"\",weight=0.0):\n",
    "        branch_f = \"\" if (branch is None) else branch+\" \"\n",
    "        tag =  f\"\\x1b[31m■ {branch_f} ({label_value}) \\x1b[0m- [e={float(entropy):2.2} ig={ig:4.2} w={float(weight):4.2}] [n={size}] {index}\"\n",
    "        data = {\"branch\":branch,\"value\":label_value,\"entropy\":entropy,\"index\":index,\"weights\":weight}\n",
    "        return     self.create_node(tag,data=data,parent=parent,branch=branch)\n",
    "\n",
    "    def create_node_class(self,classification,parent=None,branch=None,entropy=\"\",size=0,index=\"\",weight=0.0):\n",
    "        branch_f = \"\" if (branch is None) else str(branch)+\" \"\n",
    "\n",
    "        tag =  f\"\\x1b[33m◆ {branch_f} {classification} \\x1b[0m- [e={float(entropy):2.2} w={float(weight):4.2}] \\x1b[33m[n={size}]\\x1b[0m - {index}\"\n",
    "\n",
    "        # If the classsification had \"max length reached\", remove the tag from data\n",
    "        if isinstance(classification, str):\n",
    "            classification = int(classification[0])\n",
    "\n",
    "        data = {\"branch\":branch, \"class\":classification,\"entropy\":entropy,\"index\":index,\"weights\":weight}\n",
    "\n",
    "\n",
    "        return     self.create_node(tag,data=data,parent=parent,branch=branch)\n",
    "\n",
    "    #  -- EVENT TESTING\n",
    "\n",
    "    # Given a d,l couple, split the dataset in two + update starting times and dataset values.\n",
    "    def perform_event_test(self,max_dl,indexes,dataset_X,ds_ST,dataset_V,verbose=False):\n",
    "        # divide dataset in t and f...\n",
    "        i_T = []\n",
    "        i_F = []\n",
    "\n",
    "        d,l = max_dl\n",
    "        old_dataset_ST=ds_ST.copy()\n",
    "        new_dataset_ST=ds_ST.copy()\n",
    "        new_dataset_V=dataset_V.copy()\n",
    "        \n",
    "        for i in range(0,len(ds_ST)):\n",
    "            entry = dataset_X[i]\n",
    "            found=False\n",
    "\n",
    "            for item in entry:\n",
    "                #starting from the starting time, see if it exists an item with timestamp < d and label == l\n",
    "                if isinstance(old_dataset_ST[i],list):\n",
    "                    old_dataset_ST[i] = old_dataset_ST[i][0]\n",
    "                    \n",
    "                if isinstance(item[0],tuple):\n",
    "                    item[0] = item[0][0]\n",
    "                    \n",
    "\n",
    "                if(found is False and item[0]>= old_dataset_ST[i] and item[0]<=(old_dataset_ST[i]+d) and item[1]==l ): # If i'm over starting time\n",
    "                    found=True\n",
    "                    #print(\"AAAAAAAAAAAA\")\n",
    "                    new_dataset_ST[i] = item[0]\n",
    "                    new_dataset_V[i] = item[2]\n",
    "                    #print(f\"new: {item[2],item[0]}\")\n",
    "\n",
    "            if(found):\n",
    "                i_T.append(i)\n",
    "            else:\n",
    "                i_F.append(i)\n",
    "                \n",
    "        return i_T,i_F,new_dataset_ST,new_dataset_V\n",
    "\n",
    "    #  -- VALUE TESTING\n",
    "\n",
    "    # Given a d,l couple, split the dataset in two \n",
    "    def __perform_value_test(self,value,dataset_V,indexes=None):\n",
    "        i_T=[]\n",
    "        i_F=[]\n",
    "\n",
    "        if indexes == None:\n",
    "            indexes = range(0,len(dataset_V))\n",
    "\n",
    "        for i in indexes:\n",
    "            if dataset_V[i] <= value:\n",
    "                i_T.append(i)\n",
    "            else:\n",
    "                i_F.append(i)\n",
    "        return i_T,i_F\n",
    "\n",
    "    #  -- FIT ALGORITHM\n",
    "        \n",
    "    def fit(self,d,l,v,dataset_X,dataset_Y,dataset_V,dataset_ST,weights,verbose=False):\n",
    "    # Tree shape is now predefined, so fit algorithm doesn't need to be recursive anymore and it just build the same structure node per node.\n",
    "\n",
    "    \n",
    "        # -- ROOT NODE\n",
    "        i_T,i_F = test_event(dataset_X,(d,l),dataset_ST)\n",
    "\n",
    "\n",
    "        # If test didn't divide dataset, this thruple is unusable and no tree is generated.\n",
    "        if len(i_T) == 0 or len(i_F)== 0:\n",
    "            return None\n",
    "\n",
    "        # Otherwise, split dataset and generate root node (event test)\n",
    "        l_T = [(weights[i],dataset_Y[i]) for i in i_T]\n",
    "        l_F = [(weights[i],dataset_Y[i]) for i in i_F]\n",
    "        ig = weighted_ig([l_T,l_F])\n",
    "        parent = self.create_node_event((d,l),None,None,weighted_entropy(l_T+l_F)[0],len(i_T)+len(i_F),ig,\"\",weighted_entropy(l_T+l_F)[1])\n",
    "\n",
    "        # -- FALSE TEST LEAF\n",
    "        \n",
    "        # Find majority class (using weights!) and create root->false->class node.\n",
    "        classification = compute_weighted_majority(dataset_Y,weights,i_F,verbose)\n",
    "        self.create_node_class(classification,parent, \"f\",weighted_entropy(l_F)[0],len(i_F),i_F,weighted_entropy(l_F)[1])\n",
    "        \n",
    "        \n",
    "        # -- TRUE TEST NODE: VALUE TEST\n",
    "        \n",
    "        # Take only items that are true on the event test for following nodes.\n",
    "        new_dataset_V = dataset_V.copy() # python was bugging out a lot, tried this and other fixes, now it works and I'm leaaving this just to be safe\n",
    "        new_dataset_ST = dataset_ST.copy()\n",
    "        _,_,new_dataset_ST,new_dataset_V = self.perform_event_test((d,l),i_T,dataset_X,new_dataset_ST,new_dataset_V)\n",
    "       \n",
    "        # Now separate true event test items based on value test. \n",
    "        ivalue_T, ivalue_F = self.__perform_value_test(v,new_dataset_V,i_T)\n",
    "\n",
    "        # If no split can be found, no tree with predefined structure can be built. Return no tree.\n",
    "        if len(ivalue_T) == 0 or len(ivalue_F) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Else, split the dataset again! \n",
    "        lvalue_T = [(weights[i],dataset_Y[i]) for i in ivalue_T]\n",
    "        lvalue_F = [(weights[i],dataset_Y[i]) for i in ivalue_F]\n",
    "\n",
    "        ig_value = weighted_ig([lvalue_T,lvalue_F])\n",
    "        value_node = self.create_node_value(v,parent,\"t\",weighted_entropy(l_T)[0],len(l_T),ig_value,i_T,weighted_entropy(l_T)[1])\n",
    "\n",
    "        # ---- VALUE LEAVES\n",
    "        classification_value_true = compute_weighted_majority(dataset_Y,weights,ivalue_T,verbose)\n",
    "        classification_value_false = compute_weighted_majority(dataset_Y,weights,ivalue_F,verbose)\n",
    "\n",
    "        self.create_node_class(classification_value_true,value_node, \"t\",weighted_entropy(lvalue_T)[0],len(lvalue_T),ivalue_T,weighted_entropy(lvalue_T)[1])\n",
    "        self.create_node_class(classification_value_false,value_node, \"f\",weighted_entropy(lvalue_F)[0],len(lvalue_F),ivalue_F,weighted_entropy(lvalue_F)[1])\n",
    "\n",
    "        return (new_dataset_ST,new_dataset_V)\n",
    "    \n",
    "    #  -- PREDICT ALGORITHM\n",
    "\n",
    "    def __predict_r(self,entry_X,entry_ST,entry_V,node:Node,verbose=False):\n",
    "\n",
    "        if verbose:\n",
    "            print(node)\n",
    "            \n",
    "        # BASE CASE\n",
    "        \n",
    "        if node.is_leaf():\n",
    "            if verbose:\n",
    "                print(\"end\")\n",
    "            return node.data[\"class\"]\n",
    "\n",
    "\n",
    "        # INDUCTIVE CASE    \n",
    "\n",
    "        children = [self.get_node(x) for x in node.successors(self.identifier)]\n",
    "\n",
    "        if \"dl\" in node.data.keys():\n",
    "            if verbose:\n",
    "                print(\"dl test\")\n",
    "            i_T,i_F,entry_ST,entry_V = self.perform_event_test(node.data[\"dl\"],None,entry_X,entry_ST,entry_V)\n",
    "\n",
    "            branch = \"t\" if i_T else \"f\"\n",
    "            next_node = list(filter(lambda x : x.data[\"branch\"]==branch,children))\n",
    "            next_node = next_node[0]\n",
    "\n",
    "            return self.__predict_r(entry_X,entry_ST,entry_V,next_node)\n",
    "\n",
    "        elif \"value\" in node.data.keys():\n",
    "            if verbose:\n",
    "                print(\"value test\")\n",
    "            try:\n",
    "                i_T,i_F = self.__perform_value_test(node.data[\"value\"],entry_V)\n",
    "            except:\n",
    "                print(node.data)\n",
    "            branch = \"t\" if i_T else \"f\"\n",
    "            next_node = list(filter(lambda x : x.data[\"branch\"]==branch,children))[0]\n",
    "\n",
    "            return self.__predict_r(entry_X,entry_ST,entry_V,next_node)\n",
    "\n",
    "    # Given a list of new entries, computes the prediction and returns it. It also prints the confusion matrix!\n",
    "    def predict(self,entry_X,entry_ST,entry_V,entry_Y=None,verbose=False):\n",
    "        if verbose:\n",
    "            print(\"-- Predict\")\n",
    "        results=[]\n",
    "        root = self.get_node(self.root)\n",
    "\n",
    "\n",
    "        if len(entry_V)==1:\n",
    "            entry_X = [entry_X]\n",
    "            if entry_Y is not None:\n",
    "                entry_Y = [entry_Y]\n",
    "            entry_ST = [entry_ST]\n",
    "            entry_V = [entry_V]\n",
    "            results.append(self.__predict_r(entry_X,entry_ST,entry_V,root,verbose))\n",
    "        else:\n",
    "            for i in range(0,len(entry_ST)):\n",
    "                results.append(self.__predict_r([entry_X[i]],[entry_ST[i]],[entry_V[i]],root,verbose))\n",
    "        \n",
    "        if entry_Y is not None:\n",
    "            if verbose:\n",
    "                tp = sum(1 for x, y in zip(results, entry_Y) if (x == 1 and y==1))\n",
    "                tn = sum(1 for x, y in zip(results, entry_Y) if (x == 0 and y==0))\n",
    "                fp = sum(1 for x, y in zip(results, entry_Y) if (x == 1 and y==0))\n",
    "                fn = sum(1 for x, y in zip(results, entry_Y) if (x == 0 and y==1))\n",
    "\n",
    "                print(f\"\\tP\\tN\\nP\\t{tp}\\t{fn}\\nN\\t{fp}\\t{tn}\")\n",
    "                print(f\"Items: {tp+tn+fp+fn}\")\n",
    "                print(f\"Accuracy: {float((tp+tn)/(tp+tn+fp+fn)):4.3}\")\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training script (Boosting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BoostingClassifier:\n",
    "\n",
    "    def train(self,iTERATIONS, how_many_candidates, dataset):\n",
    "        random.seed()\n",
    "        self.iTERATIONS = iTERATIONS # save number of iterations used for training, in order to easily print data later\n",
    "\n",
    "        # Load dataset\n",
    "        dataset_X,dataset_Y, dataset_ST,dataset_V = dataset\n",
    "\n",
    "        # -- INITIALIZATION\n",
    "        # initialize training data\n",
    "        weights = [1/(len(dataset_Y))]*len(dataset_Y)\n",
    "        training_ST = [dataset_ST]\n",
    "        training_V = [dataset_V]\n",
    "        training_alphas = []\n",
    "        training_errors = []\n",
    "        training_phis= []\n",
    "        training_W = [weights]\n",
    "\n",
    "\n",
    "        # -- TRAINING ALGORITHM\n",
    "\n",
    "        for iteration in range(0,iTERATIONS):\n",
    "\n",
    "            # - FIND BEST TREE\n",
    "\n",
    "            # -- 1. Find all possible thruples of l,d,v\n",
    "            candidates = set()\n",
    "            count=0\n",
    "            for entry_i in range(0,len(dataset_X)):\n",
    "                for item in dataset_X[entry_i]:\n",
    "                    if item[0] > training_ST[iteration][entry_i]:\n",
    "                        candidates.add((item[0]-dataset_ST[entry_i],item[1],item[2]))\n",
    "                        count+=1          \n",
    "            candidates = list(random.sample(list(candidates),how_many_candidates)) # I sample candidates because my computer has weak p energy\n",
    "\n",
    "\n",
    "            # -- 2. For each candidate throuple, create sequence trunk and save error\n",
    "            candidate_trees = []\n",
    "            # for each thruple, compute tree\n",
    "            for candidate in candidates:\n",
    "                st = SequenceTrunk()\n",
    "                result = st.fit(candidate[0],candidate[1],candidate[2],dataset_X,dataset_Y,(training_V[iteration]).copy(),(training_ST[iteration]).copy(),training_W[iteration])\n",
    "                if result is not None:\n",
    "                    # Tree was found: compute WEIGHTED tree error\n",
    "                    err=0\n",
    "                    for leaf in st.leaves():\n",
    "                        for i in leaf.data[\"index\"]:\n",
    "                            if not dataset_Y[i] == leaf.data[\"class\"]: # if prediction was wrong add weighted error :)\n",
    "                                err+=weights[i]\n",
    "                    candidate_trees.append((st,err,candidate,result[0],result[1])) \n",
    "                else:\n",
    "                    # Tree was not found: try again with next thruple\n",
    "                    continue\n",
    "            # If it couldn't find even a single tree, quit where you are and save how many iterations you could do before quitting \n",
    "            if len(candidate_trees)==0:\n",
    "                print(\"COULDNT FIND ANY VIABLE TREES IN ITERATION. QUITTING.\")\n",
    "                iTERATIONS=iteration\n",
    "                break\n",
    "\n",
    "\n",
    "            # -- 3. Find best tree for this iteration\n",
    "            min_tuple = min(candidate_trees, key=lambda x: x[1])\n",
    "            err = min_tuple[1]\n",
    "            best_test = min_tuple[2]\n",
    "            best_tree = min_tuple[0]\n",
    "\n",
    "\n",
    "            # - COMPUTE NEW WEIGHTS AND TRAINING VALUES\n",
    "\n",
    "            # -- 1. Alpha\n",
    "            alpha = 0.5 * math.log2((1-err)/err)\n",
    "\n",
    "            # -- 2. Weights\n",
    "            # --- find correct/mistaken items\n",
    "            i_correct = []\n",
    "            i_mistaken = []\n",
    "            for i_entry in range(0,len(dataset_Y)):\n",
    "                for leaf in min_tuple[0].leaves():\n",
    "                    if i_entry in leaf.data[\"index\"]:\n",
    "                        if dataset_Y[i_entry] == leaf.data[\"class\"]:\n",
    "                            i_correct.append(i_entry)\n",
    "                            continue\n",
    "                        else:\n",
    "                            i_mistaken.append(i_entry)\n",
    "            weights_new = [None]*len(dataset_Y)\n",
    "            # --- Assign new weights according to formula\n",
    "            for i in i_mistaken:  \n",
    "                weights_new[i]   =  training_W[iteration][i] * math.sqrt((1-err)/err)\n",
    "            for i in i_correct:\n",
    "                weights_new[i]   =  training_W[iteration][i] * math.sqrt((err)/(1-err))\n",
    "            z = sum(weights_new)\n",
    "            weights_new = [w/z for w in weights_new]\n",
    "\n",
    "            # If Weights stayed the same, something is wrong (weight deltas so wrong that we got over the machine limit... or just bugs in my algorithm)\n",
    "            for i in range(0,len(dataset_Y)):\n",
    "                if weights_new[i] == weights[i]:\n",
    "                    print(\"!!!\")\n",
    "                    break\n",
    "\n",
    "            # - FINALLY: update verything for next step\n",
    "            training_ST.append(min_tuple[3])\n",
    "            training_V.append(min_tuple[4])\n",
    "            training_W.append(weights_new.copy())\n",
    "            training_errors.append(err)\n",
    "            training_alphas.append(alpha)\n",
    "            training_phis.append(best_tree)\n",
    "\n",
    "            print(f\"Iteration {iteration+1}/{iTERATIONS} done!\")\n",
    "\n",
    "\n",
    "        # Tranining is all done: save it all inside the object for further use\n",
    "        self.training_W = training_W\n",
    "        self.training_ST = training_ST\n",
    "        self.training_V = training_V\n",
    "        self.training_alphas = training_alphas\n",
    "        self.training_errors = training_errors\n",
    "        self.training_phis= training_phis\n",
    "        self.training_W = training_W\n",
    "        self.dataset_Y = dataset_Y\n",
    "        self.dataset_X = dataset_X\n",
    "\n",
    "    # Prints a table with weights for all items, and their trend over iterations\n",
    "    def print_weights_status(self):\n",
    "        if not hasattr(self,\"iTERATIONS\"):\n",
    "            print(\"Classifier has not been trained yet!\")\n",
    "            return\n",
    "        print_weights_status(self.training_W,self.dataset_Y,self.training_alphas,self.training_errors)\n",
    "\n",
    "\n",
    "    def predict(self,test_X,test_V,test_ST):\n",
    "        if not hasattr(self,\"iTERATIONS\"):\n",
    "            print(\"Classifier has not been trained yet!\")\n",
    "            return \n",
    "    \n",
    "        pred = [0]*len(test_V)\n",
    "        for iteration in range(0,self.iTERATIONS):\n",
    "            for entry_i in range(0,len(test_V)):\n",
    "                print(self.training_phis[iteration].predict([test_X[entry_i]],[test_ST[entry_i]],[test_V[entry_i]]))\n",
    "                \n",
    "                \n",
    "                pred[entry_i] = self.training_alphas[iteration]*( self.training_phis[iteration].predict([test_X[entry_i]],[test_ST[entry_i]],[test_V[entry_i]]))[0]\n",
    "\n",
    "                #self.training_alphas[iteration].perform\n",
    "        #pred[entry_i]= 1 if pred[entry_i] > 0 else -1\n",
    "        print(pred)\n",
    "                #    def predict(self,entry_X,entry_ST,entry_V,entry_Y,verbose=False):\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    # Computes error variation for each iteration (aka, it inferences using trees [0] and saves error, [0,1] and saves error, ...)\n",
    "    def compute_training_errors(self):\n",
    "\n",
    "        # If tree has not been trained, quit\n",
    "        if not hasattr(self,\"iTERATIONS\"):\n",
    "            print(\"Classifier has not been trained yet!\")\n",
    "            return\n",
    "        \n",
    "        # - COMPUTATION\n",
    "\n",
    "        errs=[]\n",
    "        predictions=[]\n",
    "        # for each tree in classifier, compute predictions\n",
    "        for tree_i in range(0,len(self.training_phis)):\n",
    "            predictions.append(self.training_phis[tree_i].predict(self.dataset_X,self.training_ST[tree_i],self.training_V[tree_i],self.dataset_Y))\n",
    "\n",
    "\n",
    "        # Now, for each iteration compute error to be saved in array\n",
    "        for iteration in range(0,self.iTERATIONS):\n",
    "            count_err = 0\n",
    "            # compute prediction for each item\n",
    "            for item_i in range(0,len(self.dataset_Y)):\n",
    "                # in order to know prediction, compute prediction by sum of prediction of each tree (up to current iteration) * alpha of that iteration\n",
    "                pred=0\n",
    "                for i in range(0,iteration):\n",
    "                    pred += self.training_alphas[i]*predictions[i][item_i]\n",
    "                pred = -1 if pred < 0 else 1\n",
    "                # If prediction was wrong, add error :(\n",
    "                if pred != self.dataset_Y[item_i]:\n",
    "                    count_err+=1\n",
    "            # convert error to percentage\n",
    "            err = count_err/len(self.dataset_Y)\n",
    "            # save error\n",
    "            errs.append(err)\n",
    "        # save prediction errors into object just in case\n",
    "        self.prediction_errors = errs\n",
    "        return errs\n",
    "    \n",
    "    # Prints a plot that showcases the trend of error with each iteration.\n",
    "    def print_error_trend(self):\n",
    "        plt.figure(figsize=(6,3))  # Width, Height in inches\n",
    "        # Adding labels\n",
    "        plt.xlabel('iterations')\n",
    "        plt.ylabel('alpha/err')\n",
    "        plt.plot(self.training_errors,label=\"error\",color=\"orange\")\n",
    "        plt.plot(self.training_alphas,label=\"alpha\",color=\"green\")\n",
    "        plt.plot(self.prediction_errors,label=\"prediction error\",color=\"#cc0000\",linewidth=3)\n",
    "        _ = plt.legend(loc='upper right')\n",
    "        plt.grid(color='grey', linestyle='--', linewidth=.5)\n",
    "        # Displaying the plot\n",
    "        plt.show()\n",
    "\n",
    "    # Prints two plots, attempting to show in a human-readable way how all weights vary across iterations.\n",
    "    def print_weights_stats(self):\n",
    "        \n",
    "        # 1. Compute range of weights for each iteration\n",
    "\n",
    "        max_weights = []\n",
    "        min_weights = []\n",
    "\n",
    "        for i in range(0,self.iTERATIONS):\n",
    "            max_weights.append(max(self.training_W[i]))\n",
    "            min_weights.append(min(self.training_W[i]))\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "        # 2. Show raw weight trend for all iterations and weights (quite messy!)\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(self.training_W)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Weights value')\n",
    "        plt.title('Weight evolution over iteration')\n",
    "        plt.grid(color='grey', linestyle='--', linewidth=.5)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        # 3. Show boxplot for each iteration + min/max \n",
    "        plt.figure(figsize=(15,5))  # Width, Height in inches\n",
    "        # Adding labels\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('max/min weight')\n",
    "        plt.title('Box plot of weights per iteration')\n",
    "        plt.plot(min_weights,label=\"min\",color=\"green\",linestyle=\"--\")\n",
    "        plt.plot(max_weights,label=\"max\",color=\"orange\",linestyle=\"--\")\n",
    "        \n",
    "        _ = plt.legend(loc='upper right')\n",
    "        plt.grid(color='grey', linestyle='--', linewidth=.5)\n",
    "\n",
    "\n",
    "        plt.boxplot(self.training_W,labels=list(range(0,len(self.training_W))),positions=range(0,len(self.training_W)),vert=True,sym='.', boxprops=dict(color=\"grey\"), whiskerprops=dict(color=\"grey\"))\n",
    "        # Displaying the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DS loader\n",
      "\tSkipped 46 items for formatting issues in data file. 70 loaded.\n",
      "-- DS builder\n",
      "\t2 entries unsuitable for selected windows.\n",
      "\tFinal dataset size: 68. Classes: 31|37, entropy 0.994\n",
      "Iteration 1/5 done!\n",
      "Iteration 2/5 done!\n",
      "Iteration 3/5 done!\n",
      "Iteration 4/5 done!\n",
      "Iteration 5/5 done!\n",
      "-- DS loader\n",
      "\tSkipped 46 items for formatting issues in data file. 70 loaded.\n",
      "-- DS builder\n",
      "\t2 entries unsuitable for selected windows.\n",
      "\tFinal dataset size: 68. Classes: 31|37, entropy 0.994\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[-1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857, 0.6685174936387857]\n"
     ]
    }
   ],
   "source": [
    "boosting_classifier = BoostingClassifier()\n",
    "boosting_classifier.train(5,20,(reload_boosting()))\n",
    "#boosting_classifier.print_weights_status()\n",
    "training_errs = boosting_classifier.compute_training_errors()\n",
    "#boosting_classifier.print_error_trend()\n",
    "#boosting_classifier.print_weights_stats()\n",
    "dataset_X,dataset_Y, dataset_ST,dataset_V = reload_boosting()\n",
    "boosting_classifier.predict(dataset_X,dataset_V,dataset_ST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
