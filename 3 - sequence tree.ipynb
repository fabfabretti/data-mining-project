{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 3 - Sequence tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "def format_timedelta(td: timedelta) -> str:\n",
    "    days = td.days\n",
    "    years, days = divmod(days, 365)\n",
    "    months, days = divmod(days, 30)\n",
    "    hours, remainder = divmod(td.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    \n",
    "    formatted_str = \"\"\n",
    "    if years:\n",
    "        formatted_str += f\"{years}y\"\n",
    "    if months:\n",
    "        formatted_str += f\"{months}mo\"\n",
    "    if days:\n",
    "        formatted_str += f\"{days}d\"\n",
    "    if hours:\n",
    "        formatted_str += f\"{hours}h\"\n",
    "    if minutes:\n",
    "        formatted_str += f\"{minutes}m\"\n",
    "    if seconds:\n",
    "        formatted_str += f\"{seconds}s\"\n",
    "    \n",
    "    return formatted_str if formatted_str else \"0s\"\n",
    "\n",
    "def print_dataset_state(dataset_X,dataset_Y,dataset_ST,dataset_V,indexes=None):\n",
    "    print(\"i\\tX\\t\\tY\\tST\\t\\t\\tV\")\n",
    "    if indexes:\n",
    "        for i in indexes:\n",
    "            print(f\"{i}\\tlen:{len(dataset_X[i])}\\t\\t{dataset_Y[i]}\\t{dataset_ST[i]}\\t{dataset_V[i]}\")\n",
    "    else:\n",
    "        for i in range(0,len(dataset_X)):\n",
    "            print(f\"{i}\\tlen:{len(dataset_X[i])}\\t\\t{dataset_Y[i]}\\t{dataset_ST[i]}\\t{dataset_V[i]}\")\n",
    "    print(f\"# entries: {len(indexes)}\")\n",
    "\n",
    "    \n",
    "def find_durations(dataset):\n",
    "    lengths = []\n",
    "    for entry in dataset:\n",
    "        min_t = datetime.max\n",
    "        max_t = datetime.min\n",
    "        for item in entry:\n",
    "            if item[0] < min_t:\n",
    "                min_t = item[0]\n",
    "            elif item[0] > max_t:\n",
    "                max_t = item[0]\n",
    "        lengths.append(max_t-min_t)\n",
    "\n",
    "    return [format_timedelta(x) for x in lengths]\n",
    "\n",
    "#find_durations(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operazioni preliminari sul DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carichiamo il DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 46 items.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def load_diabetes_dataset(verbose=False,remove_class=False)-> list: \n",
    "    folder_path=\"datasets\\\\diabetes\"\n",
    "    dataset = []\n",
    "    errcount=0\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if os.path.isfile(file_path)and filename.startswith('data'):\n",
    "            entry=[]\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.readlines()\n",
    "                for line in content:\n",
    "                    item = tuple((line[0:-1] if line.endswith('\\n') else tuple(line)).split(\"\\t\"))\n",
    "\n",
    "                    # If the item is valid, append it to the entry\n",
    "                    try:\n",
    "                        item_f = datetime.strptime(item[0]+\" \"+item[1], \"%m-%d-%Y %H:%M\")\n",
    "                        if (not remove_class):\n",
    "                            entry.append((item_f,item[2],item[3]))\n",
    "                        elif (remove_class and item[2]!=\"65\"):\n",
    "                            entry.append((item_f,item[2],item[3]))\n",
    "                    except:\n",
    "                        if(verbose):\n",
    "                            print(f\"[!] Entry {item} in file {filename} is NOT vallid. Skipped!\")\n",
    "                        else:\n",
    "                            errcount+=1\n",
    "                # add the entry to the dataset\n",
    "                dataset.append(entry)\n",
    "    print(f\"Skipped {errcount} items.\")\n",
    "    return dataset\n",
    "dataset = load_diabetes_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A partire dal dataset grezzo generiamo X,Y,ST e V.\n",
    "\n",
    "> Definiamo la classe reale come uno [0,1] che indica se l'evento 65 (65 = Hypoglycemic symptoms) si è verificato in una certa finestra di tempo `event_window` dopo un certo waiting time `waiting_window`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\tX\t\tY\tST\t\t\tV\n",
      "0\tlen:27\t\t0\t1991-04-21 09:09:00\tNone\n",
      "1\tlen:32\t\t0\t1989-10-10 08:00:00\tNone\n",
      "2\tlen:40\t\t0\t1990-07-21 06:43:00\tNone\n",
      "3\tlen:31\t\t0\t1990-08-19 17:00:00\tNone\n",
      "4\tlen:38\t\t0\t1990-09-01 16:48:00\tNone\n",
      "5\tlen:28\t\t0\t1989-04-29 08:00:00\tNone\n",
      "6\tlen:32\t\t0\t1989-03-27 22:00:00\tNone\n",
      "7\tlen:18\t\t1\t1990-07-31 12:09:00\tNone\n",
      "8\tlen:36\t\t0\t1990-04-22 18:08:00\tNone\n",
      "9\tlen:30\t\t0\t1989-02-18 08:00:00\tNone\n",
      "10\tlen:25\t\t1\t1990-07-13 09:44:00\tNone\n",
      "11\tlen:38\t\t1\t1990-07-22 09:53:00\tNone\n",
      "12\tlen:50\t\t1\t1990-09-04 05:53:00\tNone\n",
      "13\tlen:26\t\t0\t1991-03-11 18:15:00\tNone\n",
      "14\tlen:23\t\t0\t1991-04-13 08:47:00\tNone\n",
      "15\tlen:14\t\t0\t1991-05-22 07:24:00\tNone\n",
      "16\tlen:27\t\t1\t1990-07-13 09:48:00\tNone\n",
      "17\tlen:35\t\t1\t1990-08-18 07:16:00\tNone\n",
      "18\tlen:36\t\t0\t1990-09-09 17:23:00\tNone\n",
      "19\tlen:33\t\t0\t1991-05-12 06:55:00\tNone\n",
      "20\tlen:38\t\t0\t1989-09-03 08:00:00\tNone\n",
      "21\tlen:30\t\t0\t1991-03-14 22:05:00\tNone\n",
      "22\tlen:28\t\t0\t1991-04-27 23:02:00\tNone\n",
      "23\tlen:22\t\t0\t1991-05-28 21:35:00\tNone\n",
      "24\tlen:11\t\t0\t1990-07-24 16:00:00\tNone\n",
      "25\tlen:22\t\t0\t1988-07-13 08:00:00\tNone\n",
      "26\tlen:16\t\t0\t1989-01-29 08:00:00\tNone\n",
      "27\tlen:16\t\t0\t1989-11-05 07:00:00\tNone\n",
      "28\tlen:35\t\t0\t1990-04-29 07:00:00\tNone\n",
      "29\tlen:32\t\t0\t1990-12-18 07:00:00\tNone\n",
      "30\tlen:34\t\t0\t1991-05-20 08:00:00\tNone\n",
      "31\tlen:48\t\t0\t1990-07-26 12:27:00\tNone\n",
      "32\tlen:53\t\t0\t1990-07-31 18:28:00\tNone\n",
      "33\tlen:37\t\t1\t1990-08-23 07:19:00\tNone\n",
      "34\tlen:38\t\t1\t1990-09-11 18:00:00\tNone\n",
      "35\tlen:5\t\t0\t1991-03-28 15:35:00\tNone\n",
      "36\tlen:37\t\t0\t1991-04-26 06:17:00\tNone\n",
      "37\tlen:38\t\t1\t1991-06-11 18:05:00\tNone\n",
      "38\tlen:40\t\t0\t1991-07-03 12:21:00\tNone\n",
      "39\tlen:55\t\t0\t1989-11-01 06:30:00\tNone\n",
      "40\tlen:33\t\t0\t1990-12-16 08:00:00\tNone\n",
      "41\tlen:34\t\t0\t1991-06-30 08:00:00\tNone\n",
      "42\tlen:43\t\t0\t1990-07-13 11:36:00\tNone\n",
      "43\tlen:40\t\t0\t1990-08-26 17:26:00\tNone\n",
      "44\tlen:44\t\t1\t1990-09-13 20:56:00\tNone\n",
      "45\tlen:39\t\t0\t1991-03-29 18:59:00\tNone\n",
      "46\tlen:40\t\t0\t1991-05-04 01:05:00\tNone\n",
      "47\tlen:30\t\t0\t1991-07-05 20:47:00\tNone\n",
      "48\tlen:29\t\t0\t1990-07-16 11:40:00\tNone\n",
      "49\tlen:22\t\t0\t1990-08-03 06:31:00\tNone\n",
      "50\tlen:38\t\t0\t1991-03-30 05:56:00\tNone\n",
      "51\tlen:32\t\t0\t1991-04-20 11:20:00\tNone\n",
      "52\tlen:31\t\t0\t1989-03-28 08:00:00\tNone\n",
      "53\tlen:32\t\t0\t1990-07-29 07:00:00\tNone\n",
      "54\tlen:36\t\t0\t1991-03-01 08:00:00\tNone\n",
      "55\tlen:40\t\t0\t1989-02-03 08:00:00\tNone\n",
      "56\tlen:20\t\t0\t1990-06-25 19:16:00\tNone\n",
      "57\tlen:41\t\t0\t1990-08-22 05:29:00\tNone\n",
      "58\tlen:47\t\t0\t1990-09-04 05:35:00\tNone\n",
      "59\tlen:46\t\t0\t1991-04-15 13:08:00\tNone\n",
      "60\tlen:35\t\t0\t1991-05-16 20:40:00\tNone\n",
      "61\tlen:35\t\t0\t1991-06-12 05:48:00\tNone\n",
      "62\tlen:34\t\t0\t1991-07-26 22:04:00\tNone\n",
      "63\tlen:21\t\t0\t1990-01-23 07:30:00\tNone\n",
      "64\tlen:32\t\t0\t1989-04-17 06:35:00\tNone\n",
      "65\tlen:49\t\t0\t1989-11-05 06:50:00\tNone\n",
      "66\tlen:31\t\t0\t1991-01-01 09:10:00\tNone\n",
      "67\tlen:10\t\t0\t1988-03-27 08:00:00\tNone\n",
      "68\tlen:15\t\t0\t1990-08-22 08:58:00\tNone\n",
      "69\tlen:27\t\t0\t1989-03-13 08:00:00\tNone\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m prediction_window \u001b[38;5;241m=\u001b[39m relativedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     60\u001b[0m dataset_X,dataset_Y, dataset_ST,dataset_V \u001b[38;5;241m=\u001b[39m compute_datasets(dataset,observation_window,waiting_window,prediction_window)\n\u001b[1;32m---> 61\u001b[0m \u001b[43mprint_dataset_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_ST\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset_V\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[42], line 34\u001b[0m, in \u001b[0;36mprint_dataset_state\u001b[1;34m(dataset_X, dataset_Y, dataset_ST, dataset_V, indexes)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(dataset_X)):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mlen:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset_X[i])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdataset_Y[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdataset_ST[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdataset_V[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# entries: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def compute_datasets(dataset:list,observation_window,waiting_window,prediction_window)->list:\n",
    "\n",
    "    dataset_X = [0] * len(dataset)\n",
    "    dataset_Y = [0] * len(dataset)\n",
    "\n",
    "    # each entry\n",
    "    for i in range(0,len(dataset)):\n",
    "        entry = dataset[i]\n",
    "\n",
    "        j=0 # this is my iterator on items\n",
    "\n",
    "        end_observation = entry[0][0] + observation_window # this is the timestamp of the first item in the entry\n",
    "        curr_time = entry[0][0]\n",
    "\n",
    "        observation_items = []\n",
    "\n",
    "        #1. Isolate observation window! That's our actual X dataset!\n",
    "        while (curr_time <= end_observation):\n",
    "            #print(f\"{curr_time}  <=  {end_observation}\")\n",
    "            if (entry[j][1]!=\"65\"):\n",
    "                observation_items.append(entry[j])\n",
    "            j+=1\n",
    "            curr_time = entry[j][0]\n",
    "        dataset_X[i] = observation_items \n",
    "\n",
    "        #2. Skip the waiting window\n",
    "        end_waiting =  entry[0][0] + observation_window + waiting_window\n",
    "\n",
    "        while(curr_time <= end_waiting):\n",
    "            j+=1\n",
    "            curr_time = entry[j][0]\n",
    "\n",
    "        #3. Generate prediction window and assign 1 in Y if adverse effect is present, 0 else\n",
    "\n",
    "        while (curr_time <= entry[0][0]+observation_window+waiting_window+prediction_window):\n",
    "            if (entry[j][1]==\"65\"):\n",
    "                dataset_Y[i] = 1\n",
    "                break\n",
    "            j+=1\n",
    "            if (j<len(entry)):\n",
    "                curr_time = entry[j][0]\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    dataset_ST = [0]*len(dataset_X)\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        dataset_ST[i] = dataset_X[i][0][0]\n",
    "    dataset_V = [None]*len(dataset_X)\n",
    "\n",
    "    return dataset_X,dataset_Y,dataset_ST,dataset_V\n",
    "\n",
    "observation_window = relativedelta(days=+4)\n",
    "waiting_window = relativedelta(days=+1)\n",
    "prediction_window = relativedelta(days=+2)\n",
    "\n",
    "dataset_X,dataset_Y, dataset_ST,dataset_V = compute_datasets(dataset,observation_window,waiting_window,prediction_window)\n",
    "print_dataset_state(dataset_X,dataset_Y, dataset_ST,dataset_V )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stampiamo il DS elaborato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\tX\t\tY\tST\t\t\tV\n",
      "0\tlen:27\t\t0\t1991-04-21 09:09:00\tNone\n",
      "1\tlen:32\t\t0\t1989-10-10 08:00:00\tNone\n",
      "2\tlen:40\t\t0\t1990-07-21 06:43:00\tNone\n",
      "3\tlen:31\t\t0\t1990-08-19 17:00:00\tNone\n",
      "4\tlen:38\t\t0\t1990-09-01 16:48:00\tNone\n",
      "5\tlen:28\t\t0\t1989-04-29 08:00:00\tNone\n",
      "6\tlen:32\t\t0\t1989-03-27 22:00:00\tNone\n",
      "7\tlen:18\t\t1\t1990-07-31 12:09:00\tNone\n",
      "8\tlen:36\t\t0\t1990-04-22 18:08:00\tNone\n",
      "9\tlen:30\t\t0\t1989-02-18 08:00:00\tNone\n",
      "10\tlen:25\t\t1\t1990-07-13 09:44:00\tNone\n",
      "11\tlen:38\t\t1\t1990-07-22 09:53:00\tNone\n",
      "12\tlen:50\t\t1\t1990-09-04 05:53:00\tNone\n",
      "13\tlen:26\t\t0\t1991-03-11 18:15:00\tNone\n",
      "14\tlen:23\t\t0\t1991-04-13 08:47:00\tNone\n",
      "15\tlen:14\t\t0\t1991-05-22 07:24:00\tNone\n",
      "16\tlen:27\t\t1\t1990-07-13 09:48:00\tNone\n",
      "17\tlen:35\t\t1\t1990-08-18 07:16:00\tNone\n",
      "18\tlen:36\t\t0\t1990-09-09 17:23:00\tNone\n",
      "19\tlen:33\t\t0\t1991-05-12 06:55:00\tNone\n",
      "20\tlen:38\t\t0\t1989-09-03 08:00:00\tNone\n",
      "21\tlen:30\t\t0\t1991-03-14 22:05:00\tNone\n",
      "22\tlen:28\t\t0\t1991-04-27 23:02:00\tNone\n",
      "23\tlen:22\t\t0\t1991-05-28 21:35:00\tNone\n",
      "24\tlen:11\t\t0\t1990-07-24 16:00:00\tNone\n",
      "25\tlen:22\t\t0\t1988-07-13 08:00:00\tNone\n",
      "26\tlen:16\t\t0\t1989-01-29 08:00:00\tNone\n",
      "27\tlen:16\t\t0\t1989-11-05 07:00:00\tNone\n",
      "28\tlen:35\t\t0\t1990-04-29 07:00:00\tNone\n",
      "29\tlen:32\t\t0\t1990-12-18 07:00:00\tNone\n",
      "30\tlen:34\t\t0\t1991-05-20 08:00:00\tNone\n",
      "31\tlen:48\t\t0\t1990-07-26 12:27:00\tNone\n",
      "32\tlen:53\t\t0\t1990-07-31 18:28:00\tNone\n",
      "33\tlen:37\t\t1\t1990-08-23 07:19:00\tNone\n",
      "34\tlen:38\t\t1\t1990-09-11 18:00:00\tNone\n",
      "35\tlen:5\t\t0\t1991-03-28 15:35:00\tNone\n",
      "36\tlen:37\t\t0\t1991-04-26 06:17:00\tNone\n",
      "37\tlen:38\t\t1\t1991-06-11 18:05:00\tNone\n",
      "38\tlen:40\t\t0\t1991-07-03 12:21:00\tNone\n",
      "39\tlen:55\t\t0\t1989-11-01 06:30:00\tNone\n",
      "40\tlen:33\t\t0\t1990-12-16 08:00:00\tNone\n",
      "41\tlen:34\t\t0\t1991-06-30 08:00:00\tNone\n",
      "42\tlen:43\t\t0\t1990-07-13 11:36:00\tNone\n",
      "43\tlen:40\t\t0\t1990-08-26 17:26:00\tNone\n",
      "44\tlen:44\t\t1\t1990-09-13 20:56:00\tNone\n",
      "45\tlen:39\t\t0\t1991-03-29 18:59:00\tNone\n",
      "46\tlen:40\t\t0\t1991-05-04 01:05:00\tNone\n",
      "47\tlen:30\t\t0\t1991-07-05 20:47:00\tNone\n",
      "48\tlen:29\t\t0\t1990-07-16 11:40:00\tNone\n",
      "49\tlen:22\t\t0\t1990-08-03 06:31:00\tNone\n",
      "50\tlen:38\t\t0\t1991-03-30 05:56:00\tNone\n",
      "51\tlen:32\t\t0\t1991-04-20 11:20:00\tNone\n",
      "52\tlen:31\t\t0\t1989-03-28 08:00:00\tNone\n",
      "53\tlen:32\t\t0\t1990-07-29 07:00:00\tNone\n",
      "54\tlen:36\t\t0\t1991-03-01 08:00:00\tNone\n",
      "55\tlen:40\t\t0\t1989-02-03 08:00:00\tNone\n",
      "56\tlen:20\t\t0\t1990-06-25 19:16:00\tNone\n",
      "57\tlen:41\t\t0\t1990-08-22 05:29:00\tNone\n",
      "58\tlen:47\t\t0\t1990-09-04 05:35:00\tNone\n",
      "59\tlen:46\t\t0\t1991-04-15 13:08:00\tNone\n",
      "60\tlen:35\t\t0\t1991-05-16 20:40:00\tNone\n",
      "61\tlen:35\t\t0\t1991-06-12 05:48:00\tNone\n",
      "62\tlen:34\t\t0\t1991-07-26 22:04:00\tNone\n",
      "63\tlen:21\t\t0\t1990-01-23 07:30:00\tNone\n",
      "64\tlen:32\t\t0\t1989-04-17 06:35:00\tNone\n",
      "65\tlen:49\t\t0\t1989-11-05 06:50:00\tNone\n",
      "66\tlen:31\t\t0\t1991-01-01 09:10:00\tNone\n",
      "67\tlen:10\t\t0\t1988-03-27 08:00:00\tNone\n",
      "68\tlen:15\t\t0\t1990-08-22 08:58:00\tNone\n",
      "69\tlen:27\t\t0\t1989-03-13 08:00:00\tNone\n",
      "# entries: 70\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print_dataset_state(dataset_X,dataset_Y,dataset_ST, dataset_V,range(0,len(dataset_X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((datetime.timedelta(days=4, seconds=72060), '69'), 0.12934341330009297)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "# Find al possible d,l couples that I could split the tree on. Note: Ds are randomly selected bc otherwise I'd end up with ~36000 pairs...\n",
    "def create_pairs(dataset_X:list,dataset_ST:list,all_d=False,howmany_d =30):\n",
    "    #1. Create all labels available from current.\n",
    "    labels = set()\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        for item in dataset_X[i]:\n",
    "            if (item[0] > dataset_ST[i]):# Only consider label if it's not been superato\n",
    "                labels.add(item[1])\n",
    "\n",
    "    durations = set()\n",
    "\n",
    "    #2. Find all d\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        for item in dataset_X[i]:\n",
    "            if (item[0] > dataset_ST[i]): # Only consider timestamp if it's not been superato\n",
    "                durations.add(item[0]-dataset_ST[i])\n",
    "    random.seed(1)\n",
    "    durations = random.sample(sorted(durations),howmany_d) # Is this ok?\n",
    "    return list(itertools.product(durations,labels))\n",
    "\n",
    "# Given a set with binary classes, computes entropy\n",
    "def compute_entropy(dataset_Y):\n",
    "    #print(dataset_Y)\n",
    "    ones = len(list(filter(lambda classification : classification == 1,dataset_Y)))\n",
    "    zeros = len(list(filter(lambda classification : classification == 0,dataset_Y)))\n",
    "\n",
    "    if(ones == 0 or zeros==0):\n",
    "        return 0\n",
    "    #print(f\"count 1: {ones}\") #print(f\"count 0: {zeros}\")\n",
    "    #print(f\"p:{ones/len(dataset_X)}  1-p:{zeros/len(dataset_X)}\")\n",
    "    entropy = ones/len(dataset_Y)*math.log2(1/(ones/len(dataset_Y))) + zeros/len(dataset_Y)*math.log2(1/(zeros/len(dataset_Y)))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "# Given a pair of duration d and label l, computes its information gain on the dataset if we were to split it according to the sequence tree rules.\n",
    "def compute_IG(dl_pair,dataset_X,dataset_Y,verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"Computing IG for {dl_pair}\")\n",
    "    indexes_t = [] # indexes of entries that have label==l within d time\n",
    "    indexes_f = [] # indexes of entries that have DON'T HAVE label==l within d time\n",
    "    d,l = dl_pair\n",
    "\n",
    "    entropy_0 = compute_entropy(dataset_Y)\n",
    "    #print(f\"Initial entropy: {entropy_0}\")\n",
    "\n",
    "    #1. Separate entries that satisfy event test from those who don't\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        entry = dataset_X[i]\n",
    "        found=False\n",
    "\n",
    "        for item in entry:\n",
    "            #starting from the starting time, see if it exists an item with timestamp < d and label == l\n",
    "            if(item[0]> dataset_ST[i]): # If i'm over starting time\n",
    "                if(item[0]< dataset_ST[i]+d and item[2]==l):\n",
    "                    found=True\n",
    "        if(found):\n",
    "            indexes_t.append(i)\n",
    "        else:\n",
    "            indexes_f.append(i)\n",
    "    \n",
    "    #2. Compute final entropy. first let's generate our new datasets...\n",
    "    dataset_Yt=[ dataset_Y[i] for i in indexes_t]\n",
    "    dataset_Yf=[ dataset_Y[i] for i in indexes_f]\n",
    "    #print(dataset_Yt)    print(dataset_Yf)\n",
    "            \n",
    "    entropy_f = (len(indexes_t)/len(dataset_X))*compute_entropy(dataset_Yt) + (len(indexes_f)/len(dataset_X))*compute_entropy(dataset_Yf)\n",
    "\n",
    "    if verbose:\n",
    "        print(indexes_t,indexes_f)\n",
    "        print(dataset_Yt,dataset_Yf)\n",
    "        #print(f\"final entropy: {entropy_f}\")\n",
    "        print(f\"information gain is {entropy_0-entropy_f}\")\n",
    "\n",
    "    return(entropy_0-entropy_f)\n",
    "\n",
    "# Given all possible pairs of d,l finds the one with the highest information gain (aka the one I should actually split on)\n",
    "def maximize_information_gain(dataset_X,dataset_Y, dataset_ST, indexes,verbose=False):\n",
    "    dl_pairs = create_pairs([dataset_X[i] for i in indexes],[dataset_ST[i] for i in indexes])\n",
    "\n",
    "    igs_dict = {x:compute_IG(x,dataset_X,dataset_Y) for x in dl_pairs[:]}\n",
    "\n",
    "    if verbose:\n",
    "        print(igs_dict)\n",
    "        print(f\"Max IG is {igs_dict[max(igs_dict, key=igs_dict.get)]} for entry {max(igs_dict, key=igs_dict.get)}\")\n",
    "\n",
    "    return (max(igs_dict, key=igs_dict.get),igs_dict[max(igs_dict, key=igs_dict.get)])\n",
    "\n",
    "random.seed(1)\n",
    "print(maximize_information_gain(dataset_X,dataset_Y,dataset_ST,range(0,len(dataset_X))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(datetime.timedelta(days=1, seconds=2880), '35')\n",
      "\u001b[32m⬤\u001b[0m [35, 1d48m] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from treelib import Node, Tree\n",
    "class SequenceTree(Tree):\n",
    "    def __init__(self, tree=None, deep=False, node_class=None, identifier=None):\n",
    "        super(SequenceTree, self).__init__(tree=tree, deep=deep, node_class=node_class, identifier=identifier)\n",
    "\n",
    "    def create_node(self, tag=None, identifier=None, parent=None, data=None,branch=None):\n",
    "        \"\"\"\n",
    "        Create a child node for the given @parent node. If ``identifier`` is absent,\n",
    "        a UUID will be generated automatically.\n",
    "        \"\"\"\n",
    "        new_node = super(SequenceTree, self).create_node(tag=tag, identifier=identifier, parent=parent, data=data)\n",
    "        \n",
    "        siblings = super(SequenceTree,self).siblings(new_node.identifier)\n",
    "        \n",
    "\n",
    "        if len(super(SequenceTree,self).siblings(new_node.identifier))>=2:\n",
    "           raise ValueError(\"Parent node already has maximum number of children\")\n",
    "        if branch in [x.data[0] for x in siblings]:\n",
    "           raise ValueError(f\"Parent node already has a {branch} branch\")\n",
    "        return new_node\n",
    "    \n",
    "    def display(self):\n",
    "        print(self.show(stdout=False))\n",
    "    \n",
    "    def create_node_event(self,data,parent=None,branch=None):\n",
    "        branch_f = \"\" if (branch is None) else branch+\" \"\n",
    "        tag =  \"\\x1b[32m⬤\\x1b[0m\" +\" \"+ branch_f +\"[\" + str(data[1])+\", \" + format_timedelta(data[0])+ \"] \"  \n",
    "\n",
    "        return     self.create_node(tag,data=(branch,{\"classification\":data}),parent=parent,branch=branch)\n",
    "\n",
    "\n",
    "def create_node_event(tree,data,parent=None,branch=None):\n",
    "    branch_f = \"\" if (branch is None) else branch+\" \"\n",
    "    tag =  \"\\x1b[32m⬤\\x1b[0m\" +\" \"+ branch_f +\"[\" + str(data[1])+\", \" + format_timedelta(data[0])+ \"] \"  \n",
    "\n",
    "    return     tree.create_node(tag,data=(branch,{\"classification\":data}),parent=parent,branch=branch)\n",
    "\n",
    "def create_node_class(tree,classification,parent=None,branch=None):\n",
    "    branch_f = \"\" if (branch is None) else branch+\" \"\n",
    "    tag =  \"\\x1b[33m◆\\x1b[0m\" +\" \"+ branch_f + \" \" + str(classification)  \n",
    "\n",
    "    return     tree.create_node(tag,data=(branch,{\"classification\":classification}),parent=parent,branch=branch)\n",
    "\n",
    "def create_node_value(tree,data,parent=None,branch=None):\n",
    "    branch_f = \"\" if (branch is None) else branch+\" \"\n",
    "    tag =  \"\\x1b[31m■\\x1b[0m\" +\" \"+ branch_f +\"[\" + str(data[1])+\", \" + format_timedelta(data[0])+ \"] \"  \n",
    "\n",
    "    print(data)\n",
    "    return     tree.create_node(tag,data=(branch,{\"value\":data}),parent=parent,branch=branch)\n",
    "\n",
    "\n",
    "# Example tree:\n",
    "tree = SequenceTree()\n",
    "\n",
    "# Each node has:\n",
    "# - Tag (string to be printed in the tree)\n",
    "# - Data (the actual info) is a tuple with:\n",
    "#  - branch --> \"t\" or \"f\"\n",
    "#  - A dict, which changes according to the type of node:\n",
    "#     . event: {\"event\":(d,l)}\n",
    "#     . value: {\"value\":v}\n",
    "#     . classification (leaf): {\"classification\":class}\n",
    "max_dl,ig=maximize_information_gain(dataset_X,dataset_Y,dataset_ST)\n",
    "print(max_dl)\n",
    "\n",
    "root = create_node_event(tree,max_dl)\n",
    "#a = create_node_event(tree,max_dl,parent=root.identifier,branch=\"t\")\n",
    "#b = create_node_value(tree,max_dl,parent=root.identifier,branch=\"f\")\n",
    "#c = create_node_event(tree,max_dl,parent=b,branch=\"f\")\n",
    "\n",
    "\n",
    "\n",
    "print(tree.show(stdout=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((datetime.timedelta(days=1, seconds=53940), '35'), 0.06360468204456848)\n",
      "Skipped 46 items.\n",
      "-- f i r s t  s t e p --\n",
      "((datetime.timedelta(days=4, seconds=17160), '36'), 0.0859408892405984)\n",
      "(datetime.timedelta(days=4, seconds=17160), '36')\n",
      "Performing test (datetime.timedelta(days=4, seconds=17160), '36')\n",
      "i\tX\t\tY\tST\t\t\tV\n",
      "21\tlen:55\t\t0\t1991-03-14 22:06:00\t5\n",
      "# entries: 1\n",
      "i\tX\t\tY\tST\t\t\tV\n",
      "0\tlen:46\t\t1\t1991-04-21 09:09:00\tNone\n",
      "1\tlen:56\t\t0\t1989-10-10 08:00:00\tNone\n",
      "2\tlen:64\t\t1\t1990-07-21 06:43:00\tNone\n",
      "3\tlen:55\t\t0\t1990-08-19 17:00:00\tNone\n",
      "4\tlen:61\t\t0\t1990-09-01 16:48:00\tNone\n",
      "5\tlen:51\t\t0\t1989-04-29 08:00:00\tNone\n",
      "6\tlen:54\t\t0\t1989-03-27 22:00:00\tNone\n",
      "7\tlen:33\t\t1\t1990-07-31 12:09:00\tNone\n",
      "8\tlen:57\t\t0\t1990-04-22 18:08:00\tNone\n",
      "9\tlen:51\t\t0\t1989-02-18 08:00:00\tNone\n",
      "10\tlen:55\t\t1\t1990-07-13 09:44:00\tNone\n",
      "11\tlen:68\t\t1\t1990-07-22 09:53:00\tNone\n",
      "12\tlen:76\t\t1\t1990-09-04 05:53:00\tNone\n",
      "13\tlen:51\t\t1\t1991-03-11 18:15:00\tNone\n",
      "14\tlen:33\t\t1\t1991-04-13 08:47:00\tNone\n",
      "15\tlen:31\t\t1\t1991-05-22 07:24:00\tNone\n",
      "16\tlen:58\t\t1\t1990-07-13 09:48:00\tNone\n",
      "17\tlen:63\t\t1\t1990-08-18 07:16:00\tNone\n",
      "18\tlen:60\t\t1\t1990-09-09 17:23:00\tNone\n",
      "19\tlen:50\t\t0\t1991-05-12 06:55:00\tNone\n",
      "20\tlen:65\t\t0\t1989-09-03 08:00:00\tNone\n",
      "22\tlen:51\t\t0\t1991-04-27 23:02:00\tNone\n",
      "23\tlen:37\t\t0\t1991-05-28 21:35:00\tNone\n",
      "24\tlen:19\t\t1\t1990-07-24 16:00:00\tNone\n",
      "25\tlen:37\t\t0\t1988-07-13 08:00:00\tNone\n",
      "26\tlen:16\t\t0\t1989-01-29 08:00:00\tNone\n",
      "27\tlen:27\t\t0\t1989-11-05 07:00:00\tNone\n",
      "28\tlen:59\t\t0\t1990-04-29 07:00:00\tNone\n",
      "29\tlen:56\t\t0\t1990-12-18 07:00:00\tNone\n",
      "30\tlen:58\t\t0\t1991-05-20 08:00:00\tNone\n",
      "31\tlen:89\t\t0\t1990-07-26 12:27:00\tNone\n",
      "32\tlen:84\t\t1\t1990-07-31 18:28:00\tNone\n",
      "33\tlen:62\t\t1\t1990-08-23 07:19:00\tNone\n",
      "34\tlen:66\t\t1\t1990-09-11 18:00:00\tNone\n",
      "35\tlen:19\t\t1\t1991-03-28 15:35:00\tNone\n",
      "36\tlen:61\t\t1\t1991-04-26 06:17:00\tNone\n",
      "37\tlen:69\t\t1\t1991-06-11 18:05:00\tNone\n",
      "38\tlen:69\t\t0\t1991-07-03 12:21:00\tNone\n",
      "39\tlen:94\t\t0\t1989-11-01 06:30:00\tNone\n",
      "40\tlen:57\t\t0\t1990-12-16 08:00:00\tNone\n",
      "41\tlen:59\t\t0\t1991-06-30 08:00:00\tNone\n",
      "42\tlen:66\t\t0\t1990-07-13 11:36:00\tNone\n",
      "43\tlen:72\t\t1\t1990-08-26 17:26:00\tNone\n",
      "44\tlen:74\t\t0\t1990-09-13 20:56:00\tNone\n",
      "45\tlen:67\t\t1\t1991-03-29 18:59:00\tNone\n",
      "46\tlen:68\t\t0\t1991-05-04 01:05:00\tNone\n",
      "47\tlen:55\t\t0\t1991-07-05 20:47:00\tNone\n",
      "48\tlen:45\t\t0\t1990-07-16 11:40:00\tNone\n",
      "49\tlen:42\t\t0\t1990-08-03 06:31:00\tNone\n",
      "50\tlen:61\t\t1\t1991-03-30 05:56:00\tNone\n",
      "51\tlen:55\t\t0\t1991-04-20 11:20:00\tNone\n",
      "52\tlen:52\t\t0\t1989-03-28 08:00:00\tNone\n",
      "53\tlen:51\t\t0\t1990-07-29 07:00:00\tNone\n",
      "54\tlen:61\t\t0\t1991-03-01 08:00:00\tNone\n",
      "55\tlen:70\t\t0\t1989-02-03 08:00:00\tNone\n",
      "56\tlen:21\t\t0\t1990-06-25 19:16:00\tNone\n",
      "57\tlen:75\t\t0\t1990-08-22 05:29:00\tNone\n",
      "58\tlen:76\t\t1\t1990-09-04 05:35:00\tNone\n",
      "59\tlen:78\t\t0\t1991-04-15 13:08:00\tNone\n",
      "60\tlen:57\t\t0\t1991-05-16 20:40:00\tNone\n",
      "61\tlen:69\t\t0\t1991-06-12 05:48:00\tNone\n",
      "62\tlen:60\t\t1\t1991-07-26 22:04:00\tNone\n",
      "63\tlen:32\t\t0\t1990-01-23 07:30:00\tNone\n",
      "64\tlen:61\t\t0\t1989-04-17 06:35:00\tNone\n",
      "65\tlen:84\t\t1\t1989-11-05 06:50:00\tNone\n",
      "66\tlen:58\t\t1\t1991-01-01 09:10:00\tNone\n",
      "67\tlen:16\t\t0\t1988-03-27 08:00:00\tNone\n",
      "68\tlen:27\t\t0\t1990-08-22 08:58:00\tNone\n",
      "69\tlen:45\t\t0\t1989-03-13 08:00:00\tNone\n",
      "# entries: 69\n",
      "\u001b[32m⬤\u001b[0m [36, 4d4h46m] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare\n",
    "\n",
    "random.seed(1)\n",
    "print(maximize_information_gain(dataset_X,dataset_Y,dataset_ST,range(0,len(dataset_X))))\n",
    "\n",
    "random.seed(1)\n",
    "ds = load_diabetes_dataset()\n",
    "\n",
    "observation_window = relativedelta(days=+7)\n",
    "waiting_window = relativedelta(days=+0)\n",
    "prediction_window = relativedelta(days=+5)\n",
    "\n",
    "dataset_X,dataset_Y,dataset_ST, dataset_V = compute_datasets(ds,observation_window,waiting_window,prediction_window)\n",
    "\n",
    "\n",
    "sequencetree = SequenceTree()\n",
    "\n",
    "# run\n",
    "\n",
    "print(\"-- f i r s t  s t e p --\")\n",
    "\n",
    "\n",
    "def perform_event_test(event_test,indexes,dataset_X,dataset_ST,dataset_V):\n",
    "    print(f\"Performing test {event_test}\")\n",
    "    # divide dataset in t and f...\n",
    "    i_T = []\n",
    "    i_F = []\n",
    "    for i in indexes:\n",
    "        entry = dataset_X[i]\n",
    "        # check if entry satisfies test\n",
    "        testresult = False\n",
    "        for item in entry:\n",
    "            if((testresult is False) and (item[0] > dataset_ST[i]) and (item[0] < dataset_ST[i]+max_dl[0]) and (item[1]==max_dl[1])):\n",
    "                testresult = True\n",
    "                dataset_ST[i] = item[0]\n",
    "                dataset_V[i] = item[2]\n",
    "        if testresult:\n",
    "            i_T.append(i)\n",
    "        else:\n",
    "            i_F.append(i)\n",
    "    return(i_T,i_F)\n",
    "\n",
    "print(maximize_information_gain(dataset_X,dataset_Y,dataset_ST,range(0,len(dataset_X))))\n",
    "\n",
    "\n",
    "max_dl, ig = maximize_information_gain(dataset_X,dataset_Y,dataset_ST,range(0,len(dataset_X)))\n",
    "print(max_dl)\n",
    "\n",
    "\n",
    "rootnode = sequencetree.create_node_event(max_dl,None,None)\n",
    "\n",
    "\n",
    "i_T,i_F = perform_event_test(max_dl,range(len(dataset_X)),dataset_X,dataset_ST,dataset_V)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print_dataset_state(dataset_X,dataset_Y,dataset_ST, dataset_V,i_T)\n",
    "print_dataset_state(dataset_X,dataset_Y,dataset_ST, dataset_V,i_F)\n",
    "sequencetree.display()\n",
    "\n",
    "\n",
    "\n",
    "# ramo true\n",
    "ds_X_t=[dataset_X[i] for i in i_T]\n",
    "ds_X_t=[dataset_X[i] for i in i_T]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
