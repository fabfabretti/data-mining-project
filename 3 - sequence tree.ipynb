{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 3 - Sequence tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "\n",
    "# Some helper functions!\n",
    "\n",
    "def format_timedelta(td: timedelta) -> str:\n",
    "    days = td.days\n",
    "    years, days = divmod(days, 365)\n",
    "    months, days = divmod(days, 30)\n",
    "    hours, remainder = divmod(td.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    \n",
    "    formatted_str = \"\"\n",
    "    if years:\n",
    "        formatted_str += f\"{years}y \"\n",
    "    if months:\n",
    "        formatted_str += f\"{months}mo \"\n",
    "    if days:\n",
    "        formatted_str += f\"{days}d \"\n",
    "    if hours:\n",
    "        formatted_str += f\"{hours}h \"\n",
    "    if minutes:\n",
    "        formatted_str += f\"{minutes}m \"\n",
    "    if seconds:\n",
    "        formatted_str += f\"{seconds}s \"\n",
    "    \n",
    "\n",
    "    \n",
    "    return formatted_str[:-1] if formatted_str else \"0s\"\n",
    "\n",
    "    \n",
    "def find_durations(dataset):\n",
    "    lengths = []\n",
    "    for entry in dataset:\n",
    "        min_t = datetime.max\n",
    "        max_t = datetime.min\n",
    "        for item in entry:\n",
    "            if item[0] < min_t:\n",
    "                min_t = item[0]\n",
    "            elif item[0] > max_t:\n",
    "                max_t = item[0]\n",
    "        lengths.append(max_t-min_t)\n",
    "    return [(i, x) for i, x in enumerate(lengths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operazioni preliminari sul DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carichiamo il DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DS loader\n",
      "\tSkipped 46 items for formatting issues in data file. 70 loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def load_diabetes_dataset(verbose=False)-> list: \n",
    "    folder_path=\"datasets\\\\diabetes\"\n",
    "    dataset = []\n",
    "    errcount=0\n",
    "    print(f\"-- DS loader\")\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if os.path.isfile(file_path)and filename.startswith('data'):\n",
    "            entry=[]\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.readlines()\n",
    "                for line in content:\n",
    "                    item = tuple((line[0:-1] if line.endswith('\\n') else tuple(line)).split(\"\\t\"))\n",
    "\n",
    "                    # If the item is valid, append it to the entry\n",
    "                    try:\n",
    "                        item_f = datetime.strptime(item[0]+\" \"+item[1], \"%m-%d-%Y %H:%M\")\n",
    "                        entry.append((item_f,item[2],item[3]))\n",
    "                    except:\n",
    "                        if(verbose):\n",
    "                            print(f\"\\t[!] Entry {item} in file {filename} is NOT vallid. Skipped!\")\n",
    "                        errcount+=1\n",
    "                # add the entry to the dataset\n",
    "                dataset.append(entry)\n",
    "    print(f\"\\tSkipped {errcount} items for formatting issues in data file. {len(dataset)} loaded.\")\n",
    "    return dataset\n",
    "\n",
    "dataset = load_diabetes_dataset(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A partire dal dataset grezzo generiamo X,Y,ST e V.\n",
    "\n",
    "> Definiamo la classe reale come uno [0,1] che indica se l'evento 65 (65 = Hypoglycemic symptoms) si è verificato in una certa finestra di tempo `event_window` dopo un certo waiting time `waiting_window`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DS loader\n",
      "\tSkipped 46 items for formatting issues in data file. 70 loaded.\n",
      "-- DS builder\n",
      "\t6 entries unsuitable for selected windows.\n",
      "\tFinal dataset size: 64. Classes: 27|37, entropy 0.982\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def compute_datasets(dataset:list,observation_window,waiting_window,prediction_window)->list:\n",
    "\n",
    "    dataset_X = []\n",
    "    dataset_Y = []\n",
    "    dataset_F = []\n",
    "\n",
    "    dataset_ST = [entry[0][0] for entry in dataset ]\n",
    "\n",
    "    count_excluded=0\n",
    "\n",
    "    for i in range(0,len(dataset)):\n",
    "        entry = dataset[i]\n",
    "\n",
    "        end_obs = dataset_ST[i]+observation_window\n",
    "        \n",
    "        start_pred = end_obs + waiting_window\n",
    "        end_pred = start_pred + prediction_window\n",
    "\n",
    "        if end_pred < entry[-1][0]:\n",
    "            entry_X = []\n",
    "            found = 0\n",
    "\n",
    "            for item in entry:\n",
    "                if item[0]>= dataset_ST[i] and item[0]<end_obs:\n",
    "                    entry_X.append(item)\n",
    "                if item[0]>=start_pred and item[0]<end_pred:\n",
    "                    # put Y=1 if it has at least one \"65\" entry\n",
    "                    if (item[1]==\"65\"):\n",
    "                        found = 1\n",
    "            dataset_X.append(entry_X)\n",
    "            dataset_Y.append(found)\n",
    "\n",
    "        else:\n",
    "            count_excluded+=1\n",
    "    \n",
    "    dataset_ST = [entry[0][0] for entry in dataset_X ]\n",
    "    dataset_V = [None]*len(dataset_X)\n",
    "    print(f\"-- DS builder\")\n",
    "    print(f\"\\t{count_excluded} entries unsuitable for selected windows.\")\n",
    "    print(f\"\\tFinal dataset size: {len(dataset_X)}. Classes: {sum(1 for c in dataset_Y if c == 1)}|{sum(1 for c in dataset_Y if c == 0)}, entropy {compute_entropy(dataset_Y):4.3}\")    \n",
    "    return dataset_X,dataset_Y,dataset_ST,dataset_V,dataset_F\n",
    "\n",
    "dataset = load_diabetes_dataset()\n",
    "observation_window = timedelta(days=+5)\n",
    "waiting_window = timedelta(days=+5)\n",
    "prediction_window = timedelta(days=+15)\n",
    "\n",
    "# Convert raw data into dataset X,Y, etc\n",
    "dataset_X,dataset_Y, dataset_ST,dataset_V, dataset_F = compute_datasets(dataset[:],observation_window,waiting_window,prediction_window)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stampiamo il DS elaborato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--DS state\n",
      "index\tX\tY\tST\t\t\tV\n",
      "0\tl.35\t1\t1991-04-21 09:09:00\tNone\n",
      "1\tl.38\t0\t1989-10-10 08:00:00\tNone\n",
      "2\tl.49\t1\t1990-07-21 06:43:00\tNone\n",
      "3\tl.41\t0\t1990-08-19 17:00:00\tNone\n",
      "4\tl.46\t0\t1990-09-01 16:48:00\tNone\n",
      "5\tl.38\t0\t1989-03-27 22:00:00\tNone\n",
      "6\tl.24\t1\t1990-07-31 12:09:00\tNone\n",
      "7\tl.43\t0\t1990-04-22 18:08:00\tNone\n",
      "8\tl.34\t0\t1989-02-18 08:00:00\tNone\n",
      "9\tl.34\t1\t1990-07-13 09:44:00\tNone\n",
      "10\tl.46\t1\t1990-07-22 09:53:00\tNone\n",
      "11\tl.58\t1\t1990-09-04 05:53:00\tNone\n",
      "12\tl.38\t1\t1991-03-11 18:15:00\tNone\n",
      "13\tl.27\t1\t1991-04-13 08:47:00\tNone\n",
      "14\tl.22\t1\t1991-05-22 07:24:00\tNone\n",
      "15\tl.40\t1\t1990-07-13 09:48:00\tNone\n",
      "16\tl.44\t0\t1990-08-18 07:16:00\tNone\n",
      "17\tl.45\t1\t1990-09-09 17:23:00\tNone\n",
      "18\tl.39\t0\t1991-05-12 06:55:00\tNone\n",
      "19\tl.44\t0\t1989-09-03 08:00:00\tNone\n",
      "20\tl.36\t0\t1991-03-14 22:05:00\tNone\n",
      "21\tl.37\t0\t1991-04-27 23:02:00\tNone\n",
      "22\tl.28\t0\t1991-05-28 21:35:00\tNone\n",
      "23\tl.13\t0\t1990-07-24 16:00:00\tNone\n",
      "24\tl.25\t0\t1988-07-13 08:00:00\tNone\n",
      "25\tl.16\t0\t1989-01-29 08:00:00\tNone\n",
      "26\tl.16\t0\t1989-11-05 07:00:00\tNone\n",
      "27\tl.40\t0\t1990-04-29 07:00:00\tNone\n",
      "28\tl.40\t0\t1990-12-18 07:00:00\tNone\n",
      "29\tl.39\t0\t1991-05-20 08:00:00\tNone\n",
      "30\tl.65\t1\t1990-07-31 18:28:00\tNone\n",
      "31\tl.45\t1\t1990-08-23 07:19:00\tNone\n",
      "32\tl.52\t1\t1990-09-11 18:00:00\tNone\n",
      "33\tl.10\t1\t1991-03-28 15:35:00\tNone\n",
      "34\tl.47\t1\t1991-04-26 06:17:00\tNone\n",
      "35\tl.55\t0\t1991-06-11 18:05:00\tNone\n",
      "36\tl.49\t0\t1991-07-03 12:21:00\tNone\n",
      "37\tl.39\t0\t1990-12-16 08:00:00\tNone\n",
      "38\tl.41\t0\t1991-06-30 08:00:00\tNone\n",
      "39\tl.52\t1\t1990-07-13 11:36:00\tNone\n",
      "40\tl.53\t1\t1990-08-26 17:26:00\tNone\n",
      "41\tl.51\t1\t1990-09-13 20:56:00\tNone\n",
      "42\tl.48\t0\t1991-03-29 18:59:00\tNone\n",
      "43\tl.52\t1\t1991-05-04 01:05:00\tNone\n",
      "44\tl.39\t1\t1991-07-05 20:47:00\tNone\n",
      "45\tl.36\t1\t1990-07-16 11:40:00\tNone\n",
      "46\tl.28\t0\t1990-08-03 06:31:00\tNone\n",
      "47\tl.45\t0\t1991-03-30 05:56:00\tNone\n",
      "48\tl.39\t0\t1991-04-20 11:20:00\tNone\n",
      "49\tl.35\t0\t1989-03-28 08:00:00\tNone\n",
      "50\tl.39\t0\t1990-07-29 07:00:00\tNone\n",
      "51\tl.41\t0\t1991-03-01 08:00:00\tNone\n",
      "52\tl.47\t0\t1989-02-03 08:00:00\tNone\n",
      "53\tl.22\t0\t1990-06-25 19:16:00\tNone\n",
      "54\tl.54\t1\t1990-08-22 05:29:00\tNone\n",
      "55\tl.59\t1\t1990-09-04 05:35:00\tNone\n",
      "56\tl.56\t0\t1991-04-15 13:08:00\tNone\n",
      "57\tl.43\t0\t1991-05-16 20:40:00\tNone\n",
      "58\tl.48\t0\t1991-06-12 05:48:00\tNone\n",
      "59\tl.45\t1\t1991-07-26 22:04:00\tNone\n",
      "60\tl.40\t1\t1989-04-17 06:35:00\tNone\n",
      "61\tl.40\t1\t1991-01-01 09:10:00\tNone\n",
      "62\tl.12\t0\t1988-03-27 08:00:00\tNone\n",
      "63\tl.30\t0\t1989-03-13 08:00:00\tNone\n",
      "# entries: 64, entropy=0.982\n"
     ]
    }
   ],
   "source": [
    "def print_dataset_state(dataset_X,dataset_Y,dataset_ST,dataset_V,indexes=None):\n",
    "    print(\"--DS state\")\n",
    "    if (indexes and len(indexes)>len(dataset_X)):\n",
    "        indexes=None\n",
    "    \n",
    "    print(\"index\\tX\\tY\\tST\\t\\t\\tV\")\n",
    "\n",
    "    if indexes is None:\n",
    "        indexes = range(0,len(dataset_X))\n",
    "\n",
    "    for i in indexes:\n",
    "        print(f\"{i}\\tl.{len(dataset_X[i])}\\t{dataset_Y[i]}\\t{dataset_ST[i]}\\t{dataset_V[i]}\")\n",
    "    print(f\"# entries: {len(dataset_X)}, entropy={compute_entropy(dataset_Y):4.3}\")\n",
    "\n",
    "    \n",
    "print_dataset_state(dataset_X,dataset_Y, dataset_ST,dataset_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# Find al possible d,l couples that I could split the tree on. Note: Ds are randomly selected bc otherwise I'd end up with ~36000 pairs...\n",
    "def create_pairs(dataset_X:list,dataset_ST:list,howmany_d =30,random_sampling=False):\n",
    "\n",
    "    #1. Create all labels available from current\n",
    "    labels = set()\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        for item in dataset_X[i]:\n",
    "            if (item[0] > dataset_ST[i]):# Only consider label if it's not been superato\n",
    "                labels.add(item[1])\n",
    "\n",
    "\n",
    "    #2. Find all d\n",
    "    durations = set()\n",
    "    if random_sampling:\n",
    "        durations = set()\n",
    "        for i in range(0,len(dataset_X)):\n",
    "            for item in dataset_X[i]:\n",
    "                if (item[0] > dataset_ST[i]): # Only consider timestamp if it's not been superato\n",
    "                    durations.add(item[0]-dataset_ST[i])\n",
    "        print(len(\"aaaaaaaaaaaa \"),durations)\n",
    "        if len(durations)>howmany_d:\n",
    "            durations = random.sample(sorted(durations),howmany_d) # Is this ok?\n",
    "        else:\n",
    "            durations = sorted(durations)\n",
    "    else:\n",
    "        durations = set()\n",
    "        for i in range(0,len(dataset_X)):\n",
    "            for item in dataset_X[i]:\n",
    "                durations.add(item[0]-dataset_ST[i])\n",
    "        delta = max(durations)/(howmany_d+1)\n",
    "        durations = set()\n",
    "        for i in range(1,howmany_d+1):\n",
    "            durations.add(delta*i)\n",
    "\n",
    "    #print([format_timedelta(x) for x in sorted(durations)])\n",
    "    return sorted(list(itertools.product(durations,labels)))\n",
    "\n",
    "# Given a set with binary classes, computes entropy\n",
    "def compute_entropy(dataset_Y):\n",
    "    ones = len(list(filter(lambda classification : classification == 1,dataset_Y)))\n",
    "    zeros = len(list(filter(lambda classification : classification == 0,dataset_Y)))\n",
    "\n",
    "    if(ones == 0 or zeros==0):\n",
    "        return 0\n",
    "    \n",
    "    entropy = ones/len(dataset_Y)*math.log2(1/(ones/len(dataset_Y))) + zeros/len(dataset_Y)*math.log2(1/(zeros/len(dataset_Y)))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def test_event(dataset_X,dl_pair,dataset_ST,dataset_V=None,update=False):\n",
    "    i_T = [] # indexes of entries that have label==l within d time\n",
    "    i_F = [] # indexes of entries that have DON'T HAVE label==l within d time\n",
    "    d,l = dl_pair\n",
    "\n",
    "\n",
    "    #1. Separate entries that satisfy event test from those who don't\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        entry = dataset_X[i]\n",
    "        found=False\n",
    "\n",
    "        for item in entry:\n",
    "            #print(item)\n",
    "            if(found is False and item[0]>=dataset_ST[i] and item[0]<=(dataset_ST[i]+d) and item[1]==l ):\n",
    "                found=True\n",
    "\n",
    "        if(found):\n",
    "            i_T.append(i)\n",
    "        else:\n",
    "            i_F.append(i)\n",
    "    return i_T,i_F\n",
    "\n",
    "# Given a pair of duration d and label l, computes its information gain on the dataset if we were to split it according to the sequence tree rules.\n",
    "def compute_IG(dl_pair,dataset_X,dataset_Y,dataset_ST,verbose=False):\n",
    "    #if verbose:\n",
    "     #   print(f\"Computing IG for {dl_pair}\")\n",
    "\n",
    "    entropy_0 = compute_entropy(dataset_Y)\n",
    "\n",
    "    i_T, i_F = test_event(dataset_X,dl_pair,dataset_ST)\n",
    "    \n",
    "    #2. Compute final entropy. first let's generate our new datasets...\n",
    "    dataset_Yt=[ dataset_Y[i] for i in i_T]\n",
    "    dataset_Yf=[ dataset_Y[i] for i in i_F]\n",
    "            \n",
    "    entropy_f = (len(i_T)/len(dataset_X))*compute_entropy(dataset_Yt) + (len(i_F)/len(dataset_X))*compute_entropy(dataset_Yf)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"information gain is {entropy_0-entropy_f} {entropy_0}-> {[ dataset_Y[i] for i in i_T],[ dataset_Y[i] for i in i_F]} {entropy_f}\")\n",
    "        \n",
    "    return(entropy_0-entropy_f,i_T,i_F)\n",
    "\n",
    "\n",
    "# Given all possible pairs of d,l finds the one with the highest information gain (aka the one I should actually split on)\n",
    "def maximize_IG(dataset_X,dataset_Y, dataset_ST, indexes=None,verbose=False,howmany=30,random_sampling=False):\n",
    "    if indexes is None:\n",
    "        indexes = range(0,len(dataset_X))\n",
    "\n",
    "    \n",
    "    dl_pairs = create_pairs([dataset_X[i] for i in indexes],[dataset_ST[i] for i in indexes],howmany,random_sampling)\n",
    "\n",
    "    #igs_dict = {x:compute_IG(x,dataset_X,dataset_Y,dataset_ST,verbose) for x in dl_pairs}\n",
    "    igs_list = [(x,compute_IG(x,dataset_X,dataset_Y,dataset_ST,verbose)) for x in dl_pairs]\n",
    "    # ogni entry di igs_list è ( (d,l) , (ig,i_T,i_F)   )\n",
    "\n",
    "    max_ig=-1\n",
    "    max_dl=None\n",
    "\n",
    "    for ((d,l),(ig,i_T,i_F)) in igs_list:\n",
    "        if ig > max_ig:\n",
    "            max_ig=ig\n",
    "            max_dl = ((d,l),(ig,i_T,i_F))\n",
    "\n",
    "    \n",
    "    if len(max_dl[1][1])==0 or len(max_dl[1][2])==0:\n",
    "        if verbose:\n",
    "            print(\"Split failed, couldn't find a d,l that separates values :(\")\n",
    "        return None, 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Max IG is in couple d=\",format_timedelta(max_dl[0][0]),\", l=\",max_dl[0][1],\", IG=\",max_dl[1])\n",
    "\n",
    "    return max_dl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DS loader\n",
      "\tSkipped 46 items for formatting issues in data file. 70 loaded.\n",
      "-- DS builder\n",
      "\t2 entries unsuitable for selected windows.\n",
      "\tFinal dataset size: 48. Classes: 20|28, entropy 0.98\n",
      "('23h 13m 9s', '65', (0.1861381990467907, [2, 3, 5, 7, 8, 9, 12, 16], [0, 1, 4, 6, 10, 11, 13, 14, 15]))\n",
      "--DS state\n",
      "index\tX\tY\tST\t\t\tV\n",
      "0\tl.41\t0\t1990-08-19 17:00:00\tNone\n",
      "1\tl.24\t1\t1990-07-31 12:09:00\tNone\n",
      "2\tl.46\t1\t1990-07-22 09:53:00\tNone\n",
      "3\tl.58\t1\t1990-09-04 05:53:00\tNone\n",
      "4\tl.38\t1\t1991-03-11 18:15:00\tNone\n",
      "5\tl.40\t1\t1990-07-13 09:48:00\tNone\n",
      "6\tl.37\t0\t1991-04-27 23:02:00\tNone\n",
      "7\tl.45\t1\t1990-08-23 07:19:00\tNone\n",
      "8\tl.52\t1\t1990-09-11 18:00:00\tNone\n",
      "9\tl.10\t1\t1991-03-28 15:35:00\tNone\n",
      "10\tl.47\t1\t1991-04-26 06:17:00\tNone\n",
      "11\tl.55\t0\t1991-06-11 18:05:00\tNone\n",
      "12\tl.52\t1\t1990-07-13 11:36:00\tNone\n",
      "13\tl.53\t1\t1990-08-26 17:26:00\tNone\n",
      "14\tl.52\t1\t1991-05-04 01:05:00\tNone\n",
      "15\tl.39\t1\t1991-07-05 20:47:00\tNone\n",
      "16\tl.36\t1\t1990-07-16 11:40:00\tNone\n",
      "# entries: 17, entropy=0.672\n"
     ]
    }
   ],
   "source": [
    "#tests\n",
    "\n",
    "dataset = load_diabetes_dataset(False)\n",
    "observation_window = timedelta(days=+5)\n",
    "waiting_window = timedelta(days=+5)\n",
    "prediction_window = timedelta(days=+5)\n",
    "\n",
    "dataset_X,dataset_Y, dataset_ST,dataset_V, dataset_F = compute_datasets(dataset[:50],observation_window,waiting_window,prediction_window)\n",
    "indexes = [3, 7, 11, 12, 13, 16, 22, 32, 33, 34, 35, 36, 40, 41, 44, 45, 46]\n",
    "dataset_X = [dataset_X[i] for i in indexes]\n",
    "dataset_Y = [dataset_Y[i] for i in indexes]\n",
    "dataset_ST = [dataset_ST[i] for i in indexes]\n",
    "dataset_V = [dataset_V[i] for i in indexes]\n",
    "\n",
    "max_dl, max_ig = maximize_IG(dataset_X,dataset_Y,dataset_ST,range(len(dataset_X)),verbose=False,howmany=30,random_sampling=False)\n",
    "\n",
    "print(f\"{format_timedelta(max_dl[0]),max_dl[1],max_ig}\")\n",
    "print_dataset_state(dataset_X,dataset_Y,dataset_ST,dataset_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treelib import Node, Tree\n",
    "class SequenceTree(Tree):\n",
    "    def __init__(self, tree=None, deep=False, node_class=None, identifier=None):\n",
    "        super(SequenceTree, self).__init__(tree=tree, deep=deep, node_class=node_class, identifier=identifier)\n",
    "\n",
    "    def create_node(self, tag=None, identifier=None, parent=None, data=None,branch=None):\n",
    "        \"\"\"\n",
    "        Create a child node for the given @parent node. If ``identifier`` is absent,\n",
    "        a UUID will be generated automatically.\n",
    "        \"\"\"\n",
    "        new_node = super(SequenceTree, self).create_node(tag=tag, identifier=identifier, parent=parent, data=data)\n",
    "        \n",
    "        siblings = super(SequenceTree,self).siblings(new_node.identifier)\n",
    "        \n",
    "\n",
    "        if len(super(SequenceTree,self).siblings(new_node.identifier))>=2:\n",
    "           raise ValueError(\"Parent node already has maximum number of children\")\n",
    "\n",
    "        if branch in [x.data[\"branch\"] for x in siblings]:\n",
    "           raise ValueError(f\"Parent node already has a {branch} branch\")\n",
    "        return new_node\n",
    "    \n",
    "    def display(self):\n",
    "        print(self.show(stdout=False))\n",
    "    \n",
    "    def create_node_event(self,data,parent=None,branch=None):\n",
    "        branch_f = \"\" if (branch is None) else branch+\" \"\n",
    "        tag =  \"\\x1b[32m⬤\\x1b[0m\" +\" \"+ branch_f +\"[\" + str(data[1])+\", \" + format_timedelta(data[0])+ \"] \"  \n",
    "\n",
    "        return     self.create_node(tag,data=(branch,{\"classification\":data}),parent=parent,branch=branch)\n",
    "\n",
    "\n",
    "def create_node_event(tree,data,parent=None,branch=None,entropy=\"\",size=0,ig=\"\",index=\"\"):\n",
    "    branch_f = \"\" if (branch is None) else str(branch)+\" \"\n",
    "    tag =  f\"\\x1b[32m⬤ {branch_f} ({str(data[1])},{format_timedelta(data[0])})\\x1b[0m [e={entropy:4.2} ig={ig:4.2}] [n={size}] - {index}\"\n",
    "    data = {\"branch\":branch, \"dl\":(data[0],data[1]),\"entropy\":entropy,\"ig\":ig,\"index\":index}\n",
    "\n",
    "    return     tree.create_node(tag,data=data,parent=parent,branch=branch)\n",
    "\n",
    "def create_node_class(tree,classification,parent=None,branch=None,entropy=\"\",size=0,index=\"\"):\n",
    "    branch_f = \"\" if (branch is None) else str(branch)+\" \"\n",
    "    data = {\"branch\":branch, \"class\":classification,\"entropy\":entropy,\"index\":index}\n",
    "\n",
    "    tag =  f\"\\x1b[33m◆ {branch_f} {classification} \\x1b[0m- [e={float(entropy):2.2}] [n={size}] - {index}\"\n",
    "\n",
    "    return     tree.create_node(tag,data=data,parent=parent,branch=branch)\n",
    "\n",
    "def create_node_value(tree,data,parent=None,branch=None,entropy=\"\",size=0,ig=\"\",index=\"\"):\n",
    "    branch_f = \"\" if (branch is None) else branch+\" \"\n",
    "    tag =  f\"\\x1b[31m■ {branch_f} {data} \\x1b[0m- [e={float(entropy):2.2}] [n={size}] - {index}\"\n",
    " \n",
    "    return     tree.create_node(tag,data=(branch,{\"value\":data}),parent=parent,branch=branch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DS loader\n",
      "\tSkipped 46 items for formatting issues in data file. 70 loaded.\n",
      "-- DS builder\n",
      "\t2 entries unsuitable for selected windows.\n",
      "\tFinal dataset size: 68. Classes: 31|37, entropy 0.994\n",
      "--Final sequence tree\n",
      "\u001b[32m⬤  (48,2d 17h 1m 1s)\u001b[0m [e=0.99 ig=0.21] [n=68] - \n",
      "├── \u001b[32m⬤ f  (62,6h 57m 58s)\u001b[0m [e=0.95 ig= 0.1] [n=46] - \n",
      "│   ├── \u001b[32m⬤ f  (34,2d 17h 1m 1s)\u001b[0m [e=0.81 ig=0.17] [n=32] - \n",
      "│   │   ├── \u001b[32m⬤ f  (65,1d 15h 26m 17s)\u001b[0m [e=0.99 ig= 0.4] [n=11] - \n",
      "│   │   │   ├── \u001b[32m⬤ f  (57,23h 3m 52s)\u001b[0m [e=0.81 ig=0.31] [n=8] - \n",
      "│   │   │   │   ├── \u001b[32m⬤ f  (33,2h 17m 48s)\u001b[0m [e= 1.0 ig=0.31] [n=4] - \n",
      "│   │   │   │   │   ├── \u001b[33m◆ f  1 \u001b[0m- [e=0.0] [n=1] - [24]\n",
      "│   │   │   │   │   └── \u001b[33m◆ t  0 MAX DEPTH REACHED \u001b[0m- [e=0.92] [n=3] - [2, 56, 60]\n",
      "│   │   │   │   └── \u001b[33m◆ t  0 \u001b[0m- [e=0.0] [n=4] - [21, 48, 59, 62]\n",
      "│   │   │   └── \u001b[33m◆ t  1 \u001b[0m- [e=0.0] [n=3] - [35, 61, 64]\n",
      "│   │   └── \u001b[32m⬤ t  (58,2h 19m 19s)\u001b[0m [e=0.45 ig=0.18] [n=21] - \n",
      "│   │       ├── \u001b[32m⬤ f  (57,11h 36m 36s)\u001b[0m [e=0.29 ig=0.15] [n=20] - \n",
      "│   │       │   ├── \u001b[32m⬤ t  (33,2h 12m 58s)\u001b[0m [e=0.92 ig=0.92] [n=3] - \n",
      "│   │       │   │   ├── \u001b[33m◆ f  1 \u001b[0m- [e=0.0] [n=2] - [49, 55]\n",
      "│   │       │   │   └── \u001b[33m◆ t  0 \u001b[0m- [e=0.0] [n=1] - [23]\n",
      "│   │       │   └── \u001b[33m◆ f  1 \u001b[0m- [e=0.0] [n=17] - [10, 11, 12, 14, 15, 16, 17, 18, 22, 32, 33, 36, 43, 44, 45, 46, 57]\n",
      "│   │       └── \u001b[33m◆ t  0 \u001b[0m- [e=0.0] [n=1] - [66]\n",
      "│   └── \u001b[32m⬤ t  (65,2d 14h 40m 50s)\u001b[0m [e=0.94 ig=0.66] [n=14] - \n",
      "│       ├── \u001b[32m⬤ t  (35,10h 43m 3s)\u001b[0m [e=0.65 ig=0.65] [n=6] - \n",
      "│       │   ├── \u001b[33m◆ f  1 \u001b[0m- [e=0.0] [n=5] - [7, 13, 34, 37, 42]\n",
      "│       │   └── \u001b[33m◆ t  0 \u001b[0m- [e=0.0] [n=1] - [3]\n",
      "│       └── \u001b[33m◆ f  0 \u001b[0m- [e=0.0] [n=8] - [4, 8, 31, 38, 41, 47, 50, 58]\n",
      "└── \u001b[32m⬤ t  (34,10h 58m 3s)\u001b[0m [e=0.44 ig=0.044] [n=22] - \n",
      "    ├── \u001b[32m⬤ t  (48,14h 58m 3s)\u001b[0m [e=0.54 ig=0.11] [n=16] - \n",
      "    │   ├── \u001b[32m⬤ f  (60,7h 29m 1s)\u001b[0m [e=0.76 ig=0.082] [n=9] - \n",
      "    │   │   ├── \u001b[32m⬤ f  (62,10h 54m 9s)\u001b[0m [e= 1.0 ig= 1.0] [n=2] - \n",
      "    │   │   │   ├── \u001b[33m◆ f  0 \u001b[0m- [e=0.0] [n=1] - [19]\n",
      "    │   │   │   └── \u001b[33m◆ t  1 \u001b[0m- [e=0.0] [n=1] - [0]\n",
      "    │   │   └── \u001b[32m⬤ t  (58,19h 9m 40s)\u001b[0m [e=0.59 ig= 0.2] [n=7] - \n",
      "    │   │       ├── \u001b[32m⬤ t  (62,13h 53m 13s)\u001b[0m [e=0.92 ig=0.92] [n=3] - \n",
      "    │   │       │   ├── \u001b[33m◆ f  1 \u001b[0m- [e=0.0] [n=1] - [65]\n",
      "    │   │       │   └── \u001b[33m◆ t  0 \u001b[0m- [e=0.0] [n=2] - [28, 29]\n",
      "    │   │       └── \u001b[33m◆ f  0 \u001b[0m- [e=0.0] [n=4] - [5, 6, 9, 27]\n",
      "    │   └── \u001b[33m◆ t  0 \u001b[0m- [e=0.0] [n=7] - [20, 25, 30, 39, 40, 51, 53]\n",
      "    └── \u001b[33m◆ f  0 \u001b[0m- [e=0.0] [n=6] - [1, 26, 52, 54, 63, 67]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree = SequenceTree()\n",
    "\n",
    "# prepare\n",
    "dataset = load_diabetes_dataset(False)\n",
    "observation_window = timedelta(days=+3)\n",
    "waiting_window = timedelta(days=+0)\n",
    "prediction_window = timedelta(days=+10)\n",
    "\n",
    "dataset_X,dataset_Y, dataset_ST,dataset_V, dataset_F = compute_datasets(dataset[:],observation_window,waiting_window,prediction_window,)\n",
    "#print_dataset_state(dataset_X,dataset_Y,dataset_ST,dataset_V)\n",
    "\n",
    "def perform_event_test(max_dl,indexes,dataset_X,dataset_ST,dataset_V,verbose=False):\n",
    "    # divide dataset in t and f...\n",
    "    i_T = []\n",
    "    i_F = []\n",
    "\n",
    "    d,l = max_dl\n",
    "    old_dataset_ST=dataset_ST\n",
    "    \n",
    "    for i in range(0,len(dataset_X)):\n",
    "        entry = dataset_X[i]\n",
    "\n",
    "        found=False\n",
    "\n",
    "        for item in entry:\n",
    "            #starting from the starting time, see if it exists an item with timestamp < d and label == l\n",
    "            \n",
    "            if(found is False and item[0]>= old_dataset_ST[i] and item[0]<=(old_dataset_ST[i]+d) and item[1]==l ): # If i'm over starting time\n",
    "                found=True\n",
    "                dataset_ST[i] = item[0]\n",
    "                dataset_V[i] = item[2]\n",
    "\n",
    "        if(found):\n",
    "            i_T.append(i)\n",
    "        else:\n",
    "            i_F.append(i)\n",
    "            \n",
    "    return i_T,i_F,dataset_ST,dataset_V\n",
    "\n",
    "\n",
    "\n",
    "def fit(dataset_X,dataset_Y,dataset_V,dataset_ST,max_depth,parent=None,branch=None,depth=0,indexes=\"\",print_steps=False):\n",
    "\n",
    "\n",
    "    if print_steps and (len(tree.all_nodes()) != 0):\n",
    "        print(tree)\n",
    "\n",
    "    # If all the classes are the same, make a class node\n",
    "    if (len(dataset_X)==1) or (all(element == dataset_Y[0] for element in dataset_Y)):\n",
    "        if print_steps:\n",
    "            print(f\"{depth,branch} All classes are the same!\")\n",
    "        create_node_class(tree=tree,classification=dataset_Y[0],parent=parent,branch=branch,entropy=compute_entropy(dataset_Y),size=len(dataset_Y),index=indexes)\n",
    "        #print_dataset_state(dataset_X,dataset_Y,dataset_ST,dataset_V)\n",
    "        return\n",
    "\n",
    "    #1. get best event.\n",
    "    max_dl, max_ig = maximize_IG(dataset_X,dataset_Y,dataset_ST,range(len(dataset_X)),False,howmany=30,random_sampling=False)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # If classes are different but no split value is found, make a class node with majority\n",
    "    if max_dl is None:\n",
    "        if print_steps:\n",
    "            print(f\"{depth,branch} Couldn't find split node!\")\n",
    "\n",
    "        # Split not possible, make a class node!\n",
    "        y_1 = sum(1 for c in dataset_Y if c == 1)\n",
    "        y_0 = sum(1 for c in dataset_Y if c == 0)\n",
    "        estimated_class = 1 if y_1>y_0 else 0    \n",
    "        create_node_class(tree,str(estimated_class)+\"NO SPLIT FOUND\",parent,branch,compute_entropy(dataset_Y),len(dataset_Y),indexes)\n",
    "\n",
    "    elif depth > max_depth:\n",
    "        if (print_steps):\n",
    "            print(\"Reached maximum depth!\")\n",
    "        \n",
    "        y_1 = sum(1 for c in dataset_Y if c == 1)\n",
    "        y_0 = sum(1 for c in dataset_Y if c == 0)\n",
    "        estimated_class = 1 if y_1>y_0 else 0  \n",
    "        estimated_class =   str(estimated_class)+\" MAX DEPTH REACHED\"\n",
    "        return create_node_class(tree,estimated_class,parent,branch,compute_entropy(dataset_Y),len(dataset_Y),indexes)\n",
    "    \n",
    "    else:\n",
    "\n",
    "        node = create_node_event(tree,max_dl,parent,branch,compute_entropy(dataset_Y),len(dataset_Y),max_ig[0])\n",
    "        #print_dataset_state(dataset_X,dataset_Y,dataset_ST,dataset_V,dataset_F)\n",
    "\n",
    "\n",
    "        i_T, i_F, dataset_ST, dataset_V=perform_event_test(max_dl,indexes,dataset_X,dataset_ST,dataset_V)\n",
    "        fit([dataset_X[i] for i in i_T],[dataset_Y[i] for i in i_T],[dataset_V[i] for i in i_T],[dataset_ST[i] for i in i_T],max_depth,node,\"t\",depth+1,[indexes[i] for i in i_T])\n",
    "        fit([dataset_X[i] for i in i_F],[dataset_Y[i] for i in i_F],[dataset_V[i] for i in i_F],[dataset_ST[i] for i in i_F],max_depth,node,\"f\",depth+1,[indexes[i] for i in i_F])\n",
    "        \n",
    "#print_dataset_state(dataset_X,dataset_Y,dataset_ST,dataset_V)\n",
    "fit(dataset_X,dataset_Y,dataset_V,dataset_ST,max_depth=5,indexes=range(0,len(dataset_X)),print_steps=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"--Final sequence tree\")\n",
    "if len(tree.all_nodes()) != 0:\n",
    "    print(tree)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
