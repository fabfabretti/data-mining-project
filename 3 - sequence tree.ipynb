{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 3 - Sequence tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operazioni preliminari sul DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carichiamo il DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def load_diabetes_dataset(verbose=False,remove_class=False)-> list: \n",
    "    folder_path=\"datasets\\\\diabetes\"\n",
    "    dataset = []\n",
    "    errcount=0\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if os.path.isfile(file_path)and filename.startswith('data'):\n",
    "            entry=[]\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.readlines()\n",
    "                for line in content:\n",
    "                    item = tuple((line[0:-1] if line.endswith('\\n') else tuple(line)).split(\"\\t\"))\n",
    "\n",
    "                    # If the item is valid, append it to the entry\n",
    "                    try:\n",
    "                        item_f = datetime.strptime(item[0]+\" \"+item[1], \"%m-%d-%Y %H:%M\")\n",
    "                        if (not remove_class):\n",
    "                            entry.append((item_f,item[2],item[3]))\n",
    "                        elif (remove_class and item[2]!=\"65\"):\n",
    "                            entry.append((item_f,item[2],item[3]))\n",
    "                    except:\n",
    "                        if(verbose):\n",
    "                            print(f\"[!] Entry {item} in file {filename} is NOT vallid. Skipped!\")\n",
    "                        else:\n",
    "                            errcount+=1\n",
    "                # add the entry to the dataset\n",
    "                dataset.append(entry)\n",
    "    print(f\"Skipped {errcount} items.\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A partire dal dataset grezzo generiamo X,Y,ST e V.\n",
    "\n",
    "> Definiamo la classe reale come uno [0,1] che indica se l'evento 65 (65 = Hypoglycemic symptoms) si è verificato in una certa finestra di tempo `event_window` dopo un certo waiting time `waiting_window`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 46 items.\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "#Domande. \n",
    "#2.  Also, la distanza d nelle coppie d,l è definita in giorni?\n",
    "\n",
    "dataset_X = load_diabetes_dataset()\n",
    "\n",
    "def compute_datasets(dataset:list,observation_window,waiting_window,prediction_window)->list:\n",
    "    # Initialize: I assume it will not be found\n",
    "    dataset_X = [0] * len(dataset)\n",
    "    dataset_Y = [0] * len(dataset)\n",
    "\n",
    "    # each entry\n",
    "    for i in range(0,len(dataset)):\n",
    "        entry = dataset[i]\n",
    "        #i need to isolate observation items.\n",
    "        j=0\n",
    "\n",
    "        end_observation = entry[0][0] + observation_window # this is the timestamp of the first item in the entry\n",
    "        curr_time = entry[0][0]\n",
    "\n",
    "        observation_items = []\n",
    "\n",
    "        #1. Isolate observation window! That's our actual X dataset!\n",
    "        while (curr_time <= end_observation):\n",
    "            #print(f\"{curr_time}  <=  {end_observation}\")\n",
    "            if (entry[j][1]!=\"65\"):\n",
    "                observation_items.append(entry[j])\n",
    "            j+=1\n",
    "            curr_time = entry[j][0]\n",
    "        dataset_X[i] = observation_items \n",
    "\n",
    "        #2. Skip the waiting window\n",
    "        end_waiting =  entry[0][0] + observation_window + waiting_window\n",
    "\n",
    "        while(curr_time <= end_waiting):\n",
    "            j+=1\n",
    "            curr_time = entry[j][0]\n",
    "\n",
    "        #3. Generate prediction window and assign 1 in Y if adverse effect is present, 0 else\n",
    "\n",
    "        while (curr_time <= entry[0][0]+observation_window+waiting_window+prediction_window):\n",
    "            if (entry[j][1]==\"65\"):\n",
    "                dataset_Y[i] = 1\n",
    "                break\n",
    "            j+=1\n",
    "            if (j<len(entry)):\n",
    "                curr_time = entry[j][0]\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    dataset_ST = [0]*len(dataset_X)\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        dataset_ST[i] = dataset_X[i][0][0]\n",
    "    dataset_V = [None]*len(dataset_X)\n",
    "\n",
    "    return dataset_X,dataset_Y,dataset_ST,dataset_V\n",
    "\n",
    "\n",
    "\n",
    "observation_window = relativedelta(days=+7)\n",
    "waiting_window = relativedelta(days=+0)\n",
    "prediction_window = relativedelta(days=+5)\n",
    "\n",
    "dataset_X,dataset_Y, dataset_ST,dataset_V = compute_datasets(dataset_X,observation_window,waiting_window,prediction_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stampiamo il DS elaborato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\t\tY\tST\t\t\tV\n",
      "len:46\t\t1\t1991-04-21 09:09:00\tNone\n",
      "len:56\t\t0\t1989-10-10 08:00:00\tNone\n",
      "len:64\t\t1\t1990-07-21 06:43:00\tNone\n",
      "len:55\t\t0\t1990-08-19 17:00:00\tNone\n",
      "len:61\t\t0\t1990-09-01 16:48:00\tNone\n",
      "len:51\t\t0\t1989-04-29 08:00:00\tNone\n",
      "len:54\t\t0\t1989-03-27 22:00:00\tNone\n",
      "len:33\t\t1\t1990-07-31 12:09:00\tNone\n",
      "len:57\t\t0\t1990-04-22 18:08:00\tNone\n",
      "len:51\t\t0\t1989-02-18 08:00:00\tNone\n",
      "len:55\t\t1\t1990-07-13 09:44:00\tNone\n",
      "len:68\t\t1\t1990-07-22 09:53:00\tNone\n",
      "len:76\t\t1\t1990-09-04 05:53:00\tNone\n",
      "len:51\t\t1\t1991-03-11 18:15:00\tNone\n",
      "len:33\t\t1\t1991-04-13 08:47:00\tNone\n",
      "len:31\t\t1\t1991-05-22 07:24:00\tNone\n",
      "len:58\t\t1\t1990-07-13 09:48:00\tNone\n",
      "len:63\t\t1\t1990-08-18 07:16:00\tNone\n",
      "len:60\t\t1\t1990-09-09 17:23:00\tNone\n",
      "len:50\t\t0\t1991-05-12 06:55:00\tNone\n",
      "len:65\t\t0\t1989-09-03 08:00:00\tNone\n",
      "len:55\t\t0\t1991-03-14 22:05:00\tNone\n",
      "len:51\t\t0\t1991-04-27 23:02:00\tNone\n",
      "len:37\t\t0\t1991-05-28 21:35:00\tNone\n",
      "len:19\t\t1\t1990-07-24 16:00:00\tNone\n",
      "len:37\t\t0\t1988-07-13 08:00:00\tNone\n",
      "len:16\t\t0\t1989-01-29 08:00:00\tNone\n",
      "len:27\t\t0\t1989-11-05 07:00:00\tNone\n",
      "len:59\t\t0\t1990-04-29 07:00:00\tNone\n",
      "len:56\t\t0\t1990-12-18 07:00:00\tNone\n",
      "len:58\t\t0\t1991-05-20 08:00:00\tNone\n",
      "len:89\t\t0\t1990-07-26 12:27:00\tNone\n",
      "len:84\t\t1\t1990-07-31 18:28:00\tNone\n",
      "len:62\t\t1\t1990-08-23 07:19:00\tNone\n",
      "len:66\t\t1\t1990-09-11 18:00:00\tNone\n",
      "len:19\t\t1\t1991-03-28 15:35:00\tNone\n",
      "len:61\t\t1\t1991-04-26 06:17:00\tNone\n",
      "len:69\t\t1\t1991-06-11 18:05:00\tNone\n",
      "len:69\t\t0\t1991-07-03 12:21:00\tNone\n",
      "len:94\t\t0\t1989-11-01 06:30:00\tNone\n",
      "len:57\t\t0\t1990-12-16 08:00:00\tNone\n",
      "len:59\t\t0\t1991-06-30 08:00:00\tNone\n",
      "len:66\t\t0\t1990-07-13 11:36:00\tNone\n",
      "len:72\t\t1\t1990-08-26 17:26:00\tNone\n",
      "len:74\t\t0\t1990-09-13 20:56:00\tNone\n",
      "len:67\t\t1\t1991-03-29 18:59:00\tNone\n",
      "len:68\t\t0\t1991-05-04 01:05:00\tNone\n",
      "len:55\t\t0\t1991-07-05 20:47:00\tNone\n",
      "len:45\t\t0\t1990-07-16 11:40:00\tNone\n",
      "len:42\t\t0\t1990-08-03 06:31:00\tNone\n",
      "len:61\t\t1\t1991-03-30 05:56:00\tNone\n",
      "len:55\t\t0\t1991-04-20 11:20:00\tNone\n",
      "len:52\t\t0\t1989-03-28 08:00:00\tNone\n",
      "len:51\t\t0\t1990-07-29 07:00:00\tNone\n",
      "len:61\t\t0\t1991-03-01 08:00:00\tNone\n",
      "len:70\t\t0\t1989-02-03 08:00:00\tNone\n",
      "len:21\t\t0\t1990-06-25 19:16:00\tNone\n",
      "len:75\t\t0\t1990-08-22 05:29:00\tNone\n",
      "len:76\t\t1\t1990-09-04 05:35:00\tNone\n",
      "len:78\t\t0\t1991-04-15 13:08:00\tNone\n",
      "len:57\t\t0\t1991-05-16 20:40:00\tNone\n",
      "len:69\t\t0\t1991-06-12 05:48:00\tNone\n",
      "len:60\t\t1\t1991-07-26 22:04:00\tNone\n",
      "len:32\t\t0\t1990-01-23 07:30:00\tNone\n",
      "len:61\t\t0\t1989-04-17 06:35:00\tNone\n",
      "len:84\t\t1\t1989-11-05 06:50:00\tNone\n",
      "len:58\t\t1\t1991-01-01 09:10:00\tNone\n",
      "len:16\t\t0\t1988-03-27 08:00:00\tNone\n",
      "len:27\t\t0\t1990-08-22 08:58:00\tNone\n",
      "len:45\t\t0\t1989-03-13 08:00:00\tNone\n",
      "# entries: 70\n"
     ]
    }
   ],
   "source": [
    "def print_dataset_state(dataset_X,dataset_Y,dataset_ST,dataset_V,howmany=10):\n",
    "    print(\"X\\t\\tY\\tST\\t\\t\\tV\")\n",
    "    for i in range(0,len(dataset_X[:howmany])):\n",
    "        print(f\"len:{len(dataset_X[i])}\\t\\t{dataset_Y[i]}\\t{dataset_ST[i]}\\t{dataset_V[i]}\")\n",
    "\n",
    "print_dataset_state(dataset_X,dataset_Y,dataset_ST, dataset_V,len(dataset_X))\n",
    "print(f\"# entries: {len(dataset_X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((datetime.timedelta(days=1, seconds=2880), '35'), 0.04185721895377048)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "import math\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "# Find al possible d,l couples that I could split the tree on. Note: Ds are randomly selected bc otherwise I'd end up with ~36000 pairs...\n",
    "def create_pairs(dataset_X:list,dataset_ST:list,all_d=False,howmany_d =30):\n",
    "    #1. Create all labels available from current.\n",
    "    labels = set()\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        for item in dataset_X[i]:\n",
    "            if (item[0] > dataset_ST[i]):# Only consider label if it's not been superato\n",
    "                labels.add(item[1])\n",
    "\n",
    "    durations = set()\n",
    "\n",
    "    #2. Find all d\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        for item in dataset_X[i]:\n",
    "            if (item[0] > dataset_ST[i]): # Only consider timestamp if it's not been superato\n",
    "                durations.add(item[0]-dataset_ST[i])\n",
    "    durations = random.sample(sorted(durations),howmany_d) # Is this ok?\n",
    "    return list(itertools.product(durations,labels))\n",
    "\n",
    "# Given a set with binary classes, computes entropy\n",
    "def compute_entropy(dataset_Y):\n",
    "    #print(dataset_Y)\n",
    "    ones = len(list(filter(lambda classification : classification == 1,dataset_Y)))\n",
    "    zeros = len(list(filter(lambda classification : classification == 0,dataset_Y)))\n",
    "\n",
    "    if(ones == 0 or zeros==0):\n",
    "        return 0\n",
    "    #print(f\"count 1: {ones}\") #print(f\"count 0: {zeros}\")\n",
    "    #print(f\"p:{ones/len(dataset_X)}  1-p:{zeros/len(dataset_X)}\")\n",
    "    entropy = ones/len(dataset_Y)*math.log2(1/(ones/len(dataset_Y))) + zeros/len(dataset_Y)*math.log2(1/(zeros/len(dataset_Y)))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "# Given a pair of duration d and label l, computes its information gain on the dataset if we were to split it according to the sequence tree rules.\n",
    "def compute_IG(dl_pair,dataset_X,dataset_Y,verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"Computing IG for {dl_pair}\")\n",
    "    indexes_t = [] # indexes of entries that have label==l within d time\n",
    "    indexes_f = [] # indexes of entries that have DON'T HAVE label==l within d time\n",
    "    d,l = dl_pair\n",
    "\n",
    "    entropy_0 = compute_entropy(dataset_Y)\n",
    "    #print(f\"Initial entropy: {entropy_0}\")\n",
    "\n",
    "    #1. Separate entries that satisfy event test from those who don't\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        entry = dataset_X[i]\n",
    "        found=False\n",
    "\n",
    "        for item in entry:\n",
    "            #starting from the starting time, see if it exists an item with timestamp < d and label == l\n",
    "            if(item[0]> dataset_ST[i]): # If i'm over starting time\n",
    "                if(item[0]< dataset_ST[i]+d and item[2]==l):\n",
    "                    found=True\n",
    "        if(found):\n",
    "            indexes_t.append(i)\n",
    "        else:\n",
    "            indexes_f.append(i)\n",
    "    \n",
    "    #2. Compute final entropy. first let's generate our new datasets...\n",
    "    dataset_Yt=[ dataset_Y[i] for i in indexes_t]\n",
    "    dataset_Yf=[ dataset_Y[i] for i in indexes_f]\n",
    "    #print(dataset_Yt)    print(dataset_Yf)\n",
    "            \n",
    "    entropy_f = (len(indexes_t)/len(dataset_X))*compute_entropy(dataset_Yt) + (len(indexes_f)/len(dataset_X))*compute_entropy(dataset_Yf)\n",
    "\n",
    "    if verbose:\n",
    "        print(indexes_t,indexes_f)\n",
    "        print(dataset_Yt,dataset_Yf)\n",
    "        #print(f\"final entropy: {entropy_f}\")\n",
    "        print(f\"information gain is {entropy_0-entropy_f}\")\n",
    "\n",
    "    return(entropy_0-entropy_f)\n",
    "\n",
    "# Given all possible pairs of d,l finds the one with the highest information gain (aka the one I should actually split on)\n",
    "def max_IG(dataset_X,dataset_Y, dataset_ST, verbose=False):\n",
    "    dl_pairs = create_pairs(dataset_X,dataset_ST)\n",
    "    #igs_list=[(x,compute_IG(x,dataset_X,dataset_Y)) for x in dl_pairs[:] ]\n",
    "    igs_dict = {x:compute_IG(x,dataset_X,dataset_Y) for x in dl_pairs[:10]}\n",
    "    if verbose:\n",
    "        print(igs_dict)\n",
    "        print(f\"Max IG is {igs_dict[max(igs_dict, key=igs_dict.get)]} for entry {max(igs_dict, key=igs_dict.get)}\")\n",
    "    return (max(igs_dict, key=igs_dict.get),igs_dict[max(igs_dict, key=igs_dict.get)])\n",
    "\n",
    "print(max_IG(dataset_X,dataset_Y,dataset_ST))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(datetime.timedelta(days=2, seconds=32220), '35')\n",
      "\u001b[32m⬤\u001b[0m [35, 2d8h57m] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from treelib import Node, Tree\n",
    "class SequenceTree(Tree):\n",
    "    def __init__(self, tree=None, deep=False, node_class=None, identifier=None):\n",
    "        super(SequenceTree, self).__init__(tree=tree, deep=deep, node_class=node_class, identifier=identifier)\n",
    "\n",
    "    def create_node(self, tag=None, identifier=None, parent=None, data=None,branch=None):\n",
    "        \"\"\"\n",
    "        Create a child node for the given @parent node. If ``identifier`` is absent,\n",
    "        a UUID will be generated automatically.\n",
    "        \"\"\"\n",
    "        new_node = super(SequenceTree, self).create_node(tag=tag, identifier=identifier, parent=parent, data=data)\n",
    "        \n",
    "        siblings = super(SequenceTree,self).siblings(new_node.identifier)\n",
    "        \n",
    "\n",
    "        if len(super(SequenceTree,self).siblings(new_node.identifier))>=2:\n",
    "           raise ValueError(\"Parent node already has maximum number of children\")\n",
    "        if branch in [x.data[0] for x in siblings]:\n",
    "           raise ValueError(f\"Parent node already has a {branch} branch\")\n",
    "        return new_node\n",
    "    \n",
    "    def display(self):\n",
    "        print(self.show(stdout=False))\n",
    "    \n",
    "    def create_node_event(self,data,parent=None,branch=None):\n",
    "        branch_f = \"\" if (branch is None) else branch+\" \"\n",
    "        tag =  \"\\x1b[32m⬤\\x1b[0m\" +\" \"+ branch_f +\"[\" + str(data[1])+\", \" + format_timedelta(data[0])+ \"] \"  \n",
    "\n",
    "        return     self.create_node(tag,data=(branch,{\"classification\":data}),parent=parent,branch=branch)\n",
    "\n",
    "def format_timedelta(td: timedelta) -> str:\n",
    "    days = td.days\n",
    "    hours, remainder = divmod(td.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    \n",
    "    formatted_str = \"\"\n",
    "    if days:\n",
    "        formatted_str += f\"{days}d\"\n",
    "    if hours:\n",
    "        formatted_str += f\"{hours}h\"\n",
    "    if minutes:\n",
    "        formatted_str += f\"{minutes}m\"\n",
    "    if seconds:\n",
    "        formatted_str += f\"{seconds}s\"\n",
    "    \n",
    "    return formatted_str if formatted_str else \"0s\"\n",
    "\n",
    "def create_node_event(tree,data,parent=None,branch=None):\n",
    "    branch_f = \"\" if (branch is None) else branch+\" \"\n",
    "    tag =  \"\\x1b[32m⬤\\x1b[0m\" +\" \"+ branch_f +\"[\" + str(data[1])+\", \" + format_timedelta(data[0])+ \"] \"  \n",
    "\n",
    "    return     tree.create_node(tag,data=(branch,{\"classification\":data}),parent=parent,branch=branch)\n",
    "\n",
    "def create_node_class(tree,classification,parent=None,branch=None):\n",
    "    branch_f = \"\" if (branch is None) else branch+\" \"\n",
    "    tag =  \"\\x1b[33m◆\\x1b[0m\" +\" \"+ branch_f + \" \" + str(classification)  \n",
    "\n",
    "    return     tree.create_node(tag,data=(branch,{\"classification\":classification}),parent=parent,branch=branch)\n",
    "\n",
    "def create_node_value(tree,data,parent=None,branch=None):\n",
    "    branch_f = \"\" if (branch is None) else branch+\" \"\n",
    "    tag =  \"\\x1b[31m■\\x1b[0m\" +\" \"+ branch_f +\"[\" + str(data[1])+\", \" + format_timedelta(data[0])+ \"] \"  \n",
    "\n",
    "    print(data)\n",
    "    return     tree.create_node(tag,data=(branch,{\"value\":data}),parent=parent,branch=branch)\n",
    "\n",
    "\n",
    "# Example tree:\n",
    "tree = SequenceTree()\n",
    "\n",
    "# Each node has:\n",
    "# - Tag (string to be printed in the tree)\n",
    "# - Data (the actual info) is a tuple with:\n",
    "#  - branch --> \"t\" or \"f\"\n",
    "#  - A dict, which changes according to the type of node:\n",
    "#     . event: {\"event\":(d,l)}\n",
    "#     . value: {\"value\":v}\n",
    "#     . classification (leaf): {\"classification\":class}\n",
    "max_dl,ig=max_IG(dataset_X,dataset_Y,dataset_ST)\n",
    "print(max_dl)\n",
    "\n",
    "root = create_node_event(tree,max_dl)\n",
    "#a = create_node_event(tree,max_dl,parent=root.identifier,branch=\"t\")\n",
    "#b = create_node_value(tree,max_dl,parent=root.identifier,branch=\"f\")\n",
    "#c = create_node_event(tree,max_dl,parent=b,branch=\"f\")\n",
    "\n",
    "\n",
    "\n",
    "print(tree.show(stdout=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_diabetes_dataset()\n",
    "\n",
    "dataset_X,dataset_Y,dataset_ST, dataset_V = compute_datasets(ds,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
