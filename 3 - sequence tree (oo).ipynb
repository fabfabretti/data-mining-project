{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 3 - Sequence tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import os\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Some helper functions!\n",
    "\n",
    "#takes a timedelta and prints it in a human-readable format\n",
    "def format_timedelta(td: timedelta) -> str:\n",
    "    days = td.days\n",
    "    years, days = divmod(days, 365)\n",
    "    months, days = divmod(days, 30)\n",
    "    hours, remainder = divmod(td.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    \n",
    "    formatted_str = \"\"\n",
    "    if years:\n",
    "        formatted_str += f\"{years}y \"\n",
    "    if months:\n",
    "        formatted_str += f\"{months}mo \"\n",
    "    if days:\n",
    "        formatted_str += f\"{days}d \"\n",
    "    if hours:\n",
    "        formatted_str += f\"{hours}h \"\n",
    "    if minutes:\n",
    "        formatted_str += f\"{minutes}m \"\n",
    "    if seconds:\n",
    "        formatted_str += f\"{seconds}s \"\n",
    "    \n",
    "\n",
    "    \n",
    "    return formatted_str[:-1] if formatted_str else \"0s\"\n",
    "  \n",
    "# Given a set with binary classes, computes entropy\n",
    "def compute_entropy(dataset_Y):\n",
    "    ones = len(list(filter(lambda classification : classification == 1,dataset_Y)))\n",
    "    zeros = len(list(filter(lambda classification : classification == 0,dataset_Y)))\n",
    "\n",
    "    if(ones == 0 or zeros==0):\n",
    "        return 0\n",
    "    \n",
    "    entropy = ones/len(dataset_Y)*math.log2(1/(ones/len(dataset_Y))) + zeros/len(dataset_Y)*math.log2(1/(zeros/len(dataset_Y)))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "# returns a list of tuples (i,x) where i is the index of the patient in the dataset and x is a timedelta of per quanto tempo abbiamo rilevazioni\n",
    "def find_durations(dataset):\n",
    "    lengths = []\n",
    "    for entry in dataset:\n",
    "        min_t = datetime.max\n",
    "        max_t = datetime.min\n",
    "        for item in entry:\n",
    "            if item[0] < min_t:\n",
    "                min_t = item[0]\n",
    "            elif item[0] > max_t:\n",
    "                max_t = item[0]\n",
    "        lengths.append(max_t-min_t)\n",
    "    return [(i, x) for i, x in enumerate(lengths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operazioni preliminari sul DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loadind raw DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DS loader\n",
      "\tSkipped 46 items for formatting issues in data file. 70 loaded.\n"
     ]
    }
   ],
   "source": [
    "def load_diabetes_dataset(verbose=False)-> list: \n",
    "    folder_path=\"datasets\\\\diabetes\"\n",
    "    dataset = []\n",
    "    errcount=0\n",
    "    print(f\"-- DS loader\")\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if os.path.isfile(file_path)and filename.startswith('data'):\n",
    "            entry=[]\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.readlines()\n",
    "                for line in content:\n",
    "                    item = tuple((line[0:-1] if line.endswith('\\n') else tuple(line)).split(\"\\t\"))\n",
    "\n",
    "                    # If the item is valid, append it to the entry\n",
    "                    try:\n",
    "                        item_f = datetime.strptime(item[0]+\" \"+item[1], \"%m-%d-%Y %H:%M\")\n",
    "                        entry.append((item_f,item[2],item[3]))\n",
    "                    except:\n",
    "                        if(verbose):\n",
    "                            print(f\"\\t[!] Entry {item} in file {filename} is NOT vallid. Skipped!\")\n",
    "                        errcount+=1\n",
    "                # add the entry to the dataset\n",
    "                dataset.append(entry)\n",
    "    print(f\"\\tSkipped {errcount} items for formatting issues in data file. {len(dataset)} loaded.\")\n",
    "    return dataset\n",
    "\n",
    "dataset = load_diabetes_dataset(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given raw DS, generate actual DS and classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo la classe reale come uno [0,1] che indica se l'evento 65 (65 = Hypoglycemic symptoms) si Ã¨ verificato in una certa finestra di tempo `event_window` dopo un certo waiting time `waiting_window`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DS builder\n",
      "\t6 entries unsuitable for selected windows.\n",
      "\tFinal dataset size: 64. Classes: 27|37, entropy 0.982\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def compute_datasets(dataset:list,observation_window,waiting_window,prediction_window):\n",
    "\n",
    "    dataset_X = []\n",
    "    dataset_Y = []\n",
    "    \n",
    "    dataset_ST = [entry[0][0] for entry in dataset ]\n",
    "\n",
    "    count_excluded=0\n",
    "\n",
    "    for i in range(0,len(dataset)):\n",
    "        entry = dataset[i]\n",
    "\n",
    "        end_obs = dataset_ST[i]+observation_window\n",
    "        \n",
    "        start_pred = end_obs + waiting_window\n",
    "        end_pred = start_pred + prediction_window\n",
    "\n",
    "        if end_pred < entry[-1][0]:\n",
    "            entry_X = []\n",
    "            found = 0\n",
    "\n",
    "            for item in entry:\n",
    "                if item[0]>= dataset_ST[i] and item[0]<end_obs:\n",
    "                    entry_X.append(item)\n",
    "                if item[0]>=start_pred and item[0]<end_pred:\n",
    "                    # put Y=1 if it has at least one \"65\" entry\n",
    "                    if (item[1]==\"65\"):\n",
    "                        found = 1\n",
    "            dataset_X.append(entry_X)\n",
    "            dataset_Y.append(found)\n",
    "\n",
    "        else:\n",
    "            count_excluded+=1\n",
    "    \n",
    "    dataset_ST = [entry[0][0] for entry in dataset_X ]\n",
    "    dataset_V = [None]*len(dataset_X)\n",
    "    print(f\"-- DS builder\")\n",
    "    print(f\"\\t{count_excluded} entries unsuitable for selected windows.\")\n",
    "    print(f\"\\tFinal dataset size: {len(dataset_X)}. Classes: {sum(1 for c in dataset_Y if c == 1)}|{sum(1 for c in dataset_Y if c == 0)}, entropy {float(compute_entropy(dataset_Y)):4.3}\")    \n",
    "    return dataset_X,dataset_Y,dataset_ST,dataset_V\n",
    "\n",
    "observation_window = timedelta(days=+5)\n",
    "waiting_window = timedelta(days=+5)\n",
    "prediction_window = timedelta(days=+15)\n",
    "\n",
    "# Convert raw data into dataset X,Y, etc\n",
    "dataset_X,dataset_Y, dataset_ST,dataset_V = compute_datasets(dataset[:],observation_window,waiting_window,prediction_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reload if needed for testing/showcasing purposes\n",
    "def reload_ds():\n",
    "    # prepare\n",
    "    dataset = load_diabetes_dataset(False)\n",
    "    observation_window = timedelta(days=+3)\n",
    "    waiting_window = timedelta(days=+0)\n",
    "    prediction_window = timedelta(days=+10)\n",
    "\n",
    "    dataset_X,dataset_Y, dataset_ST,dataset_V = compute_datasets(dataset,observation_window,waiting_window,prediction_window,)\n",
    "    return dataset_X,dataset_Y, dataset_ST,dataset_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stampiamo il DS elaborato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--DS state\n",
      "index\tX\tY\tST\t\t\tV\n",
      "0\tl.35\t1\t1991-04-21 09:09:00\tNone\n",
      "1\tl.38\t0\t1989-10-10 08:00:00\tNone\n",
      "2\tl.49\t1\t1990-07-21 06:43:00\tNone\n",
      "3\tl.41\t0\t1990-08-19 17:00:00\tNone\n",
      "4\tl.46\t0\t1990-09-01 16:48:00\tNone\n",
      "5\tl.38\t0\t1989-03-27 22:00:00\tNone\n",
      "6\tl.24\t1\t1990-07-31 12:09:00\tNone\n",
      "7\tl.43\t0\t1990-04-22 18:08:00\tNone\n",
      "8\tl.34\t0\t1989-02-18 08:00:00\tNone\n",
      "9\tl.34\t1\t1990-07-13 09:44:00\tNone\n",
      "10\tl.46\t1\t1990-07-22 09:53:00\tNone\n",
      "11\tl.58\t1\t1990-09-04 05:53:00\tNone\n",
      "12\tl.38\t1\t1991-03-11 18:15:00\tNone\n",
      "13\tl.27\t1\t1991-04-13 08:47:00\tNone\n",
      "14\tl.22\t1\t1991-05-22 07:24:00\tNone\n",
      "15\tl.40\t1\t1990-07-13 09:48:00\tNone\n",
      "16\tl.44\t0\t1990-08-18 07:16:00\tNone\n",
      "17\tl.45\t1\t1990-09-09 17:23:00\tNone\n",
      "18\tl.39\t0\t1991-05-12 06:55:00\tNone\n",
      "19\tl.44\t0\t1989-09-03 08:00:00\tNone\n",
      "20\tl.36\t0\t1991-03-14 22:05:00\tNone\n",
      "21\tl.37\t0\t1991-04-27 23:02:00\tNone\n",
      "22\tl.28\t0\t1991-05-28 21:35:00\tNone\n",
      "23\tl.13\t0\t1990-07-24 16:00:00\tNone\n",
      "24\tl.25\t0\t1988-07-13 08:00:00\tNone\n",
      "25\tl.16\t0\t1989-01-29 08:00:00\tNone\n",
      "26\tl.16\t0\t1989-11-05 07:00:00\tNone\n",
      "27\tl.40\t0\t1990-04-29 07:00:00\tNone\n",
      "28\tl.40\t0\t1990-12-18 07:00:00\tNone\n",
      "29\tl.39\t0\t1991-05-20 08:00:00\tNone\n",
      "30\tl.65\t1\t1990-07-31 18:28:00\tNone\n",
      "31\tl.45\t1\t1990-08-23 07:19:00\tNone\n",
      "32\tl.52\t1\t1990-09-11 18:00:00\tNone\n",
      "33\tl.10\t1\t1991-03-28 15:35:00\tNone\n",
      "34\tl.47\t1\t1991-04-26 06:17:00\tNone\n",
      "35\tl.55\t0\t1991-06-11 18:05:00\tNone\n",
      "36\tl.49\t0\t1991-07-03 12:21:00\tNone\n",
      "37\tl.39\t0\t1990-12-16 08:00:00\tNone\n",
      "38\tl.41\t0\t1991-06-30 08:00:00\tNone\n",
      "39\tl.52\t1\t1990-07-13 11:36:00\tNone\n",
      "40\tl.53\t1\t1990-08-26 17:26:00\tNone\n",
      "41\tl.51\t1\t1990-09-13 20:56:00\tNone\n",
      "42\tl.48\t0\t1991-03-29 18:59:00\tNone\n",
      "43\tl.52\t1\t1991-05-04 01:05:00\tNone\n",
      "44\tl.39\t1\t1991-07-05 20:47:00\tNone\n",
      "45\tl.36\t1\t1990-07-16 11:40:00\tNone\n",
      "46\tl.28\t0\t1990-08-03 06:31:00\tNone\n",
      "47\tl.45\t0\t1991-03-30 05:56:00\tNone\n",
      "48\tl.39\t0\t1991-04-20 11:20:00\tNone\n",
      "49\tl.35\t0\t1989-03-28 08:00:00\tNone\n",
      "50\tl.39\t0\t1990-07-29 07:00:00\tNone\n",
      "51\tl.41\t0\t1991-03-01 08:00:00\tNone\n",
      "52\tl.47\t0\t1989-02-03 08:00:00\tNone\n",
      "53\tl.22\t0\t1990-06-25 19:16:00\tNone\n",
      "54\tl.54\t1\t1990-08-22 05:29:00\tNone\n",
      "55\tl.59\t1\t1990-09-04 05:35:00\tNone\n",
      "56\tl.56\t0\t1991-04-15 13:08:00\tNone\n",
      "57\tl.43\t0\t1991-05-16 20:40:00\tNone\n",
      "58\tl.48\t0\t1991-06-12 05:48:00\tNone\n",
      "59\tl.45\t1\t1991-07-26 22:04:00\tNone\n",
      "60\tl.40\t1\t1989-04-17 06:35:00\tNone\n",
      "61\tl.40\t1\t1991-01-01 09:10:00\tNone\n",
      "62\tl.12\t0\t1988-03-27 08:00:00\tNone\n",
      "63\tl.30\t0\t1989-03-13 08:00:00\tNone\n",
      "# entries: 64, entropy=0.982\n"
     ]
    }
   ],
   "source": [
    "def print_dataset_state(dataset_X,dataset_Y,dataset_ST,dataset_V,indexes=None):\n",
    "    print(\"--DS state\")\n",
    "    if (indexes and len(indexes)>len(dataset_X)):\n",
    "        indexes=None\n",
    "    \n",
    "    print(\"index\\tX\\tY\\tST\\t\\t\\tV\")\n",
    "\n",
    "    if indexes is None:\n",
    "        indexes = range(0,len(dataset_X))\n",
    "\n",
    "    for i in indexes:\n",
    "        print(f\"{i}\\tl.{len(dataset_X[i])}\\t{dataset_Y[i]}\\t{dataset_ST[i]}\\t{dataset_V[i]}\")\n",
    "    print(f\"# entries: {len(dataset_X)}, entropy={float(compute_entropy(dataset_Y)):4.3}\")\n",
    "    \n",
    "print_dataset_state(dataset_X,dataset_Y, dataset_ST,dataset_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximization functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define all functions to compute the best (d,l) couple and split a dataset according to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "# Find al possible d,l couples that I could split the tree on. Note: Ds are randomly selected bc otherwise I'd end up with ~36000 pairs...\n",
    "def create_pairs(dataset_X:list,dataset_ST:list,howmany_d =30,random_sampling=False):\n",
    "\n",
    "    #1. Create all labels available from current\n",
    "    labels = set()\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        for item in dataset_X[i]:\n",
    "            if (item[0] > dataset_ST[i]):# Only consider label if it's not been superato\n",
    "                labels.add(item[1])\n",
    "\n",
    "\n",
    "    #2. Find all d\n",
    "    durations = set()\n",
    "    if random_sampling:\n",
    "        durations = set()\n",
    "        for i in range(0,len(dataset_X)):\n",
    "            for item in dataset_X[i]:\n",
    "                if (item[0] > dataset_ST[i]): # Only consider timestamp if it's not been superato\n",
    "                    durations.add(item[0]-dataset_ST[i])\n",
    "        if len(durations)>howmany_d:\n",
    "            durations = random.sample(sorted(durations),howmany_d) # Is this ok?\n",
    "        else:\n",
    "            durations = sorted(durations)\n",
    "    else:\n",
    "        durations = set()\n",
    "        for i in range(0,len(dataset_X)):\n",
    "            for item in dataset_X[i]:\n",
    "                durations.add(item[0]-dataset_ST[i])\n",
    "        delta = max(durations)/(howmany_d+1)\n",
    "        durations = set()\n",
    "        for i in range(1,howmany_d+1):\n",
    "            durations.add(delta*i)\n",
    "\n",
    "    #print([format_timedelta(x) for x in sorted(durations)])\n",
    "    return sorted(list(itertools.product(durations,labels)))\n",
    "\n",
    "# Given a d,l couple, split the dataset and return indexes of true and false entries.\n",
    "def test_event(dataset_X,dl_pair,dataset_ST,dataset_V=None,update=False):\n",
    "    i_T = [] # indexes of entries that have label==l within d time\n",
    "    i_F = [] # indexes of entries that have DON'T HAVE label==l within d time\n",
    "    d,l = dl_pair\n",
    "\n",
    "\n",
    "    #1. Separate entries that satisfy event test from those who don't\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        entry = dataset_X[i]\n",
    "        found=False\n",
    "\n",
    "        for item in entry:\n",
    "            #print(item)\n",
    "            if(found is False and item[0]>=dataset_ST[i] and item[0]<=(dataset_ST[i]+d) and item[1]==l ):\n",
    "                found=True\n",
    "\n",
    "        if(found):\n",
    "            i_T.append(i)\n",
    "        else:\n",
    "            i_F.append(i)\n",
    "    return i_T,i_F\n",
    "\n",
    "# Given a pair of duration d and label l, computes its information gain on the dataset if we were to split it according to the sequence tree rules.\n",
    "def compute_IG(dl_pair,dataset_X,dataset_Y,dataset_ST,verbose=False):\n",
    "    #if verbose:\n",
    "     #   print(f\"Computing IG for {dl_pair}\")\n",
    "\n",
    "    entropy_0 = compute_entropy(dataset_Y)\n",
    "\n",
    "    i_T, i_F = test_event(dataset_X,dl_pair,dataset_ST)\n",
    "    \n",
    "    #2. Compute final entropy. first let's generate our new datasets...\n",
    "    dataset_Yt=[ dataset_Y[i] for i in i_T]\n",
    "    dataset_Yf=[ dataset_Y[i] for i in i_F]\n",
    "            \n",
    "    entropy_f = (len(i_T)/len(dataset_X))*compute_entropy(dataset_Yt) + (len(i_F)/len(dataset_X))*compute_entropy(dataset_Yf)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"information gain is {entropy_0-entropy_f} {entropy_0}-> {[ dataset_Y[i] for i in i_T],[ dataset_Y[i] for i in i_F]} {entropy_f}\")\n",
    "        \n",
    "    return(entropy_0-entropy_f,i_T,i_F)\n",
    "\n",
    "# Given all possible pairs of d,l finds the one with the highest information gain (aka the one I should actually split on)\n",
    "def maximize_IG_event(dataset_X,dataset_Y, dataset_ST, indexes=None,verbose=False,howmany=30,random_sampling=False):\n",
    "    if indexes is None:\n",
    "        indexes = range(0,len(dataset_X))\n",
    "\n",
    "    \n",
    "    dl_pairs = create_pairs([dataset_X[i] for i in indexes],[dataset_ST[i] for i in indexes],howmany,random_sampling)\n",
    "\n",
    "\n",
    "    igs_list = [(x,compute_IG(x,dataset_X,dataset_Y,dataset_ST,verbose)) for x in dl_pairs]\n",
    "    # ogni entry di igs_list Ã¨ ( (d,l) , (ig,i_T,i_F)   )\n",
    "\n",
    "    max_ig=-1\n",
    "    max_dl=None\n",
    "\n",
    "    for ((d,l),(ig,i_T,i_F)) in igs_list:\n",
    "        if ig > max_ig:\n",
    "            max_ig=ig\n",
    "            max_dl = ((d,l),(ig,i_T,i_F))\n",
    "\n",
    "    \n",
    "    if len(max_dl[1][1])==0 or len(max_dl[1][2])==0:\n",
    "        if verbose:\n",
    "            print(\"Split failed, couldn't find a d,l that separates values :(\")\n",
    "        return None, 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Max IG is in couple d=\",format_timedelta(max_dl[0][0]),\", l=\",max_dl[0][1],\", IG=\",max_dl[1])\n",
    "\n",
    "    max_dl = (max_dl[0],max_dl[1][0])\n",
    "    return max_dl\n",
    "\n",
    "# Given a d,l couple, split the dataset in two and update starting times and dataset values.\n",
    "def perform_event_test(max_dl,indexes,dataset_X,dataset_ST,dataset_V,verbose=False):\n",
    "    # divide dataset in t and f...\n",
    "    i_T = []\n",
    "    i_F = []\n",
    "\n",
    "    d,l = max_dl\n",
    "    old_dataset_ST=dataset_ST\n",
    "    \n",
    "    for i in range(0,len(dataset_ST)):\n",
    "        entry = dataset_X[i]\n",
    "\n",
    "        found=False\n",
    "\n",
    "        for item in entry:\n",
    "            #starting from the starting time, see if it exists an item with timestamp < d and label == l\n",
    "            \n",
    "            if(found is False and item[0]>= old_dataset_ST[i] and item[0]<=(old_dataset_ST[i]+d) and item[1]==l ): # If i'm over starting time\n",
    "                found=True\n",
    "                dataset_ST[i] = item[0]\n",
    "                dataset_V[i] = item[2]\n",
    "\n",
    "        if(found):\n",
    "            i_T.append(i)\n",
    "        else:\n",
    "            i_F.append(i)\n",
    "            \n",
    "    return i_T,i_F,dataset_ST,dataset_V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define all functions to compute the best value split and split a dataset according to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a dataset, test out all possible values and see which one would lead to the best split.\n",
    "# If we have called this function, it means we are in a true branch and all values in dataset_V are already of a single label.\n",
    "def maximize_IG_value(dataset_V,dataset_Y):\n",
    "    values = set([v for v in dataset_V])\n",
    "    values_ig = []\n",
    "\n",
    "    for v in values:   \n",
    "        entropy_0 = compute_entropy(dataset_Y)\n",
    "        i_T=[]\n",
    "        i_F=[]\n",
    "        for i in range(0,len(dataset_V)):\n",
    "            if dataset_V[i] <= v:\n",
    "                i_T.append(i)\n",
    "            else:\n",
    "                i_F.append(i)\n",
    "        dataset_Yt=[ dataset_Y[i] for i in i_T]\n",
    "        dataset_Yf=[ dataset_Y[i] for i in i_F]\n",
    "        entropy_f = (len(i_T)/len(dataset_Y))*compute_entropy(dataset_Yt) + (len(i_F)/len(dataset_Y))*compute_entropy(dataset_Yf)\n",
    "        ig = entropy_0-entropy_f\n",
    "        values_ig.append((v,ig))\n",
    "\n",
    "    return max(values_ig, key=lambda x: x[1])\n",
    "\n",
    "# Given a d,l couple, split the dataset in two and update dataset values.\n",
    "def perform_value_test(value,dataset_V):\n",
    "    i_T=[]\n",
    "    i_F=[]\n",
    "    for i in range(0,len(dataset_V)):\n",
    "        if dataset_V[i] <= value:\n",
    "            i_T.append(i)\n",
    "        else:\n",
    "            i_F.append(i)\n",
    "    return i_T,i_F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SequenceTree definition and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DS loader\n",
      "\tSkipped 46 items for formatting issues in data file. 70 loaded.\n",
      "-- DS builder\n",
      "\t2 entries unsuitable for selected windows.\n",
      "\tFinal dataset size: 68. Classes: 31|37, entropy 0.994\n",
      "-- Fit\n",
      "\u001b[32mâ¬¤  (48,2d 17h 1m 1s)\u001b[0m - [e=0.99 ig=0.21] [n=68] \n",
      "âââ \u001b[31mâ  t  (48, 109) \u001b[0m- [e=0.44 ig=0.053] [n=22] \n",
      "â   âââ \u001b[31mâ  f  (48, 123) \u001b[0m- [e=0.57 ig=0.22] [n=15] \n",
      "â   â   âââ \u001b[31mâ  f  (48, 210) \u001b[0m- [e=0.37 ig=0.093] [n=14] \n",
      "â   â   â   âââ \u001b[32mâ¬¤ f  (34,9h 8m 23s)\u001b[0m - [e=0.65 ig=0.65] [n=6] \n",
      "â   â   â   â   âââ \u001b[33mâ f  0 \u001b[0m- [e=0.0] \u001b[33m[n=5]\u001b[0m - [9, 53, 54, 63, 67]\n",
      "â   â   â   â   âââ \u001b[33mâ t  1 \u001b[0m- [e=0.0] \u001b[33m[n=1]\u001b[0m - [65]\n",
      "â   â   â   âââ \u001b[33mâ t  0 \u001b[0m- [e=0.0] \u001b[33m[n=8]\u001b[0m - [5, 6, 19, 20, 25, 29, 39, 51]\n",
      "â   â   âââ \u001b[33mâ t  1 \u001b[0m- [e=0.0] \u001b[33m[n=1]\u001b[0m - [0]\n",
      "â   âââ \u001b[33mâ t  0 \u001b[0m- [e=0.0] \u001b[33m[n=7]\u001b[0m - [1, 26, 27, 28, 30, 40, 52]\n",
      "âââ \u001b[32mâ¬¤ f  (62,6h 57m 58s)\u001b[0m - [e=0.95 ig= 0.1] [n=46] \n",
      "    âââ \u001b[32mâ¬¤ f  (34,2d 17h 1m 1s)\u001b[0m - [e=0.81 ig=0.17] [n=32] \n",
      "    â   âââ \u001b[31mâ  t  (34, 12) \u001b[0m- [e=0.45 ig=0.32] [n=21] \n",
      "    â   â   âââ \u001b[32mâ¬¤ t  (33,4h 38m 38s)\u001b[0m - [e=0.92 ig=0.92] [n=3] \n",
      "    â   â   â   âââ \u001b[33mâ f  0 \u001b[0m- [e=0.0] \u001b[33m[n=2]\u001b[0m - [23, 66]\n",
      "    â   â   â   âââ \u001b[33mâ t  1 \u001b[0m- [e=0.0] \u001b[33m[n=1]\u001b[0m - [32]\n",
      "    â   â   âââ \u001b[33mâ f  1 \u001b[0m- [e=0.0] \u001b[33m[n=18]\u001b[0m - [10, 11, 12, 14, 15, 16, 17, 18, 22, 33, 36, 43, 44, 45, 46, 49, 55, 57]\n",
      "    â   âââ \u001b[32mâ¬¤ f  (65,1d 15h 26m 17s)\u001b[0m - [e=0.99 ig= 0.4] [n=11] \n",
      "    â       âââ \u001b[32mâ¬¤ f  (57,23h 3m 52s)\u001b[0m - [e=0.81 ig=0.31] [n=8] \n",
      "    â       â   âââ \u001b[32mâ¬¤ f  (33,2h 17m 48s)\u001b[0m - [e= 1.0 ig=0.31] [n=4] \n",
      "    â       â   â   âââ \u001b[32mâ¬¤ t  (56,2h 17m 42s)\u001b[0m - [e=0.92 ig=0.92] [n=3] \n",
      "    â       â   â   â   âââ \u001b[33mâ f  0 \u001b[0m- [e=0.0] \u001b[33m[n=2]\u001b[0m - [56, 60]\n",
      "    â       â   â   â   âââ \u001b[33mâ t  1 \u001b[0m- [e=0.0] \u001b[33m[n=1]\u001b[0m - [2]\n",
      "    â       â   â   âââ \u001b[33mâ f  1 \u001b[0m- [e=0.0] \u001b[33m[n=1]\u001b[0m - [24]\n",
      "    â       â   âââ \u001b[33mâ t  0 \u001b[0m- [e=0.0] \u001b[33m[n=4]\u001b[0m - [21, 48, 59, 62]\n",
      "    â       âââ \u001b[33mâ t  1 \u001b[0m- [e=0.0] \u001b[33m[n=3]\u001b[0m - [35, 61, 64]\n",
      "    âââ \u001b[32mâ¬¤ t  (65,2d 14h 40m 50s)\u001b[0m - [e=0.94 ig=0.66] [n=14] \n",
      "        âââ \u001b[32mâ¬¤ t  (35,10h 43m 3s)\u001b[0m - [e=0.65 ig=0.65] [n=6] \n",
      "        â   âââ \u001b[33mâ f  1 \u001b[0m- [e=0.0] \u001b[33m[n=5]\u001b[0m - [7, 13, 34, 37, 42]\n",
      "        â   âââ \u001b[33mâ t  0 \u001b[0m- [e=0.0] \u001b[33m[n=1]\u001b[0m - [3]\n",
      "        âââ \u001b[33mâ f  0 \u001b[0m- [e=0.0] \u001b[33m[n=8]\u001b[0m - [4, 8, 31, 38, 41, 47, 50, 58]\n",
      "\n",
      "-- Predict\n",
      "\tP\tN\n",
      "P\t31\t0\n",
      "N\t0\t37\n",
      "Items: 68\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from treelib import Tree,Node\n",
    "\n",
    "class SequenceTree(Tree):\n",
    "    def __init__(self, tree=None, deep=False, node_class=None, identifier=None):\n",
    "        super(SequenceTree, self).__init__(tree=tree, deep=deep, node_class=node_class, identifier=identifier)\n",
    "\n",
    "    # Let's override original create_node method in order to add new constraints such as child node number and true/false branchs.\n",
    "    def create_node(self, tag=None, identifier=None, parent=None, data=None,branch=None):\n",
    "        \"\"\"\n",
    "        Create a child node for the given @parent node. If ``identifier`` is absent,\n",
    "        a UUID will be generated automatically.\n",
    "        \"\"\"\n",
    "        \n",
    "        new_node = super(SequenceTree, self).create_node(tag=tag, parent=parent, data=data)\n",
    "        siblings = super(SequenceTree,self).siblings(new_node.identifier)\n",
    "        \n",
    "        if len(super(SequenceTree,self).siblings(new_node.identifier))>=2:\n",
    "           raise ValueError(\"Parent node already has maximum number of children\")\n",
    "\n",
    "        if branch in [x.data[\"branch\"] for x in siblings]:\n",
    "           raise ValueError(f\"Parent node already has a {branch} branch\")\n",
    "        \n",
    "        return new_node\n",
    "    \n",
    "    # Library has a bug that won't show trees correctly unless stdout=False is added.\n",
    "    def display(self):\n",
    "        print(self.show(stdout=False))\n",
    "\n",
    "    def create_node_event(self,data,parent=None,branch=None,entropy=\"\",size=0,ig=\"\",index=\"\"):\n",
    "        branch_f = \"\" if (branch is None) else str(branch)+\" \"\n",
    "        tag =  f\"\\x1b[32mâ¬¤ {branch_f} ({str(data[1])},{format_timedelta(data[0])})\\x1b[0m - [e={entropy:4.2} ig={ig:4.2}] [n={size}] {index}\"\n",
    "        data = {\"branch\":branch, \"dl\":(data[0],data[1]),\"entropy\":entropy,\"ig\":ig,\"index\":index}\n",
    "\n",
    "        return     self.create_node(tag,data=data,parent=parent,branch=branch)\n",
    "\n",
    "    def create_node_value(self,label_value,parent=None,branch=None,entropy=\"\",size=0,ig=\"\",index=\"\"):\n",
    "        branch_f = \"\" if (branch is None) else branch+\" \"\n",
    "        tag =  f\"\\x1b[31mâ  {branch_f} ({label_value[0]}, {label_value[1]}) \\x1b[0m- [e={float(entropy):2.2} ig={ig:4.2}] [n={size}] {index}\"\n",
    "        data = {\"branch\":branch,\"value\":label_value,\"entropy\":entropy,\"index\":index}\n",
    "        return     self.create_node(tag,data=data,parent=parent,branch=branch)\n",
    "\n",
    "    def create_node_class(self,classification,parent=None,branch=None,entropy=\"\",size=0,index=\"\"):\n",
    "        branch_f = \"\" if (branch is None) else str(branch)+\" \"\n",
    "\n",
    "        tag =  f\"\\x1b[33mâ {branch_f} {classification} \\x1b[0m- [e={float(entropy):2.2}] \\x1b[33m[n={size}]\\x1b[0m - {index}\"\n",
    "\n",
    "        # If the classsification had \"max length reached\", remove the tag from data\n",
    "        if isinstance(classification, str):\n",
    "            classification = int(classification[0])\n",
    "\n",
    "        data = {\"branch\":branch, \"class\":classification,\"entropy\":entropy,\"index\":index}\n",
    "\n",
    "\n",
    "        return     self.create_node(tag,data=data,parent=parent,branch=branch)\n",
    "    \n",
    "        \n",
    "    def fit(self,dataset_X,dataset_Y,dataset_V,dataset_ST,max_depth:int,parent=None,branch=None,depth=0,indexes=[],verbose=False):\n",
    "        \"\"\"\n",
    "        Implements the fit algorithm as described in the exercise text.\n",
    "\n",
    "        :param list dataset_X: Dataset entries, where each entry is a list of items (time,label,value)\n",
    "        :param list dataset_Y: classes of the dataset, each entry is the class of the dataset entry with the same index\n",
    "        :param list dataset_V: it's a int value if there has been a previous (d,l) test, otherwise it's None\n",
    "        :param list dataset_ST: current starting time (aka, how much of the list I've already read)\n",
    "        :param SequenceTree tree: the SequenceTree tree we're building.\n",
    "        :param int max_depth: how deep can the SequenceTree be before we stop creating new nodes and approximate the result.\n",
    "        :param parent: identifier of the parent node.\n",
    "        :param branch: whether the true node is on the true or false branch.\n",
    "        :param depth: depth of the current node.\n",
    "        :param indexes: original indexes of the entries currently being processed.\n",
    "        :param verbose: prints more info.\n",
    "        \"\"\"\n",
    "        if(parent is None):\n",
    "            print(\"-- Fit\")\n",
    "        \n",
    "\n",
    "        # BASE CASES:\n",
    "        \n",
    "        # 1. If I have 1 node only or all the classes are the same, make a leaf\n",
    "        if (len(dataset_X)==1) or (all(element == dataset_Y[0] for element in dataset_Y)):\n",
    "            if verbose:\n",
    "                print(f\"{depth,branch} All classes are the same!\")\n",
    "            self.create_node_class(classification=dataset_Y[0],parent=parent,branch=branch,entropy=compute_entropy(dataset_Y),size=len(dataset_Y),index=indexes)\n",
    "            return\n",
    "        \n",
    "        # 2. If I've reached the maximum number of tests allowed, create a leaf with the extimation of the class.\n",
    "        elif depth > max_depth:\n",
    "            if (verbose):\n",
    "                print(\"Reached maximum depth!\")\n",
    "\n",
    "            y_1 = sum(1 for c in dataset_Y if c == 1)\n",
    "            y_0 = sum(1 for c in dataset_Y if c == 0)\n",
    "            estimated_class = 1 if y_1>y_0 else 0  \n",
    "            estimated_class = str(estimated_class)+\" MAX DEPTH REACHED\"\n",
    "\n",
    "            self.create_node_class(estimated_class,parent,branch,compute_entropy(dataset_Y),len(dataset_Y),indexes)\n",
    "            return\n",
    "\n",
    "\n",
    "        # INDUCTIVE CASE\n",
    "\n",
    "        #Compute d,l that maximises IG.\n",
    "        max_dl, max_dl_ig = maximize_IG_event(dataset_X,dataset_Y,dataset_ST,range(len(dataset_X)),False,howmany=30,random_sampling=False)\n",
    "        \n",
    "        # If previous node is an event test and branch is True, or if previous node is an event test, compute value that maximises IG.\n",
    "        max_value_ig=0\n",
    "        if parent and ((\"dl\" in parent.data.keys() and branch==\"t\") or \"value\" in parent.data.keys() ):\n",
    "            max_value, max_value_ig=maximize_IG_value(dataset_V,dataset_Y)\n",
    "            \n",
    "\n",
    "        # If value beats event, create a new event node.\n",
    "        if max_value_ig > max_dl_ig:\n",
    "\n",
    "            label = parent.data[\"dl\"][1] if \"dl\" in parent.data.keys() else parent.data[\"value\"][0]\n",
    "\n",
    "            node = self.create_node_value((label,max_value),parent,branch,compute_entropy(dataset_Y),len(dataset_Y),max_value_ig)\n",
    "            i_T, i_F=perform_value_test(max_value,dataset_V)\n",
    "\n",
    "            self.fit([dataset_X[i] for i in i_T],[dataset_Y[i] for i in i_T],[dataset_V[i] for i in i_T],[dataset_ST[i] for i in i_T],max_depth,node,\"t\",depth+1,[indexes[i] for i in i_T])\n",
    "            self.fit([dataset_X[i] for i in i_F],[dataset_Y[i] for i in i_F],[dataset_V[i] for i in i_F],[dataset_ST[i] for i in i_F],max_depth,node,\"f\",depth+1,[indexes[i] for i in i_F])\n",
    "        \n",
    "        # Otherwise,create a new event node (shocking!).\n",
    "        else:\n",
    "\n",
    "            node = self.create_node_event(max_dl,parent,branch,compute_entropy(dataset_Y),len(dataset_Y),max_dl_ig)\n",
    "            i_T, i_F, dataset_ST, dataset_V=perform_event_test(max_dl,indexes,dataset_X,dataset_ST,dataset_V)\n",
    "\n",
    "            self.fit([dataset_X[i] for i in i_T],[dataset_Y[i] for i in i_T],[dataset_V[i] for i in i_T],[dataset_ST[i] for i in i_T],max_depth,node,\"t\",depth+1,[indexes[i] for i in i_T])\n",
    "            self.fit([dataset_X[i] for i in i_F],[dataset_Y[i] for i in i_F],[dataset_V[i] for i in i_F],[dataset_ST[i] for i in i_F],max_depth,node,\"f\",depth+1,[indexes[i] for i in i_F])\n",
    "\n",
    "    def predict_r(self,entry_X,entry_ST,entry_V,node:Node,verbose=False):\n",
    "\n",
    "        if verbose:\n",
    "            print(node)\n",
    "            \n",
    "        # BASE CASE\n",
    "        \n",
    "        if node.is_leaf():\n",
    "            if verbose:\n",
    "                print(\"end\")\n",
    "            return node.data[\"class\"]\n",
    "\n",
    "\n",
    "        # INDUCTIVE CASE    \n",
    "\n",
    "        children = [self.get_node(x) for x in node.successors(tree.identifier)]\n",
    "\n",
    "        if \"dl\" in node.data.keys():\n",
    "            if verbose:\n",
    "                print(\"dl test\")\n",
    "            i_T,i_F,entry_ST,entry_V = perform_event_test(node.data[\"dl\"],None,entry_X,entry_ST,entry_V)\n",
    "\n",
    "            branch = \"t\" if i_T else \"f\"\n",
    "            next_node = list(filter(lambda x : x.data[\"branch\"]==branch,children))[0]\n",
    "\n",
    "            return self.predict_r(entry_X,entry_ST,entry_V,next_node)\n",
    "\n",
    "        elif \"value\" in node.data.keys():\n",
    "            if verbose:\n",
    "                print(\"value test\")\n",
    "            i_T,i_F = perform_value_test(node.data[\"value\"][1],entry_V)\n",
    "            branch = \"t\" if i_T else \"f\"\n",
    "            next_node = list(filter(lambda x : x.data[\"branch\"]==branch,children))[0]\n",
    "\n",
    "            return self.predict_r(entry_X,entry_ST,entry_V,next_node)\n",
    "\n",
    "    # Given a list of new entries, computes the prediction and returns it. It also prints the confusion matrix!\n",
    "    def predict(self,entry_X,entry_ST,entry_V,entry_Y,verbose=False):\n",
    "        print(\"-- Predict\")\n",
    "        results=[]\n",
    "        root = self.get_node(self.root)\n",
    "\n",
    "\n",
    "        if isinstance(dataset_V,list) and len(dataset_V)==1:\n",
    "            entry_X = [entry_X]\n",
    "            entry_Y = [entry_Y]\n",
    "            entry_ST = [entry_ST]\n",
    "            entry_V = [entry_V]\n",
    "            results.append(self.predict_r(self.entry_X,entry_ST,entry_V,root),verbose)\n",
    "        else:\n",
    "            for i in range(0,len(entry_ST)):\n",
    "                results.append(self.predict_r([entry_X[i]],[entry_ST[i]],[entry_V[i]],root,verbose))\n",
    "        \n",
    "        tp = sum(1 for x, y in zip(results, entry_Y) if (x == 1 and y==1))\n",
    "        tn = sum(1 for x, y in zip(results, entry_Y) if (x == 0 and y==0))\n",
    "        fp = sum(1 for x, y in zip(results, entry_Y) if (x == 1 and y==0))\n",
    "        fn = sum(1 for x, y in zip(results, entry_Y) if (x == 0 and y==1))\n",
    "\n",
    "        print(f\"\\tP\\tN\\nP\\t{tp}\\t{fn}\\nN\\t{fp}\\t{tn}\")\n",
    "        print(f\"Items: {tp+tn+fp+fn}\")\n",
    "        print(f\"Accuracy: {float((tp+tn)/(tp+tn+fp+fn)):4.3}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reload ds\n",
    "dataset_X,dataset_Y, dataset_ST,dataset_V = reload_ds()\n",
    "\n",
    "#--Fit\n",
    "tree = SequenceTree()\n",
    "tree.fit(dataset_X,dataset_Y,dataset_V,dataset_ST,max_depth=100,indexes=range(0,len(dataset_X)))\n",
    "if len(tree.all_nodes()) != 0:\n",
    "    print(tree)\n",
    "\n",
    "#--Predict\n",
    "# Choose which entries to test\n",
    "rangee=len(dataset_V)\n",
    "entry_X,entry_Y, entry_ST,entry_V = dataset_X[0:rangee],dataset_Y[0:rangee], dataset_ST[0:rangee],dataset_V[0:rangee]\n",
    "# Test'em!\n",
    "predictions = tree.predict(entry_X,entry_ST,entry_V,entry_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
